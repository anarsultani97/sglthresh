{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#imports\" data-toc-modified-id=\"imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>imports</a></span></li><li><span><a href=\"#load-original-subsets:-train,-test-and-split-or...\" data-toc-modified-id=\"load-original-subsets:-train,-test-and-split-or...-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>load original subsets: train, test and split or...</a></span></li><li><span><a href=\"#...load-my-subsets:-train,-dev,-test\" data-toc-modified-id=\"...load-my-subsets:-train,-dev,-test-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>...load my subsets: train, dev, test</a></span></li><li><span><a href=\"#Neural-net-to-make-predictions...\" data-toc-modified-id=\"Neural-net-to-make-predictions...-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Neural net to make predictions...</a></span></li><li><span><a href=\"#Or-load-LabelPowerset-probas...\" data-toc-modified-id=\"Or-load-LabelPowerset-probas...-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Or load LabelPowerset probas...</a></span></li><li><span><a href=\"#Static-thresholds\" data-toc-modified-id=\"Static-thresholds-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Static thresholds</a></span></li><li><span><a href=\"#SGLThresh\" data-toc-modified-id=\"SGLThresh-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>SGLThresh</a></span><ul class=\"toc-item\"><li><span><a href=\"#SurrogateHeaviside-definition\" data-toc-modified-id=\"SurrogateHeaviside-definition-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>SurrogateHeaviside definition</a></span></li><li><span><a href=\"#numerical-application\" data-toc-modified-id=\"numerical-application-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>numerical application</a></span></li><li><span><a href=\"#numerical-application,-for-loop\" data-toc-modified-id=\"numerical-application,-for-loop-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>numerical application, for loop</a></span></li></ul></li><li><span><a href=\"#NumThresh\" data-toc-modified-id=\"NumThresh-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>NumThresh</a></span><ul class=\"toc-item\"><li><span><a href=\"#utility-function-for-F1-with-multiprocessing:-TODO\" data-toc-modified-id=\"utility-function-for-F1-with-multiprocessing:-TODO-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>utility function for F1 with multiprocessing: TODO</a></span></li><li><span><a href=\"#utility-function-for-F1-w/o-multiprocessing\" data-toc-modified-id=\"utility-function-for-F1-w/o-multiprocessing-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>utility function for F1 w/o multiprocessing</a></span></li><li><span><a href=\"#numerical-application\" data-toc-modified-id=\"numerical-application-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>numerical application</a></span></li><li><span><a href=\"#for-BCE\" data-toc-modified-id=\"for-BCE-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>for BCE</a></span></li><li><span><a href=\"#numerical-application-with-BCE\" data-toc-modified-id=\"numerical-application-with-BCE-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>numerical application with BCE</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T13:37:53.011568Z",
     "start_time": "2020-03-05T13:37:47.999336Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# paper http://lpis.csd.auth.gr/publications/tsoumakas-ismir08.pdf\n",
    "\n",
    "# !pip install scikit-multilearn\n",
    "# !pip install skorch\n",
    "# !pip install liac-arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:10:50.598010Z",
     "start_time": "2020-03-22T15:10:50.007204Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "# from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:14:48.796533Z",
     "start_time": "2020-03-22T15:14:48.775934Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "from skmultilearn.dataset import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:10:52.205411Z",
     "start_time": "2020-03-22T15:10:52.041277Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Corel5k',\n",
       " 'bibtex',\n",
       " 'birds',\n",
       " 'delicious',\n",
       " 'emotions',\n",
       " 'enron',\n",
       " 'genbase',\n",
       " 'mediamill',\n",
       " 'medical',\n",
       " 'rcv1subset1',\n",
       " 'rcv1subset2',\n",
       " 'rcv1subset3',\n",
       " 'rcv1subset4',\n",
       " 'rcv1subset5',\n",
       " 'scene',\n",
       " 'tmc2007_500',\n",
       " 'yeast'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://scikit.ml/datasets.html\n",
    "from skmultilearn.dataset import available_data_sets\n",
    "set([x[0] for x in available_data_sets().keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:12:01.400377Z",
     "start_time": "2020-03-22T15:12:01.393492Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:12:18.464015Z",
     "start_time": "2020-03-22T15:12:18.442688Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50\n"
     ]
    }
   ],
   "source": [
    "def predict(mydataloader, y, model):\n",
    "    model.eval()\n",
    "\n",
    "    all_outputs = torch.zeros_like(y)\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(mydataloader):\n",
    "\n",
    "        X_batch, y_batch = sample_batched\n",
    "        outputs = model(X_batch)\n",
    "        B = X_batch.size()[0]\n",
    "        all_outputs[i_batch*B:(i_batch+1)*B]=outputs\n",
    "    return all_outputs\n",
    "\n",
    "\n",
    "def print_thresholds(thresh, nb_classes):\n",
    "    s = ''\n",
    "    for c in range(nb_classes):\n",
    "        s+='%.4f '%thresh[c]\n",
    "    print(\"auto_thresholds\", s)\n",
    "\n",
    "    \n",
    "def compute_instance_F1(gt_y, preds):\n",
    "    num = gt_y*preds\n",
    "    num = 2*np.sum(num, axis=1)\n",
    "    den = np.sum(gt_y, axis=1) + np.sum(preds, axis=1)\n",
    "    return np.mean(num/den)\n",
    "\n",
    "\n",
    "def unit_test(gt_y, preds):\n",
    "    print(\"%.2f\"%compute_instance_F1(gt_y, preds))\n",
    "\n",
    "    \n",
    "def print_scores(gt_y, preds):\n",
    "    print(classification_report(gt_y, preds, digits=3))    \n",
    "    set_accuracy = accuracy_score(gt_y, preds)\n",
    "    print('set acc: %.3f'%(set_accuracy))\n",
    "\n",
    "    \n",
    "def micro_prec_rec_fscore(gt_y, preds):\n",
    "    prec, rec, fscore, support = precision_recall_fscore_support(gt_y, preds, pos_label=1, average='micro')\n",
    "    return prec, rec, fscore\n",
    "\n",
    "\n",
    "def micro_prec_rec_fscore_class(gt_y, preds):\n",
    "    prec, rec, fscore, support = precision_recall_fscore_support(gt_y, preds, pos_label=1, average=None)\n",
    "    return prec, rec, fscore\n",
    "    \n",
    "# def print_scores(gt_y, preds):\n",
    "#     p2, r2, fscore2, support = precision_recall_fscore_support(gt_y, preds, pos_label=1, average='micro')\n",
    "#     p3, r3, fscore3, support = precision_recall_fscore_support(gt_y, preds, pos_label=1, average='macro')\n",
    "#     set_accuracy = accuracy_score(gt_y, preds)\n",
    "#     print('micro: p:%.2f r:%.2f f1:%.2f'%(100.*p2, 100.*r2, 100.*fscore2))\n",
    "#     print('macro: p:%.2f r:%.2f f1:%.2f'%(100.*p3, 100.*r3, 100.*fscore3))\n",
    "#     print('set acc:%.2f instance-F1:%.2f'%(100.*set_accuracy, 100*compute_instance_F1(gt_y, preds)))\n",
    "    \n",
    "def compute_accuracy_from_numpy_tensors(gt_y_numpy, preds_numpy):\n",
    "    acc_per_class = sum(gt_y_numpy==preds_numpy)/len(gt_y_numpy)\n",
    "    \n",
    "    gt_y_numpy_vec = np.reshape(gt_y_numpy, -1)\n",
    "    preds_numpy_vec = np.reshape(preds_numpy, -1)\n",
    "    acc = sum(gt_y_numpy_vec==preds_numpy_vec)/len(gt_y_numpy_vec)\n",
    "    print(\"Acc per class:\", acc_per_class)\n",
    "    print(\"Acc: %.3f\"%acc)\n",
    "\n",
    "\n",
    "dummy_y = np.array([[1,0,0], [0,1,0]])\n",
    "dummy_preds = np.array([[1,0,0], [0,0,0]])\n",
    "unit_test(dummy_y, dummy_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load original subsets: train, test and split or..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:56.778684Z",
     "start_time": "2020-03-22T15:18:56.056965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bibtex:train - exists, not redownloading\n",
      "bibtex:test - exists, not redownloading\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4880, 1836), (4880, 159), (2515, 1836), (2515, 159))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset='bibtex'\n",
    "\n",
    "X_train, y_train, feature_names, label_names = load_dataset('%s'%(dataset), 'train')\n",
    "X_test, y_test, _, _ = load_dataset('%s'%(dataset), 'test')\n",
    "\n",
    "X_train_numpy = X_train.toarray()\n",
    "X_test_numpy = X_test.toarray()\n",
    "y_train_numpy = y_train.toarray()\n",
    "y_test_numpy = y_test.toarray()\n",
    "X_train_numpy.shape, y_train_numpy.shape, X_test_numpy.shape, y_test_numpy.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:17:32.827587Z",
     "start_time": "2020-03-22T15:17:32.777828Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_numpy, X_dev_numpy, y_train_numpy, y_dev_numpy = train_test_split(\n",
    "    X_train_numpy, y_train_numpy, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:17:33.765336Z",
     "start_time": "2020-03-22T15:17:33.758150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3269, 1836),\n",
       " (3269, 159),\n",
       " (1611, 1836),\n",
       " (1611, 159),\n",
       " (2515, 1836),\n",
       " (2515, 159))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_numpy.shape, y_train_numpy.shape, X_dev_numpy.shape, y_dev_numpy.shape, X_test_numpy.shape, y_test_numpy.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:19:04.623272Z",
     "start_time": "2020-03-22T15:19:01.221448Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.savez(\"datasets/%s/%s_train_dev_test.npz\"%(dataset, dataset), \n",
    "#          X_train_numpy=X_train_numpy, \n",
    "#          X_dev_numpy = X_dev_numpy,\n",
    "#          X_test_numpy = X_test_numpy,\n",
    "#          y_train_numpy=y_train_numpy, \n",
    "#          y_dev_numpy = y_dev_numpy,\n",
    "#          y_test_numpy = y_test_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ...load my subsets: train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3269, 1836),\n",
       " (1611, 1836),\n",
       " (2515, 1836),\n",
       " (3269, 159),\n",
       " (1611, 159),\n",
       " (2515, 159))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classes=159\n",
    "dataset = 'bibtex'\n",
    "\n",
    "arr = np.load(\"datasets/%s/%s_train_dev_test.npz\"%(dataset, dataset))\n",
    "# print(arr[\"train_proba\"])\n",
    "\n",
    "X_train_numpy = arr[\"X_train_numpy\"]\n",
    "X_dev_numpy = arr['X_dev_numpy']\n",
    "X_test_numpy = arr['X_test_numpy']\n",
    "y_train_numpy = arr['y_train_numpy']\n",
    "y_dev_numpy = arr['y_dev_numpy']\n",
    "y_test_numpy = arr['y_test_numpy']\n",
    "                \n",
    "X_train_numpy.shape, X_dev_numpy.shape, X_test_numpy.shape, y_train_numpy.shape, y_dev_numpy.shape, y_test_numpy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize for logReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_numpy_normed = scaler.fit_transform(X_train_numpy)\n",
    "X_dev_numpy_normed = scaler.transform(X_dev_numpy)\n",
    "X_test_numpy_normed = scaler.transform(X_test_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ...load logReg-BR probs or..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.load(\"exp/%s/LOGREG_BR_standardization_probs.npz\"%dataset)\n",
    "# print(arr[\"train_proba\"])\n",
    "train_outputs_numpy = arr[\"logreg_train_prob\"]\n",
    "dev_outputs_numpy = arr[\"logreg_dev_prob\"]\n",
    "test_outputs_numpy = arr[\"logreg_test_prob\"]\n",
    "\n",
    "train_outputs_numpy.shape, dev_outputs_numpy.shape, test_outputs_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01152718, 0.02557088, 0.01219484, 0.02094228, 0.01386934,\n",
       "        0.02079769, 0.0433515 , 0.01506147, 0.01252245, 0.02286109],\n",
       "       [0.01104598, 0.02529008, 0.01248954, 0.0220631 , 0.0138653 ,\n",
       "        0.02149845, 0.0417898 , 0.01507798, 0.01258443, 0.0258428 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ... load logReg-LP probs or..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3269, 159), (1611, 159), (2515, 159))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.load(\"exp/%s/logReg_LP_standardization_probs.npz\"%dataset)\n",
    "# arr = np.load(\"exp/%s/logReg_L2_LP_standardization_probs.npz\"%dataset)\n",
    "\n",
    "# print(arr[\"train_proba\"])\n",
    "train_outputs_numpy = arr[\"logReg_train_prob\"]\n",
    "dev_outputs_numpy = arr[\"logReg_dev_prob\"]\n",
    "test_outputs_numpy = arr[\"logReg_test_prob\"]\n",
    "\n",
    "train_outputs_numpy.shape, dev_outputs_numpy.shape, test_outputs_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.2104302250516185e-11, 0.9999999512801558)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(test_outputs_numpy), np.max(test_outputs_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ...load SVM-BR probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.load(\"exp/%s/SVM_BR_probs.npz\"%dataset)\n",
    "# print(arr[\"train_proba\"])\n",
    "\n",
    "train_outputs_numpy = arr[\"svm_train_prob\"]\n",
    "dev_outputs_numpy = arr[\"svm_dev_prob\"]\n",
    "test_outputs_numpy = arr[\"svm_test_prob\"]\n",
    "\n",
    "train_outputs_numpy.shape, dev_outputs_numpy.shape, test_outputs_numpy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T12:23:10.598946Z",
     "start_time": "2020-03-18T12:23:10.276964Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pth = torch.tensor(X_train_numpy, dtype=torch.float).to(device)\n",
    "X_dev_pth = torch.tensor(X_dev_numpy, dtype=torch.float).to(device)\n",
    "X_test_pth = torch.tensor(X_test_numpy, dtype=torch.float).to(device)\n",
    "\n",
    "y_train_pth = torch.tensor(y_train_numpy, dtype=torch.float).to(device)\n",
    "y_dev_pth = torch.tensor(y_dev_numpy, dtype=torch.float).to(device)\n",
    "y_test_pth = torch.tensor(y_test_numpy, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T12:23:11.182313Z",
     "start_time": "2020-03-18T12:23:11.178559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3269, 1836]),\n",
       " torch.Size([3269, 159]),\n",
       " torch.Size([2515, 1836]),\n",
       " torch.Size([2515, 159]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pth.size(), y_train_pth.size(), X_test_pth.size(), y_test_pth.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Neural net to make predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T10:46:49.791482Z",
     "start_time": "2020-03-18T10:46:49.782585Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_dim = X_train_numpy.shape[1]\n",
    "hidden_dim = 200\n",
    "# output_dim = len(np.unique(y_train.rows))\n",
    "output_dim = y_train_numpy.shape[1]\n",
    "nb_classes = output_dim\n",
    "\n",
    "input_dim, hidden_dim, output_dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T21:53:31.381940Z",
     "start_time": "2020-03-17T21:53:31.374889Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_dataset = data.TensorDataset(X_train_pth, y_train_pth) # create your datset\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "train_dataloader_noShuffle = data.DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "dev_dataset = data.TensorDataset(X_dev_pth, y_dev_pth) # create your datset\n",
    "dev_dataloader = data.DataLoader(dev_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = data.TensorDataset(X_test_pth, y_test_pth) # create your datset\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T21:51:43.133364Z",
     "start_time": "2020-03-17T21:51:43.123355Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MultiClassClassifierModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=output_dim,\n",
    "            dropout=0.5,\n",
    "    ):\n",
    "        super(MultiClassClassifierModule, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = F.relu(self.hidden(X))\n",
    "        X = self.dropout(X)\n",
    "        X = self.output(X)\n",
    "#         X = F.softmax(X, dim=-1)\n",
    "        X = torch.sigmoid(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:01:49.271616Z",
     "start_time": "2020-03-17T22:01:49.255351Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = MultiClassClassifierModule(input_dim, hidden_dim, output_dim)\n",
    "model.to(device)\n",
    "\n",
    "criterion = torch.nn.BCELoss(reduction=\"mean\")\n",
    "# criterion = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:02:00.249668Z",
     "start_time": "2020-03-17T22:01:49.945317Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    debut = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "        \n",
    "        X_batch, y_batch = sample_batched\n",
    "        \n",
    "#         print(i_batch, X_batch.size(), y_batch.size())\n",
    "    \n",
    "        # Forward pass\n",
    "        # inputs:  predictions_tensor\n",
    "        outputs = model(X_batch)\n",
    "#         print(outputs.size())\n",
    "    #     if epoch % 10 == 0:\n",
    "    #         print(outputs[-1])\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss)\n",
    "    \n",
    "    duree_epoch = time.time() - debut\n",
    "\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}, Duration: {:.1f} s' \n",
    "           .format(epoch+1, num_epochs, loss, duree_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:02:01.536289Z",
     "start_time": "2020-03-17T22:02:01.292051Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:02:04.465633Z",
     "start_time": "2020-03-17T22:02:04.093508Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_outputs = predict(train_dataloader, y_train_pth, model)\n",
    "dev_outputs = predict(dev_dataloader, y_dev_pth, model)\n",
    "test_outputs = predict(test_dataloader, y_test_pth, model)\n",
    "\n",
    "train_outputs_numpy = train_outputs.clone().detach().cpu().numpy()\n",
    "dev_outputs_numpy = dev_outputs.clone().detach().cpu().numpy()\n",
    "test_outputs_numpy = test_outputs.clone().detach().cpu().numpy()\n",
    "# y_test[:5], test_outputs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "# Static thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T10:44:00.673092Z",
     "start_time": "2020-03-18T10:44:00.666703Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "static_thresh = 0.1\n",
    "train_pred = train_outputs_numpy>static_thresh\n",
    "dev_pred = dev_outputs_numpy>static_thresh\n",
    "test_pred = test_outputs_numpy>static_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T10:44:01.527368Z",
     "start_time": "2020-03-18T10:44:01.383271Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.962     0.980        26\n",
      "           1      1.000     0.937     0.967        63\n",
      "           2      0.960     0.889     0.923        27\n",
      "           3      1.000     1.000     1.000        48\n",
      "           4      1.000     1.000     1.000        23\n",
      "           5      1.000     1.000     1.000        45\n",
      "           6      1.000     0.924     0.960        92\n",
      "           7      1.000     0.925     0.961        40\n",
      "           8      0.960     0.960     0.960        25\n",
      "           9      1.000     1.000     1.000        65\n",
      "          10      1.000     1.000     1.000       131\n",
      "          11      0.977     0.935     0.956        46\n",
      "          12      1.000     0.929     0.963        28\n",
      "          13      1.000     1.000     1.000        61\n",
      "          14      1.000     1.000     1.000       215\n",
      "          15      0.976     0.872     0.921        47\n",
      "          16      1.000     1.000     1.000        43\n",
      "          17      0.964     1.000     0.982        27\n",
      "          18      0.966     0.966     0.966        58\n",
      "          19      0.979     0.940     0.959        50\n",
      "          20      0.919     0.944     0.932        36\n",
      "          21      0.930     0.952     0.941        42\n",
      "          22      1.000     1.000     1.000        58\n",
      "          23      1.000     1.000     1.000        39\n",
      "          24      0.971     1.000     0.985        33\n",
      "          25      0.944     1.000     0.971        34\n",
      "          26      1.000     1.000     1.000        41\n",
      "          27      1.000     1.000     1.000        52\n",
      "          28      1.000     0.950     0.974        20\n",
      "          29      1.000     0.897     0.945        29\n",
      "          30      1.000     0.860     0.925        43\n",
      "          31      1.000     1.000     1.000        25\n",
      "          32      1.000     1.000     1.000        36\n",
      "          33      1.000     0.833     0.909        30\n",
      "          34      0.926     0.781     0.847        32\n",
      "          35      1.000     1.000     1.000        40\n",
      "          36      0.971     0.981     0.976       103\n",
      "          37      1.000     0.826     0.905        23\n",
      "          38      1.000     1.000     1.000        34\n",
      "          39      0.923     0.923     0.923        26\n",
      "          40      1.000     1.000     1.000        24\n",
      "          41      1.000     1.000     1.000        65\n",
      "          42      0.979     1.000     0.989        46\n",
      "          43      1.000     0.919     0.958        37\n",
      "          44      1.000     1.000     1.000        88\n",
      "          45      1.000     1.000     1.000        25\n",
      "          46      1.000     0.923     0.960        26\n",
      "          47      1.000     1.000     1.000        24\n",
      "          48      0.974     0.927     0.950        41\n",
      "          49      1.000     1.000     1.000        49\n",
      "          50      1.000     1.000     1.000        24\n",
      "          51      0.964     0.964     0.964        28\n",
      "          52      0.953     0.976     0.964       125\n",
      "          53      1.000     0.875     0.933        24\n",
      "          54      0.943     0.909     0.926        55\n",
      "          55      1.000     0.868     0.930        38\n",
      "          56      1.000     0.950     0.974        20\n",
      "          57      1.000     1.000     1.000        27\n",
      "          58      1.000     1.000     1.000        20\n",
      "          59      0.955     0.840     0.894        25\n",
      "          60      0.935     0.967     0.951        30\n",
      "          61      1.000     1.000     1.000        51\n",
      "          62      1.000     1.000     1.000        30\n",
      "          63      1.000     1.000     1.000       124\n",
      "          64      1.000     1.000     1.000        29\n",
      "          65      1.000     1.000     1.000        37\n",
      "          66      0.985     0.942     0.963        69\n",
      "          67      1.000     0.839     0.912        31\n",
      "          68      1.000     1.000     1.000        27\n",
      "          69      1.000     1.000     1.000        25\n",
      "          70      0.986     0.932     0.958        74\n",
      "          71      1.000     0.927     0.962        41\n",
      "          72      1.000     0.939     0.969        33\n",
      "          73      1.000     0.941     0.970        34\n",
      "          74      1.000     1.000     1.000        27\n",
      "          75      0.993     0.979     0.986       144\n",
      "          76      1.000     1.000     1.000        32\n",
      "          77      1.000     1.000     1.000        48\n",
      "          78      1.000     0.955     0.977        22\n",
      "          79      0.926     0.962     0.943        26\n",
      "          80      0.978     0.917     0.946        48\n",
      "          81      1.000     0.984     0.992        63\n",
      "          82      0.870     1.000     0.930        20\n",
      "          83      1.000     0.988     0.994        85\n",
      "          84      0.966     1.000     0.982        56\n",
      "          85      1.000     0.945     0.972        55\n",
      "          86      0.974     1.000     0.987        37\n",
      "          87      0.966     0.824     0.889        34\n",
      "          88      0.982     0.991     0.986       109\n",
      "          89      0.973     1.000     0.986        36\n",
      "          90      1.000     1.000     1.000        38\n",
      "          91      1.000     1.000     1.000        28\n",
      "          92      1.000     1.000     1.000        25\n",
      "          93      0.914     0.928     0.921        69\n",
      "          94      1.000     0.964     0.982        28\n",
      "          95      1.000     0.909     0.952        22\n",
      "          96      0.987     0.925     0.955        80\n",
      "          97      0.979     0.969     0.974        97\n",
      "          98      1.000     1.000     1.000        23\n",
      "          99      1.000     1.000     1.000        24\n",
      "         100      0.962     0.980     0.971        51\n",
      "         101      0.846     1.000     0.917        44\n",
      "         102      1.000     1.000     1.000        26\n",
      "         103      0.962     0.926     0.943        27\n",
      "         104      0.940     0.898     0.919        88\n",
      "         105      0.964     0.931     0.947        29\n",
      "         106      0.966     0.966     0.966        29\n",
      "         107      1.000     1.000     1.000        40\n",
      "         108      0.970     1.000     0.985        32\n",
      "         109      1.000     0.913     0.955        23\n",
      "         110      1.000     0.967     0.983        30\n",
      "         111      1.000     1.000     1.000        60\n",
      "         112      1.000     1.000     1.000        24\n",
      "         113      1.000     1.000     1.000        63\n",
      "         114      1.000     1.000     1.000        42\n",
      "         115      1.000     0.905     0.950        21\n",
      "         116      1.000     0.957     0.978        23\n",
      "         117      1.000     0.987     0.993        76\n",
      "         118      0.974     0.927     0.950        41\n",
      "         119      1.000     1.000     1.000        47\n",
      "         120      1.000     0.964     0.982        28\n",
      "         121      0.972     0.875     0.921        40\n",
      "         122      0.966     0.933     0.949        90\n",
      "         123      0.964     0.964     0.964        28\n",
      "         124      0.986     0.910     0.947        78\n",
      "         125      1.000     1.000     1.000        22\n",
      "         126      1.000     1.000     1.000        29\n",
      "         127      1.000     1.000     1.000        22\n",
      "         128      0.938     0.833     0.882        36\n",
      "         129      0.970     0.916     0.942       107\n",
      "         130      0.963     0.963     0.963        27\n",
      "         131      0.768     1.000     0.869       195\n",
      "         132      1.000     1.000     1.000        23\n",
      "         133      0.920     0.920     0.920        25\n",
      "         134      0.692     1.000     0.818       458\n",
      "         135      0.959     1.000     0.979        47\n",
      "         136      1.000     0.826     0.905        23\n",
      "         137      0.939     0.939     0.939        33\n",
      "         138      1.000     1.000     1.000        59\n",
      "         139      0.939     0.902     0.920        51\n",
      "         140      0.966     0.933     0.949        30\n",
      "         141      0.988     0.976     0.982        85\n",
      "         142      1.000     1.000     1.000        34\n",
      "         143      1.000     1.000     1.000        57\n",
      "         144      1.000     1.000     1.000        64\n",
      "         145      1.000     1.000     1.000        31\n",
      "         146      1.000     1.000     1.000        64\n",
      "         147      1.000     1.000     1.000        26\n",
      "         148      1.000     1.000     1.000        27\n",
      "         149      1.000     1.000     1.000        54\n",
      "         150      1.000     1.000     1.000        36\n",
      "         151      1.000     1.000     1.000        47\n",
      "         152      1.000     0.843     0.915        51\n",
      "         153      1.000     1.000     1.000        44\n",
      "         154      1.000     1.000     1.000        32\n",
      "         155      0.976     0.952     0.964        42\n",
      "         156      0.989     0.928     0.957        97\n",
      "         157      0.971     0.791     0.872        43\n",
      "         158      0.963     0.867     0.912        30\n",
      "\n",
      "   micro avg      0.953     0.966     0.959      7789\n",
      "   macro avg      0.981     0.960     0.969      7789\n",
      "weighted avg      0.962     0.966     0.961      7789\n",
      " samples avg      0.955     0.972     0.960      7789\n",
      "\n",
      "set acc: 0.919\n",
      "dev\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        13\n",
      "           1      0.286     0.091     0.138        22\n",
      "           2      0.000     0.000     0.000        24\n",
      "           3      0.526     0.435     0.476        23\n",
      "           4      0.000     0.000     0.000        10\n",
      "           5      0.409     0.409     0.409        22\n",
      "           6      0.500     0.114     0.186        35\n",
      "           7      0.000     0.000     0.000        20\n",
      "           8      1.000     0.182     0.308        11\n",
      "           9      0.667     0.640     0.653        25\n",
      "          10      0.881     0.981     0.929        53\n",
      "          11      0.583     0.412     0.483        17\n",
      "          12      0.000     0.000     0.000        10\n",
      "          13      1.000     1.000     1.000        42\n",
      "          14      0.936     0.911     0.923       112\n",
      "          15      0.368     0.350     0.359        20\n",
      "          16      0.733     0.367     0.489        30\n",
      "          17      0.000     0.000     0.000        13\n",
      "          18      0.750     0.387     0.511        31\n",
      "          19      0.000     0.000     0.000        21\n",
      "          20      0.000     0.000     0.000        16\n",
      "          21      0.400     0.083     0.138        24\n",
      "          22      0.400     0.273     0.324        22\n",
      "          23      0.800     0.222     0.348        18\n",
      "          24      0.000     0.000     0.000        16\n",
      "          25      0.000     0.000     0.000         8\n",
      "          26      0.583     0.538     0.560        13\n",
      "          27      0.250     0.034     0.061        29\n",
      "          28      0.000     0.000     0.000         9\n",
      "          29      0.250     0.083     0.125        12\n",
      "          30      0.333     0.125     0.182         8\n",
      "          31      0.750     1.000     0.857        12\n",
      "          32      0.000     0.000     0.000        13\n",
      "          33      0.000     0.000     0.000        13\n",
      "          34      1.000     0.100     0.182        10\n",
      "          35      0.870     0.870     0.870        23\n",
      "          36      0.417     0.106     0.169        47\n",
      "          37      0.000     0.000     0.000         9\n",
      "          38      0.750     0.286     0.414        21\n",
      "          39      0.333     0.077     0.125        13\n",
      "          40      0.600     0.375     0.462         8\n",
      "          41      0.500     0.018     0.034        57\n",
      "          42      0.667     0.160     0.258        25\n",
      "          43      0.400     0.105     0.167        19\n",
      "          44      0.754     0.920     0.829        50\n",
      "          45      0.500     0.200     0.286        10\n",
      "          46      0.000     0.000     0.000         9\n",
      "          47      0.000     0.000     0.000        27\n",
      "          48      0.273     0.143     0.187        21\n",
      "          49      0.680     0.810     0.739        21\n",
      "          50      0.000     0.000     0.000        12\n",
      "          51      0.500     0.100     0.167        20\n",
      "          52      0.711     0.457     0.557        70\n",
      "          53      0.667     0.182     0.286        11\n",
      "          54      0.545     0.222     0.316        27\n",
      "          55      0.333     0.056     0.095        18\n",
      "          56      0.000     0.000     0.000         8\n",
      "          57      1.000     0.182     0.308        11\n",
      "          58      0.000     0.000     0.000        12\n",
      "          59      1.000     0.200     0.333        10\n",
      "          60      0.000     0.000     0.000        13\n",
      "          61      0.542     0.520     0.531        25\n",
      "          62      0.889     0.533     0.667        15\n",
      "          63      0.612     0.929     0.738        56\n",
      "          64      0.538     0.412     0.467        17\n",
      "          65      0.429     0.214     0.286        14\n",
      "          66      0.571     0.089     0.154        45\n",
      "          67      0.250     0.056     0.091        18\n",
      "          68      0.250     0.071     0.111        14\n",
      "          69      0.500     0.500     0.500        12\n",
      "          70      0.200     0.029     0.051        34\n",
      "          71      0.429     0.200     0.273        15\n",
      "          72      0.000     0.000     0.000         9\n",
      "          73      0.000     0.000     0.000        16\n",
      "          74      0.385     0.833     0.526         6\n",
      "          75      0.556     0.250     0.345        60\n",
      "          76      0.643     0.750     0.692        12\n",
      "          77      0.625     0.238     0.345        21\n",
      "          78      0.571     0.308     0.400        13\n",
      "          79      0.625     0.312     0.417        16\n",
      "          80      0.600     0.214     0.316        28\n",
      "          81      0.625     0.577     0.600        26\n",
      "          82      0.917     0.917     0.917        12\n",
      "          83      0.735     0.556     0.633        45\n",
      "          84      1.000     0.097     0.176        31\n",
      "          85      0.538     0.250     0.341        28\n",
      "          86      0.722     0.765     0.743        17\n",
      "          87      0.333     0.067     0.111        15\n",
      "          88      0.333     0.019     0.036        53\n",
      "          89      0.000     0.000     0.000        19\n",
      "          90      0.000     0.000     0.000        21\n",
      "          91      0.000     0.000     0.000        24\n",
      "          92      0.000     0.000     0.000        11\n",
      "          93      0.100     0.048     0.065        42\n",
      "          94      0.000     0.000     0.000        13\n",
      "          95      0.000     0.000     0.000        13\n",
      "          96      0.250     0.030     0.054        33\n",
      "          97      0.625     0.256     0.364        39\n",
      "          98      0.000     0.000     0.000        15\n",
      "          99      0.000     0.000     0.000        15\n",
      "         100      0.304     0.241     0.269        29\n",
      "         101      0.727     0.941     0.821        17\n",
      "         102      0.667     0.200     0.308        10\n",
      "         103      0.000     0.000     0.000        19\n",
      "         104      0.538     0.389     0.452        54\n",
      "         105      0.500     0.071     0.125        14\n",
      "         106      0.000     0.000     0.000        19\n",
      "         107      0.000     0.000     0.000        25\n",
      "         108      0.833     0.294     0.435        17\n",
      "         109      0.000     0.000     0.000        15\n",
      "         110      0.000     0.000     0.000        21\n",
      "         111      0.419     0.600     0.493        30\n",
      "         112      1.000     0.500     0.667        12\n",
      "         113      0.600     0.321     0.419        28\n",
      "         114      0.000     0.000     0.000        10\n",
      "         115      0.750     0.273     0.400        11\n",
      "         116      0.000     0.000     0.000        14\n",
      "         117      0.792     0.388     0.521        49\n",
      "         118      0.000     0.000     0.000        16\n",
      "         119      0.571     0.190     0.286        21\n",
      "         120      0.833     0.278     0.417        18\n",
      "         121      0.250     0.250     0.250        16\n",
      "         122      0.520     0.260     0.347        50\n",
      "         123      0.000     0.000     0.000        15\n",
      "         124      0.292     0.171     0.215        41\n",
      "         125      0.750     0.231     0.353        13\n",
      "         126      0.000     0.000     0.000        22\n",
      "         127      0.000     0.000     0.000        13\n",
      "         128      0.000     0.000     0.000         7\n",
      "         129      0.000     0.000     0.000        44\n",
      "         130      1.000     0.100     0.182        10\n",
      "         131      0.362     0.681     0.472        94\n",
      "         132      0.667     0.118     0.200        17\n",
      "         133      0.000     0.000     0.000         7\n",
      "         134      0.194     1.000     0.325       233\n",
      "         135      0.000     0.000     0.000        18\n",
      "         136      0.000     0.000     0.000         7\n",
      "         137      0.000     0.000     0.000        11\n",
      "         138      0.000     0.000     0.000        31\n",
      "         139      0.846     0.407     0.550        27\n",
      "         140      0.000     0.000     0.000        13\n",
      "         141      0.000     0.000     0.000        41\n",
      "         142      0.100     0.050     0.067        20\n",
      "         143      0.750     0.107     0.188        28\n",
      "         144      0.444     0.118     0.186        34\n",
      "         145      0.000     0.000     0.000        19\n",
      "         146      0.222     0.216     0.219        37\n",
      "         147      0.333     0.250     0.286         8\n",
      "         148      0.462     0.375     0.414        16\n",
      "         149      0.464     0.419     0.441        31\n",
      "         150      0.250     0.308     0.276        13\n",
      "         151      0.615     0.421     0.500        19\n",
      "         152      0.000     0.000     0.000        24\n",
      "         153      1.000     0.059     0.111        17\n",
      "         154      0.412     0.538     0.467        13\n",
      "         155      1.000     0.172     0.294        29\n",
      "         156      0.412     0.286     0.337        49\n",
      "         157      0.000     0.000     0.000        18\n",
      "         158      0.250     0.167     0.200         6\n",
      "\n",
      "   micro avg      0.412     0.323     0.362      3827\n",
      "   macro avg      0.380     0.224     0.250      3827\n",
      "weighted avg      0.422     0.323     0.307      3827\n",
      " samples avg      0.402     0.377     0.356      3827\n",
      "\n",
      "set acc: 0.135\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.033     0.065        30\n",
      "           1      0.214     0.064     0.098        47\n",
      "           2      0.000     0.000     0.000        20\n",
      "           3      0.650     0.394     0.491        33\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.652     0.484     0.556        31\n",
      "           6      0.389     0.090     0.146        78\n",
      "           7      0.000     0.000     0.000        22\n",
      "           8      0.500     0.160     0.242        25\n",
      "           9      0.619     0.578     0.598        45\n",
      "          10      0.844     0.991     0.911       109\n",
      "          11      0.565     0.433     0.491        30\n",
      "          12      0.000     0.000     0.000        23\n",
      "          13      1.000     0.955     0.977        44\n",
      "          14      0.974     0.944     0.958       195\n",
      "          15      0.192     0.172     0.182        29\n",
      "          16      0.778     0.467     0.583        30\n",
      "          17      0.000     0.000     0.000        21\n",
      "          18      0.562     0.214     0.310        42\n",
      "          19      0.000     0.000     0.000        26\n",
      "          20      0.200     0.091     0.125        22\n",
      "          21      0.750     0.120     0.207        50\n",
      "          22      0.448     0.271     0.338        48\n",
      "          23      0.600     0.158     0.250        19\n",
      "          24      0.000     0.000     0.000        19\n",
      "          25      0.000     0.000     0.000        17\n",
      "          26      0.765     0.361     0.491        36\n",
      "          27      0.667     0.111     0.190        36\n",
      "          28      0.667     0.091     0.160        22\n",
      "          29      0.333     0.050     0.087        20\n",
      "          30      0.429     0.115     0.182        26\n",
      "          31      0.667     0.857     0.750        14\n",
      "          32      0.000     0.000     0.000        23\n",
      "          33      0.000     0.000     0.000        21\n",
      "          34      0.000     0.000     0.000        19\n",
      "          35      0.957     0.647     0.772        34\n",
      "          36      0.583     0.084     0.147        83\n",
      "          37      0.100     0.043     0.061        23\n",
      "          38      1.000     0.484     0.652        31\n",
      "          39      0.167     0.042     0.067        24\n",
      "          40      0.714     0.500     0.588        20\n",
      "          41      1.000     0.021     0.042        47\n",
      "          42      0.400     0.045     0.082        44\n",
      "          43      0.444     0.133     0.205        30\n",
      "          44      0.797     0.895     0.843        57\n",
      "          45      1.000     0.263     0.417        19\n",
      "          46      0.000     0.000     0.000        17\n",
      "          47      0.000     0.000     0.000        21\n",
      "          48      0.778     0.333     0.467        42\n",
      "          49      0.673     0.917     0.776        36\n",
      "          50      0.000     0.000     0.000        17\n",
      "          51      0.125     0.043     0.065        23\n",
      "          52      0.575     0.465     0.514        99\n",
      "          53      0.000     0.000     0.000        19\n",
      "          54      0.762     0.314     0.444        51\n",
      "          55      0.500     0.037     0.069        27\n",
      "          56      0.500     0.077     0.133        26\n",
      "          57      1.000     0.056     0.105        18\n",
      "          58      0.250     0.040     0.069        25\n",
      "          59      1.000     0.158     0.273        19\n",
      "          60      0.000     0.000     0.000        21\n",
      "          61      0.600     0.480     0.533        25\n",
      "          62      0.938     0.789     0.857        19\n",
      "          63      0.562     0.900     0.692        70\n",
      "          64      0.462     0.353     0.400        17\n",
      "          65      0.333     0.091     0.143        22\n",
      "          66      0.444     0.070     0.121        57\n",
      "          67      0.250     0.037     0.065        27\n",
      "          68      0.250     0.100     0.143        20\n",
      "          69      0.533     0.471     0.500        17\n",
      "          70      0.143     0.013     0.024        75\n",
      "          71      0.412     0.269     0.326        26\n",
      "          72      0.500     0.027     0.051        37\n",
      "          73      0.000     0.000     0.000        32\n",
      "          74      0.346     0.429     0.383        21\n",
      "          75      0.536     0.291     0.377       103\n",
      "          76      0.500     0.250     0.333        16\n",
      "          77      0.778     0.226     0.350        31\n",
      "          78      0.750     0.273     0.400        22\n",
      "          79      0.333     0.455     0.385        11\n",
      "          80      0.286     0.043     0.075        46\n",
      "          81      0.600     0.682     0.638        44\n",
      "          82      0.833     0.870     0.851        23\n",
      "          83      0.682     0.500     0.577        60\n",
      "          84      1.000     0.021     0.042        47\n",
      "          85      0.647     0.234     0.344        47\n",
      "          86      0.455     0.833     0.588        18\n",
      "          87      0.200     0.042     0.069        24\n",
      "          88      0.333     0.041     0.072        74\n",
      "          89      0.400     0.061     0.105        33\n",
      "          90      0.000     0.000     0.000        38\n",
      "          91      0.000     0.000     0.000        16\n",
      "          92      0.000     0.000     0.000        19\n",
      "          93      0.194     0.106     0.137        66\n",
      "          94      0.000     0.000     0.000        20\n",
      "          95      0.500     0.087     0.148        23\n",
      "          96      0.143     0.016     0.029        61\n",
      "          97      0.458     0.162     0.239        68\n",
      "          98      0.000     0.000     0.000        20\n",
      "          99      1.000     0.067     0.125        30\n",
      "         100      0.174     0.157     0.165        51\n",
      "         101      0.689     0.816     0.747        38\n",
      "         102      0.909     0.385     0.541        26\n",
      "         103      0.250     0.034     0.061        29\n",
      "         104      0.491     0.286     0.361        91\n",
      "         105      1.000     0.042     0.080        24\n",
      "         106      0.000     0.000     0.000        24\n",
      "         107      0.000     0.000     0.000        43\n",
      "         108      0.000     0.000     0.000        23\n",
      "         109      1.000     0.050     0.095        20\n",
      "         110      0.500     0.065     0.114        31\n",
      "         111      0.362     0.459     0.405        37\n",
      "         112      0.909     0.556     0.690        18\n",
      "         113      0.684     0.361     0.473        36\n",
      "         114      0.000     0.000     0.000        18\n",
      "         115      1.000     0.042     0.080        24\n",
      "         116      0.000     0.000     0.000        16\n",
      "         117      0.700     0.375     0.488        56\n",
      "         118      0.000     0.000     0.000        36\n",
      "         119      0.286     0.047     0.080        43\n",
      "         120      0.000     0.000     0.000        31\n",
      "         121      0.269     0.280     0.275        25\n",
      "         122      0.433     0.312     0.362        93\n",
      "         123      0.000     0.000     0.000        25\n",
      "         124      0.300     0.155     0.205        58\n",
      "         125      0.714     0.263     0.385        19\n",
      "         126      1.000     0.023     0.045        43\n",
      "         127      0.000     0.000     0.000        19\n",
      "         128      0.000     0.000     0.000        15\n",
      "         129      0.045     0.018     0.025        57\n",
      "         130      0.000     0.000     0.000        24\n",
      "         131      0.420     0.753     0.540       154\n",
      "         132      1.000     0.105     0.190        19\n",
      "         133      0.500     0.043     0.080        23\n",
      "         134      0.191     0.997     0.320       351\n",
      "         135      0.000     0.000     0.000        29\n",
      "         136      0.000     0.000     0.000        21\n",
      "         137      0.000     0.000     0.000        19\n",
      "         138      1.000     0.019     0.038        52\n",
      "         139      0.880     0.407     0.557        54\n",
      "         140      0.333     0.143     0.200        21\n",
      "         141      0.200     0.013     0.024        80\n",
      "         142      0.500     0.188     0.273        32\n",
      "         143      0.375     0.067     0.113        45\n",
      "         144      0.500     0.156     0.237        45\n",
      "         145      0.000     0.000     0.000        25\n",
      "         146      0.350     0.280     0.311        50\n",
      "         147      0.250     0.107     0.150        28\n",
      "         148      0.286     0.200     0.235        20\n",
      "         149      0.316     0.273     0.293        44\n",
      "         150      0.400     0.333     0.364        24\n",
      "         151      0.429     0.375     0.400        16\n",
      "         152      0.200     0.018     0.033        56\n",
      "         153      0.333     0.054     0.093        37\n",
      "         154      0.611     0.458     0.524        24\n",
      "         155      0.600     0.075     0.133        40\n",
      "         156      0.250     0.163     0.197        86\n",
      "         157      0.118     0.050     0.070        40\n",
      "         158      0.786     0.458     0.579        24\n",
      "\n",
      "   micro avg      0.406     0.303     0.347      6146\n",
      "   macro avg      0.420     0.211     0.239      6146\n",
      "weighted avg      0.449     0.303     0.290      6146\n",
      " samples avg      0.394     0.359     0.343      6146\n",
      "\n",
      "set acc: 0.132\n"
     ]
    }
   ],
   "source": [
    "print(\"train\")\n",
    "print_scores(y_train_numpy, train_pred)\n",
    "print(\"dev\")\n",
    "print_scores(y_dev_numpy, dev_pred)\n",
    "print(\"test\")\n",
    "print_scores(y_test_numpy, test_pred)\n",
    "# compute_accuracy_from_numpy_tensors(y_test_numpy, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGLThresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## SurrogateHeaviside definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T10:45:03.059829Z",
     "start_time": "2020-03-18T10:45:03.035177Z"
    }
   },
   "outputs": [],
   "source": [
    "# device = 'cpu'\n",
    "\n",
    "class SurrogateHeaviside(torch.autograd.Function):\n",
    "    \n",
    "    # Activation function with surrogate gradient\n",
    "#     sigma = 100.0\n",
    "\n",
    "    @staticmethod \n",
    "    def forward(ctx, input, sigma):\n",
    "        \n",
    "        output = torch.zeros_like(input)\n",
    "        output[input > 0] = 1.0\n",
    "        ctx.save_for_backward(input, sigma)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, sigma = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        # approximation of the gradient using sigmoid function\n",
    "        grad = grad_input*torch.sigmoid(sigma*input)*torch.sigmoid(-sigma*input)\n",
    "        \n",
    "        grad_sigma = grad_input*input*torch.sigmoid(sigma*input)*torch.sigmoid(-sigma*input)\n",
    "        \n",
    "        return grad, grad_sigma\n",
    "\n",
    "threshold_fn = SurrogateHeaviside.apply\n",
    "\n",
    "\n",
    "class ThresholdModel(nn.Module):\n",
    "    def __init__(self, threshold_fn, t=0.5, sigma=100., nb_classes=10):\n",
    "        super(ThresholdModel, self).__init__()\n",
    "        \n",
    "        # define nb_classes seuils differents, initialisés à 0.5\n",
    "\n",
    "#         self.dense = torch.nn.Linear(10, 10)\n",
    "\n",
    "        self.thresh = torch.nn.Parameter(t*torch.ones(nb_classes), requires_grad=True)\n",
    "        self.sigma = torch.nn.Parameter(sigma*torch.ones(nb_classes), requires_grad=True)\n",
    "        self.threshold_fn = threshold_fn\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.threshold_fn(x.to(device, dtype=torch.float)-self.thresh.to(device, dtype=torch.float), \n",
    "                                self.sigma.to(device, dtype=torch.float))\n",
    "#         out = out.clamp_(min=0.01, max=0.99)\n",
    "        # out = self.dense(x.to(device, dtype=torch.float))\n",
    "        # out = F.sigmoid(out)\n",
    "        # out = self.threshold_fn(out-F.sigmoid(self.thresh.to(device, dtype=torch.float)))\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def clamp(self):\n",
    "        \n",
    "        self.thresh.data.clamp_(min=0., max=1.)\n",
    "\n",
    "        \n",
    "def F1_loss_objective(binarized_output, y_true):\n",
    "    # let's first convert binary vector prob into logits\n",
    "#     prob = torch.clamp(prob, 1.e-12, 0.9999999)\n",
    "    \n",
    "#     average = 'macro'\n",
    "    average = 'micro'\n",
    "    epsilon = torch.tensor(1e-12)\n",
    "    \n",
    "    if average == 'micro':\n",
    "        y_true = torch.flatten(y_true)\n",
    "        binarized_output = torch.flatten(binarized_output)\n",
    "        \n",
    "    true_positives = torch.sum(y_true * binarized_output, dim=0)\n",
    "    predicted_positives = torch.sum(binarized_output, dim=0)\n",
    "    positives = torch.sum(y_true, dim=0)\n",
    "    precision = true_positives / (predicted_positives + epsilon)\n",
    "    recall = true_positives / (positives + epsilon)\n",
    "\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + epsilon))\n",
    "#     return precision, recall, f1\n",
    "    return - f1.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## numerical application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T10:46:05.704734Z",
     "start_time": "2020-03-18T10:46:02.571789Z"
    }
   },
   "outputs": [],
   "source": [
    "pth_train_probs = torch.tensor(train_outputs_numpy, dtype=torch.float).to(device)\n",
    "pth_dev_probs = torch.tensor(dev_outputs_numpy, dtype=torch.float).to(device)\n",
    "pth_test_probs = torch.tensor(test_outputs_numpy, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "pth_train_gt = y_train_pth.to(device, dtype=torch.float)\n",
    "pth_dev_gt = y_dev_pth.to(device, dtype=torch.float)\n",
    "pth_test_gt = y_test_pth.to(device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T10:47:43.655861Z",
     "start_time": "2020-03-18T10:47:43.635014Z"
    }
   },
   "outputs": [],
   "source": [
    "THRESHmodel = ThresholdModel(threshold_fn=threshold_fn, t=0.1, sigma=140., nb_classes=nb_classes)\n",
    "THRESHmodel = THRESHmodel.to(device, dtype=torch.float)\n",
    "# criterion = torch.nn.BCELoss(reduction=\"mean\")\n",
    "criterion = F1_loss_objective\n",
    "\n",
    "learning_rate = 1e-3\n",
    "# THRESHoptimizer = torch.optim.Adam(THRESHmodel.parameters(), lr=learning_rate)\n",
    "\n",
    "# # # learn only the thresholds:\n",
    "THRESHoptimizer = torch.optim.Adam([\n",
    "                {'params': THRESHmodel.thresh}\n",
    "            ], lr=learning_rate)\n",
    "\n",
    "# # learn the thresholds and sigma:\n",
    "# THRESHoptimizer = torch.optim.Adam([\n",
    "#                 {'params': THRESHmodel.thresh},\n",
    "#                 {'params': THRESHmodel.sigma, 'lr': 1.}\n",
    "#             ], lr=learning_rate)\n",
    "\n",
    "# scheduler = MultiStepLR(THRESHoptimizer, milestones=[180], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T10:47:46.353912Z",
     "start_time": "2020-03-18T10:47:45.415707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: -0.3623, Duration: 0.0 s\n",
      "threshs[:20] tensor([0.1010, 0.1010, 0.1010, 0.0990, 0.0990, 0.0990, 0.1010, 0.0990, 0.0990,\n",
      "        0.0990, 0.1010, 0.1010, 0.1010, 0.1009, 0.0990, 0.1010, 0.0990, 0.1010,\n",
      "        0.0990, 0.1010], grad_fn=<SliceBackward>)\n",
      "Epoch [2/300], Loss: -0.3643, Duration: 0.0 s\n",
      "Epoch [3/300], Loss: -0.3665, Duration: 0.0 s\n",
      "Epoch [4/300], Loss: -0.3706, Duration: 0.0 s\n",
      "Epoch [5/300], Loss: -0.3714, Duration: 0.0 s\n",
      "Epoch [6/300], Loss: -0.3738, Duration: 0.0 s\n",
      "Epoch [7/300], Loss: -0.3753, Duration: 0.0 s\n",
      "Epoch [8/300], Loss: -0.3755, Duration: 0.0 s\n",
      "Epoch [9/300], Loss: -0.3781, Duration: 0.0 s\n",
      "Epoch [10/300], Loss: -0.3797, Duration: 0.0 s\n",
      "Epoch [11/300], Loss: -0.3811, Duration: 0.0 s\n",
      "Epoch [12/300], Loss: -0.3830, Duration: 0.0 s\n",
      "Epoch [13/300], Loss: -0.3840, Duration: 0.0 s\n",
      "Epoch [14/300], Loss: -0.3858, Duration: 0.0 s\n",
      "Epoch [15/300], Loss: -0.3871, Duration: 0.0 s\n",
      "Epoch [16/300], Loss: -0.3895, Duration: 0.0 s\n",
      "Epoch [17/300], Loss: -0.3913, Duration: 0.0 s\n",
      "Epoch [18/300], Loss: -0.3922, Duration: 0.0 s\n",
      "Epoch [19/300], Loss: -0.3939, Duration: 0.0 s\n",
      "Epoch [20/300], Loss: -0.3951, Duration: 0.0 s\n",
      "Epoch [21/300], Loss: -0.3962, Duration: 0.0 s\n",
      "Epoch [22/300], Loss: -0.3982, Duration: 0.0 s\n",
      "Epoch [23/300], Loss: -0.3988, Duration: 0.0 s\n",
      "Epoch [24/300], Loss: -0.4007, Duration: 0.0 s\n",
      "Epoch [25/300], Loss: -0.4010, Duration: 0.0 s\n",
      "Epoch [26/300], Loss: -0.4021, Duration: 0.0 s\n",
      "Epoch [27/300], Loss: -0.4026, Duration: 0.0 s\n",
      "Epoch [28/300], Loss: -0.4040, Duration: 0.0 s\n",
      "Epoch [29/300], Loss: -0.4049, Duration: 0.0 s\n",
      "Epoch [30/300], Loss: -0.4066, Duration: 0.0 s\n",
      "Epoch [31/300], Loss: -0.4073, Duration: 0.0 s\n",
      "Epoch [32/300], Loss: -0.4080, Duration: 0.0 s\n",
      "Epoch [33/300], Loss: -0.4099, Duration: 0.0 s\n",
      "Epoch [34/300], Loss: -0.4120, Duration: 0.0 s\n",
      "Epoch [35/300], Loss: -0.4124, Duration: 0.0 s\n",
      "Epoch [36/300], Loss: -0.4135, Duration: 0.0 s\n",
      "Epoch [37/300], Loss: -0.4142, Duration: 0.0 s\n",
      "Epoch [38/300], Loss: -0.4159, Duration: 0.0 s\n",
      "Epoch [39/300], Loss: -0.4166, Duration: 0.0 s\n",
      "Epoch [40/300], Loss: -0.4181, Duration: 0.0 s\n",
      "Epoch [41/300], Loss: -0.4192, Duration: 0.0 s\n",
      "Epoch [42/300], Loss: -0.4201, Duration: 0.0 s\n",
      "Epoch [43/300], Loss: -0.4207, Duration: 0.0 s\n",
      "Epoch [44/300], Loss: -0.4218, Duration: 0.0 s\n",
      "Epoch [45/300], Loss: -0.4225, Duration: 0.0 s\n",
      "Epoch [46/300], Loss: -0.4244, Duration: 0.0 s\n",
      "Epoch [47/300], Loss: -0.4250, Duration: 0.0 s\n",
      "Epoch [48/300], Loss: -0.4261, Duration: 0.0 s\n",
      "Epoch [49/300], Loss: -0.4271, Duration: 0.0 s\n",
      "Epoch [50/300], Loss: -0.4283, Duration: 0.0 s\n",
      "Epoch [51/300], Loss: -0.4298, Duration: 0.0 s\n",
      "threshs[:20] tensor([0.1441, 0.1424, 0.1259, 0.0722, 0.0657, 0.0641, 0.1041, 0.0837, 0.0832,\n",
      "        0.0785, 0.1294, 0.1073, 0.1321, 0.1228, 0.0963, 0.1162, 0.0579, 0.1252,\n",
      "        0.0696, 0.1335], grad_fn=<SliceBackward>)\n",
      "Epoch [52/300], Loss: -0.4308, Duration: 0.0 s\n",
      "Epoch [53/300], Loss: -0.4313, Duration: 0.0 s\n",
      "Epoch [54/300], Loss: -0.4323, Duration: 0.0 s\n",
      "Epoch [55/300], Loss: -0.4332, Duration: 0.0 s\n",
      "Epoch [56/300], Loss: -0.4350, Duration: 0.0 s\n",
      "Epoch [57/300], Loss: -0.4360, Duration: 0.0 s\n",
      "Epoch [58/300], Loss: -0.4369, Duration: 0.0 s\n",
      "Epoch [59/300], Loss: -0.4379, Duration: 0.0 s\n",
      "Epoch [60/300], Loss: -0.4388, Duration: 0.0 s\n",
      "Epoch [61/300], Loss: -0.4407, Duration: 0.0 s\n",
      "Epoch [62/300], Loss: -0.4413, Duration: 0.0 s\n",
      "Epoch [63/300], Loss: -0.4426, Duration: 0.0 s\n",
      "Epoch [64/300], Loss: -0.4437, Duration: 0.0 s\n",
      "Epoch [65/300], Loss: -0.4450, Duration: 0.0 s\n",
      "Epoch [66/300], Loss: -0.4465, Duration: 0.0 s\n",
      "Epoch [67/300], Loss: -0.4473, Duration: 0.0 s\n",
      "Epoch [68/300], Loss: -0.4485, Duration: 0.0 s\n",
      "Epoch [69/300], Loss: -0.4496, Duration: 0.0 s\n",
      "Epoch [70/300], Loss: -0.4502, Duration: 0.0 s\n",
      "Epoch [71/300], Loss: -0.4502, Duration: 0.0 s\n",
      "Epoch [72/300], Loss: -0.4503, Duration: 0.0 s\n",
      "Epoch [73/300], Loss: -0.4507, Duration: 0.0 s\n",
      "Epoch [74/300], Loss: -0.4515, Duration: 0.0 s\n",
      "Epoch [75/300], Loss: -0.4529, Duration: 0.0 s\n",
      "Epoch [76/300], Loss: -0.4536, Duration: 0.0 s\n",
      "Epoch [77/300], Loss: -0.4544, Duration: 0.0 s\n",
      "Epoch [78/300], Loss: -0.4553, Duration: 0.0 s\n",
      "Epoch [79/300], Loss: -0.4563, Duration: 0.0 s\n",
      "Epoch [80/300], Loss: -0.4565, Duration: 0.0 s\n",
      "Epoch [81/300], Loss: -0.4571, Duration: 0.0 s\n",
      "Epoch [82/300], Loss: -0.4575, Duration: 0.0 s\n",
      "Epoch [83/300], Loss: -0.4574, Duration: 0.0 s\n",
      "Epoch [84/300], Loss: -0.4576, Duration: 0.0 s\n",
      "Epoch [85/300], Loss: -0.4575, Duration: 0.0 s\n",
      "Epoch [86/300], Loss: -0.4579, Duration: 0.0 s\n",
      "Epoch [87/300], Loss: -0.4578, Duration: 0.0 s\n",
      "Epoch [88/300], Loss: -0.4583, Duration: 0.0 s\n",
      "Epoch [89/300], Loss: -0.4583, Duration: 0.0 s\n",
      "Epoch [90/300], Loss: -0.4590, Duration: 0.0 s\n",
      "Epoch [91/300], Loss: -0.4591, Duration: 0.0 s\n",
      "Epoch [92/300], Loss: -0.4594, Duration: 0.0 s\n",
      "Epoch [93/300], Loss: -0.4596, Duration: 0.0 s\n",
      "Epoch [94/300], Loss: -0.4593, Duration: 0.0 s\n",
      "Epoch [95/300], Loss: -0.4594, Duration: 0.0 s\n",
      "Epoch [96/300], Loss: -0.4594, Duration: 0.0 s\n",
      "Epoch [97/300], Loss: -0.4594, Duration: 0.0 s\n",
      "Epoch [98/300], Loss: -0.4595, Duration: 0.0 s\n",
      "Epoch [99/300], Loss: -0.4596, Duration: 0.0 s\n",
      "Epoch [100/300], Loss: -0.4597, Duration: 0.0 s\n",
      "Epoch [101/300], Loss: -0.4598, Duration: 0.0 s\n",
      "threshs[:20] tensor([0.1538, 0.1519, 0.1320, 0.0796, 0.0676, 0.0675, 0.1047, 0.0842, 0.0841,\n",
      "        0.0803, 0.1315, 0.1081, 0.1879, 0.1285, 0.0969, 0.1179, 0.0616, 0.1312,\n",
      "        0.0722, 0.1798], grad_fn=<SliceBackward>)\n",
      "Epoch [102/300], Loss: -0.4599, Duration: 0.0 s\n",
      "Epoch [103/300], Loss: -0.4599, Duration: 0.0 s\n",
      "Epoch [104/300], Loss: -0.4600, Duration: 0.0 s\n",
      "Epoch [105/300], Loss: -0.4601, Duration: 0.0 s\n",
      "Epoch [106/300], Loss: -0.4602, Duration: 0.0 s\n",
      "Epoch [107/300], Loss: -0.4602, Duration: 0.0 s\n",
      "Epoch [108/300], Loss: -0.4602, Duration: 0.0 s\n",
      "Epoch [109/300], Loss: -0.4603, Duration: 0.0 s\n",
      "Epoch [110/300], Loss: -0.4604, Duration: 0.0 s\n",
      "Epoch [111/300], Loss: -0.4604, Duration: 0.0 s\n",
      "Epoch [112/300], Loss: -0.4605, Duration: 0.0 s\n",
      "Epoch [113/300], Loss: -0.4605, Duration: 0.0 s\n",
      "Epoch [114/300], Loss: -0.4607, Duration: 0.0 s\n",
      "Epoch [115/300], Loss: -0.4607, Duration: 0.0 s\n",
      "Epoch [116/300], Loss: -0.4608, Duration: 0.0 s\n",
      "Epoch [117/300], Loss: -0.4608, Duration: 0.0 s\n",
      "Epoch [118/300], Loss: -0.4608, Duration: 0.0 s\n",
      "Epoch [119/300], Loss: -0.4609, Duration: 0.0 s\n",
      "Epoch [120/300], Loss: -0.4610, Duration: 0.0 s\n",
      "Epoch [121/300], Loss: -0.4610, Duration: 0.0 s\n",
      "Epoch [122/300], Loss: -0.4610, Duration: 0.0 s\n",
      "Epoch [123/300], Loss: -0.4611, Duration: 0.0 s\n",
      "Epoch [124/300], Loss: -0.4611, Duration: 0.0 s\n",
      "Epoch [125/300], Loss: -0.4611, Duration: 0.0 s\n",
      "Epoch [126/300], Loss: -0.4611, Duration: 0.0 s\n",
      "Epoch [127/300], Loss: -0.4612, Duration: 0.0 s\n",
      "Epoch [128/300], Loss: -0.4612, Duration: 0.0 s\n",
      "Epoch [129/300], Loss: -0.4612, Duration: 0.0 s\n",
      "Epoch [130/300], Loss: -0.4612, Duration: 0.0 s\n",
      "Epoch [131/300], Loss: -0.4613, Duration: 0.0 s\n",
      "Epoch [132/300], Loss: -0.4614, Duration: 0.0 s\n",
      "Epoch [133/300], Loss: -0.4613, Duration: 0.0 s\n",
      "Epoch [134/300], Loss: -0.4615, Duration: 0.0 s\n",
      "Epoch [135/300], Loss: -0.4615, Duration: 0.0 s\n",
      "Epoch [136/300], Loss: -0.4615, Duration: 0.0 s\n",
      "Epoch [137/300], Loss: -0.4615, Duration: 0.0 s\n",
      "Epoch [138/300], Loss: -0.4615, Duration: 0.0 s\n",
      "Epoch [139/300], Loss: -0.4615, Duration: 0.0 s\n",
      "Epoch [140/300], Loss: -0.4615, Duration: 0.0 s\n",
      "Epoch [141/300], Loss: -0.4616, Duration: 0.0 s\n",
      "Epoch [142/300], Loss: -0.4616, Duration: 0.0 s\n",
      "Epoch [143/300], Loss: -0.4616, Duration: 0.0 s\n",
      "Epoch [144/300], Loss: -0.4616, Duration: 0.0 s\n",
      "Epoch [145/300], Loss: -0.4617, Duration: 0.0 s\n",
      "Epoch [146/300], Loss: -0.4618, Duration: 0.0 s\n",
      "Epoch [147/300], Loss: -0.4618, Duration: 0.0 s\n",
      "Epoch [148/300], Loss: -0.4618, Duration: 0.0 s\n",
      "Epoch [149/300], Loss: -0.4618, Duration: 0.0 s\n",
      "Epoch [150/300], Loss: -0.4618, Duration: 0.0 s\n",
      "Epoch [151/300], Loss: -0.4619, Duration: 0.0 s\n",
      "threshs[:20] tensor([0.1640, 0.1561, 0.1361, 0.0793, 0.0675, 0.0677, 0.1047, 0.0842, 0.0842,\n",
      "        0.0803, 0.1316, 0.1081, 0.2390, 0.1320, 0.0970, 0.1180, 0.0616, 0.1350,\n",
      "        0.0724, 0.1970], grad_fn=<SliceBackward>)\n",
      "Epoch [152/300], Loss: -0.4619, Duration: 0.0 s\n",
      "Epoch [153/300], Loss: -0.4620, Duration: 0.0 s\n",
      "Epoch [154/300], Loss: -0.4620, Duration: 0.0 s\n",
      "Epoch [155/300], Loss: -0.4620, Duration: 0.0 s\n",
      "Epoch [156/300], Loss: -0.4620, Duration: 0.0 s\n",
      "Epoch [157/300], Loss: -0.4620, Duration: 0.0 s\n",
      "Epoch [158/300], Loss: -0.4620, Duration: 0.0 s\n",
      "Epoch [159/300], Loss: -0.4620, Duration: 0.0 s\n",
      "Epoch [160/300], Loss: -0.4620, Duration: 0.0 s\n",
      "Epoch [161/300], Loss: -0.4620, Duration: 0.0 s\n",
      "Epoch [162/300], Loss: -0.4619, Duration: 0.0 s\n",
      "Epoch [163/300], Loss: -0.4619, Duration: 0.0 s\n",
      "Epoch [164/300], Loss: -0.4619, Duration: 0.0 s\n",
      "Epoch [165/300], Loss: -0.4619, Duration: 0.0 s\n",
      "Epoch [166/300], Loss: -0.4619, Duration: 0.0 s\n",
      "Epoch [167/300], Loss: -0.4620, Duration: 0.0 s\n",
      "Epoch [168/300], Loss: -0.4621, Duration: 0.0 s\n",
      "Epoch [169/300], Loss: -0.4621, Duration: 0.0 s\n",
      "Epoch [170/300], Loss: -0.4622, Duration: 0.0 s\n",
      "Epoch [171/300], Loss: -0.4622, Duration: 0.0 s\n",
      "Epoch [172/300], Loss: -0.4622, Duration: 0.0 s\n",
      "Epoch [173/300], Loss: -0.4622, Duration: 0.0 s\n",
      "Epoch [174/300], Loss: -0.4623, Duration: 0.0 s\n",
      "Epoch [175/300], Loss: -0.4623, Duration: 0.0 s\n",
      "Epoch [176/300], Loss: -0.4623, Duration: 0.0 s\n",
      "Epoch [177/300], Loss: -0.4623, Duration: 0.0 s\n",
      "Epoch [178/300], Loss: -0.4623, Duration: 0.0 s\n",
      "Epoch [179/300], Loss: -0.4625, Duration: 0.0 s\n",
      "Epoch [180/300], Loss: -0.4625, Duration: 0.0 s\n",
      "Epoch [181/300], Loss: -0.4625, Duration: 0.0 s\n",
      "Epoch [182/300], Loss: -0.4625, Duration: 0.0 s\n",
      "Epoch [183/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [184/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [185/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [186/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [187/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [188/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [189/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [190/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [191/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [192/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [193/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [194/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [195/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [196/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [197/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [198/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [199/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [200/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [201/300], Loss: -0.4629, Duration: 0.0 s\n",
      "threshs[:20] tensor([0.2207, 0.1592, 0.1395, 0.0794, 0.0676, 0.0678, 0.1047, 0.0842, 0.0842,\n",
      "        0.0803, 0.1316, 0.1081, 0.2460, 0.1345, 0.0970, 0.1180, 0.0617, 0.1378,\n",
      "        0.0724, 0.1999], grad_fn=<SliceBackward>)\n",
      "Epoch [202/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [203/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [204/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [205/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [206/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [207/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [208/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [209/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [210/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [211/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [212/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [213/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [214/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [215/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [216/300], Loss: -0.4625, Duration: 0.0 s\n",
      "Epoch [217/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [218/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [219/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [220/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [221/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [222/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [223/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [224/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [225/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [226/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [227/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [228/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [229/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [230/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [231/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [232/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [233/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [234/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [235/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [236/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [237/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [238/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [239/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [240/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [241/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [242/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [243/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [244/300], Loss: -0.4627, Duration: 0.0 s\n",
      "Epoch [245/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [246/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [247/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [248/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [249/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [250/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [251/300], Loss: -0.4628, Duration: 0.0 s\n",
      "threshs[:20] tensor([0.2314, 0.1615, 0.1429, 0.0794, 0.0676, 0.0678, 0.1047, 0.0842, 0.0842,\n",
      "        0.0803, 0.1316, 0.1081, 0.2507, 0.1364, 0.0970, 0.1180, 0.0617, 0.1399,\n",
      "        0.0724, 0.2021], grad_fn=<SliceBackward>)\n",
      "Epoch [252/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [253/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [254/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [255/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [256/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [257/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [258/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [259/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [260/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [261/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [262/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [263/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [264/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [265/300], Loss: -0.4628, Duration: 0.0 s\n",
      "Epoch [266/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [267/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [268/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [269/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [270/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [271/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [272/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [273/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [274/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [275/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [276/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [277/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [278/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [279/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [280/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [281/300], Loss: -0.4629, Duration: 0.0 s\n",
      "Epoch [282/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [283/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [284/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [285/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [286/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [287/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [288/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [289/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [290/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [291/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [292/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [293/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [294/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [295/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [296/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [297/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [298/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [299/300], Loss: -0.4626, Duration: 0.0 s\n",
      "Epoch [300/300], Loss: -0.4626, Duration: 0.0 s\n",
      "delta: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f111e79d050>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXQd9X338fdXu7XvsmVZ3jEYMAYrxCwmptgkIRCnadKmpYmz1U/i0jbNIY1z0vZJDj2t2waa5OFJGh6S1JANCCGQZinGYEgcA5EXbAO25V22hfdNtmRt3+ePOyayuNp8Lc29ms/rHJ1778xPms945PvRzNw719wdERGJrrSwA4iISLhUBCIiEaciEBGJOBWBiEjEqQhERCIuI+wAF6K8vNwnTJgQdgwRkZSyZs2aw+5e0XN6ShbBhAkTqK+vDzuGiEhKMbPd8abr0JCISMQlVARmVmpmy82sIbgtiTNmvJmtMbP1ZvaqmX2q27wsM3vAzLaa2WYz+6NE8oiIyOAlukewBFjh7lOBFcHjnpqA6919JvB2YImZVQfzvggcdPdLgOnA8wnmERGRQUr0HMECYG5wfxmwEvh89wHu3tbtYTbnl8/HgUuDcV3A4QTziIjIICW6R1Dl7k0AwW1lvEFmNs7MNgCNwL+6+34zKw5m32Nma83sMTOr6m1BZrbIzOrNrP7QoUMJxhYRkXP6LQIze8bMNsX5WjDQhbh7o7vPAKYAC4Mn/AygBljl7tcAq4Gv9PEzHnD3Onevq6h4y6ufRETkAvV7aMjd5/U2z8wOmNkYd28yszHAwX5+1n4zexWYAzwOnAGeCGY/BnxiwMlFROSiSPTQ0FPAwuD+QuDJngPMrMbMRgX3S4AbgC0eu/71z/j9OYZbgNcSzNOnh1fv4r837B/KRYiIpJxETxYvBR41s08Ae4APAphZHfApd/8kcBlwr5k5YMBX3H1j8P2fBx42s68Ch4CPJZinT4+t2UtOZjq3z6juf7CISEQkVATufoTYX/I9p9cDnwzuLwdm9PL9u4GbEskwGNdNKuM7q3bS0tbJqKz04VqsiEhSi9Q7i6+bXEZ7p7Nm97Gwo4iIJI1IFcHbJpSSkWb8drveriAick6kiiAvO4MZNUW8tPNo2FFERJJGpIoAoG5CKRv3neBsR2fYUUREkkLkiuCa2hLaOrrYtO9k2FFERJJC5Ipg1vjYBVLX7NbhIRERiGARVBRkM74sl3V7jocdRUQkKUSuCACmVOSz68iZsGOIiCSFSBZBTcko9h49Q+wqFyIi0RbJIhhXmsupsx2cbOkIO4qISOgiWQQ1JaMAaDymw0MiIhEtglwA9qoIRESiWQTj3iyClpCTiIiEL5JFUDgqg4LsDBWBiAgRLQIzo6Y0l/WNx+nq0iuHRCTaIlkEAB+ePZ71jcf5/ku7w44iIhKqyBbBn147jlnjS/jei3vCjiIiEqrIFoGZceOUcrYePEXzWb2fQESiK7JFAHB1bTHusGGvrjskItEV6SKYOa4YgPWNKgIRia5IF0FxbhYTy/NYu1tFICLRFekiALhhShmrth3mTJvOE4hINEW+CN5zZTUt7Z08u/lg2FFEREIR+SK4dmIpFQXZfH1FA2v3HAs7jojIsIt8EaSnGf94+3SONLdx92OvhB1HRGTYRb4IAO64qpqPXj+BHYdOc7K1Pew4IiLDSkUQuLKmCIBX950MOYmIyPBSEQSuHBsrgk37ToScRERkeKkIAmX52VQX5bBRRSAiEaMi6GZmbTEv7zyqS1OLSKQkVARmVmpmy82sIbgtiTNmvJmtMbP1ZvaqmX2q27w/NbONZrbBzH5lZuWJ5EnUu64YwxsnW3l519EwY4iIDKtE9wiWACvcfSqwInjcUxNwvbvPBN4OLDGzajPLAL4G3OzuM4ANwF0J5knIvMsqyc1K58n1+8OMISIyrBItggXAsuD+MuB9PQe4e5u7nw0eZndbpgVfeWZmQCEQ6jNwblYGt06v4hcbm2jr6AoziojIsEm0CKrcvQkguK2MN8jMxpnZBqAR+Fd33+/u7cCngY3ECmA68O3eFmRmi8ys3szqDx06lGDs3r13ZjUnWtp5YevQLUNEJJn0WwRm9oyZbYrztWCgC3H3xuDwzxRgoZlVmVkmsSK4GqgmdmjoC338jAfcvc7d6yoqKga66EGbM7WCktxMfrp+35AtQ0QkmWT0N8Dd5/U2z8wOmNkYd28yszFAn1duc/f9ZvYqMAfYHUzbHvysR4l/jmFYZaan8c7LR/PfG5po7+wiM10vrBKRkS3RZ7mngIXB/YXAkz0HmFmNmY0K7pcANwBbgH3AdDM79+f9fOD1BPNcFHOmVtB8tkOfXCYikZBoESwF5ptZA7En8qUAZlZnZg8GYy4DXjKzV4Dnga+4+0Z33w98GXghOH8wE/jnBPNcFNdPLsMMft1wOOwoIiJDztxT781TdXV1Xl9fP6TLeO/9vyE9zXhi8Q1DuhwRkeFiZmvcva7ndB0A78XtM8awbs9x1uzWm8tEZGRTEfTiz2ePpywvi6+t2BZ2FBGRIaUi6EVuVgZ3vr2W3zQc4tjptrDjiIgMGRVBH/7gsiq6HJ7Xm8tEZARTEfRhxtgiyvKyeG6LPtheREYuFUEf0tKMudMqeXbzQVraOsOOIyIyJFQE/fjArBpOtXbw841NYUcRERkSKoJ+zJ5UyqTyPH748p6wo4iIDAkVQT/MjDuuqmbtnmM0n+0IO46IyEWnIhiAq2uLcYcNjbr2kIiMPCqCAZg5rhiAdSoCERmBVAQDUJybxaTyPNarCERkBFIRDNDMccWs23OMVLxIn4hIX1QEA3Td5DION7fxWtPJsKOIiFxUKoIBese02OfnrNyiy02IyMiiIhigyoIcLq8uZKUuNyEiI4yKYBBunT6a+t3H9BGWIjKiqAgG4eM3TqAsL5sv/+y1sKOIiFw0KoJBKMjJ5NNzJ7Nm9zF2Hj4ddhwRkYtCRTBIt06vAuDZzTpXICIjg4pgkMaV5jK1Mp9nNx8IO4qIyEWhIrgAt1xWxUs7jvLGidawo4iIJExFcAHufHstDnz7NzvCjiIikjAVwQUYV5rL7TPG8IOX9nDiTHvYcUREEqIiuECfesdkTrd18vCLu8KOIiKSEBXBBbpsTCFzp1Xw3VW7aO/sCjuOiMgFUxEk4IOzxnHkdBsb9p4IO4qIyAVTESRg9qRSAF7ccSTkJCIiF05FkICy/GwuHV3A6u0qAhFJXSqCBM2eVEb97qPsO94SdhQRkQuSUBGYWamZLTezhuC2pI+xhWa2z8zu7zZtlpltNLNtZvZ1M7NE8oThz2ePJzM9jY9992WdNBaRlJToHsESYIW7TwVWBI97cw/wfI9p3wQWAVODr3clmGfYTanM554FV7D1QDOv6DONRSQFJVoEC4Blwf1lwPviDTKzWUAV8HS3aWOAQndf7bEPAn6ot+9PdjdPqyTN4NcNh8OOIiIyaIkWQZW7NwEEt5U9B5hZGnAv8Lkes8YCe7s93htMi8vMFplZvZnVHzqUXB8XWZSbyZU1xfy6IblyiYgMRL9FYGbPmNmmOF8LBriMxcAv3L2x54+OM9Z7+yHu/oC717l7XUVFxQAXPXzmTClnfeNxHvz1DmI7OCIiqSGjvwHuPq+3eWZ2wMzGuHtTcKgn3kX6rwPmmNliIB/IMrNm4GtATbdxNcD+QaVPIh+/cSKb9p/gn37+OpeOLuTGqeVhRxIRGZBEDw09BSwM7i8Enuw5wN3vdPdad58A3A085O5LgkNJp8xsdvBqoY/E+/5UUZqXxbc+PIvKgmy++fy2sOOIiAxYokWwFJhvZg3A/OAxZlZnZg8O4Ps/DTwIbAO2A79MME+osjPS+Ys5k1i17Qj3Pr0l7DgiIgNiqXg8u66uzuvr68OOEVdHZxdf+MlGHluzlycWX8/Vtb2+tUJEZFiZ2Rp3r+s5Xe8svsgy0tP4xzumk52Rxk/X7Qs7johIv1QEQ6AgJ5N5l1Xxsw1NerexiCQ9FcEQ+WBdDUdPt/G9F3eHHUVEpE8qgiHyjksqmDO1nPue3srxM21hxxER6ZWKYIiYGZ975zROne1g+WsHwo4jItIrFcEQunJsEdVFOSoCEUlqKoIhZGbMm17FrxsO09LWGXYcEZG4VARD7I6rqmlp7+QreoOZiCQpFcEQe9uEUj56/QS+/ZudOkQkIklJRTAMvnDbpVwxtpC7H3uFw81nw44jInIeFcEwyM5IZ+n7Z3CipZ3nNse7QKuISHhUBMPk8upCyvKyWL39SNhRRETOoyIYJmbGdZPL+O32I/rgGhFJKiqCYXT95HLeONnKzsOnw44iIvImFcEwun5yGQC/1eEhEUkiKoJhNL4sl+qiHJ0nEJGkoiIYRrHzBOWs3nGEri6dJxCR5KAiGGbXTy7j6Ok2Vm7Vy0hFJDmoCIbZ/MuruKQqn089vJZN+06EHUdEREUw3ApzMnlk0XXkZqfz1We2hh1HRERFEIaSvCw+fsNEnnn9IGt2Hws7johEnIogJB+9YQJji0fxmUfWcaq1Pew4IhJhKoKQFOZk8h9/MpPGoy08sW5f2HFEJMJUBCG6dmIpkyrydHlqEQmViiBk86dX8eKOI5zU4SERCYmKIGS3Th9Ne6fzX6t2hR1FRCJKRRCya2qLWTCzmq8+s5X1jcfDjiMiEaQiCJmZ8c9/eCU5mek8Vt8YdhwRiSAVQRLIy87gDy6t5H9efYNOXYNIRIaZiiBJ3HblGA43t/HiDl2ZVESGV0JFYGalZrbczBqC25I+xhaa2T4zuz94nGtmPzezzWb2qpktTSRLqrt5WiVleVl8Y+W2sKOISMQkukewBFjh7lOBFcHj3twDPN9j2lfc/VLgauAGM3t3gnlS1qisdBbfPIVV247w222Hw44jIhGSaBEsAJYF95cB74s3yMxmAVXA0+emufsZd38uuN8GrAVqEsyT0u58ey1jinL496e36HONRWTYJFoEVe7eBBDcVvYcYGZpwL3A53r7IWZWDNxBbK8isnIy0/nrW6aybs9xnnplf9hxRCQiMvobYGbPAKPjzPriAJexGPiFuzeaWbyfnwH8EPi6u+/oI8ciYBFAbW3tABedej4wq4bH6hv5ux9vYEplPpdXF4UdSURGOEvkEISZbQHmunuTmY0BVrr7tB5jvg/MAbqAfCAL+Ia7Lwnmfwdodve/Huhy6+rqvL6+/oJzJ7sjzWeZd9/zzBpfwoML3xZ2HBEZIcxsjbvX9Zye6KGhp4CFwf2FwJM9B7j7ne5e6+4TgLuBh7qVwD8BRcBnEswxopTlZ7Pw+gk88/pBth44FXYcERnhEi2CpcB8M2sA5gePMbM6M3uwr280sxpih5emA2vNbL2ZfTLBPCPGR66bQEF2Bp9/fANtHV1hxxGRESyhQ0NhGemHhs75+YYm/vIHa/nfd0znYzdMDDuOiKS4oTo0JEPoPTPGcElVvj6vQESGlIogyc2dVsnvdh2l+WxH2FFEZIRSESS5udMqaO90ftOgdxuLyNBQESS5uvGljC7MYekvX9enmInIkFARJLmsjDTu/7OraTzWwn1Pbw07joiMQCqCFFA3oZT3Xz2WH768h0OnzoYdR0RGGBVBivj03Mm0d3bxnVU7w44iIiOMiiBFTKrI57Yrx/Dw6t2cOKNzBSJy8agIUsjiuVNoPtvBQ6t3hR1FREYQFUEKmV5dyC2XVvKdVTs5rfcViMhFoiJIMYtvnsKxM+088rvGsKOIyAihIkgxs8aXcFVNEY+v3Rt2FBEZIVQEKeiOq6p5df9Jth9qDjuKiIwAKoIUdMdV1ZjB42u0VyAiiVMRpKCqwhzeOX00D6/ezbHTbWHHEZEUpyJIUX87/xKa2zpY+svNpOJnSohI8lARpKhpowv4XzdN5pH6Rr67alfYcUQkhakIUtjfvXMac6dVcN/yrRxu1jWIROTCqAhSWFqa8ffvmU5Leydfe6Yh7DgikqJUBCluSmU+f3ZtLT94eQ/bDurlpCIyeCqCEeBv5k1lVGY631i5LewoIpKCVAQjQHl+NrdeXsWzmw/S0dkVdhwRSTEqghFi3mVVHD/TzrrG42FHEZEUoyIYIeZMLScjzVj+2oGwo4hIilERjBAFOZnMu6yKH760R+82FpFBURGMIJ+99RJOt3XopLGIDIqKYAS5pKqA919Tw7LVu9l3vCXsOCKSIlQEI8xn5k0Fh68u3xp2FBFJESqCEaamJJcPXzeex9fupeHAqbDjiEgKUBGMQH958xRyszL49//ZEnYUEUkBKoIRqDQvi0U3TeLp1w6wZvexsOOISJJLqAjMrNTMlptZQ3Bb0sfYQjPbZ2b3x5n3lJltSiSLnO8TN06kPD+LbzynVxCJSN8S3SNYAqxw96nAiuBxb+4Bnu850czeD+hqaRdZXnYGt8+oZtX2w7S2d4YdR0SSWKJFsABYFtxfBrwv3iAzmwVUAU/3mJ4PfBb4pwRzSBxzppbT2t6lw0Mi0qdEi6DK3ZsAgtvKngPMLA24F/hcnO+/J5h3pr8FmdkiM6s3s/pDhw4lljoiZk8qIzPdeKFB/14i0rt+i8DMnjGzTXG+FgxwGYuBX7h7Y4+fOxOY4u5PDOSHuPsD7l7n7nUVFRUDXHS05WVncP3kch5evZuXdx4NO46IJKmM/ga4+7ze5pnZATMb4+5NZjYGOBhn2HXAHDNbDOQDWWbWDOwGZpnZriBHpZmtdPe5F7Ae0ot//+AM/uRbL/K5H7/CyrvnYmZhRxKRJJPooaGngIXB/YXAkz0HuPud7l7r7hOAu4GH3H2Ju3/T3auD6TcCW1UCF19lQQ533TyF3UfOaK9AROJKtAiWAvPNrAGYHzzGzOrM7MFEw8nF8e4rR5OfncGj9XvDjiIiSSihInD3I+5+i7tPDW6PBtPr3f2Tccb/l7vfFWf6Lne/IpEs0rvcrAwWzKzmZxv2c/BUa9hxRCTJ6J3FEfHJOZNo7+ziu6t2hR1FRJKMiiAiJpbncev0Kn708h7aOvS5xiLyeyqCCPnQ22o5dqadZzfHe3GXiESViiBC5kwtp6Igmx+v0UljEfk9FUGEZKSn8f6rx/LcloMcOnU27DgikiRUBBHzgVk1dHY5T67fF3YUEUkSKoKImVpVwFXjivn+S3s426GrkoqIiiCSPjNvKjsPn+abK7eHHUVEkoCKIIJunlbJe64cw7ee38Gx021hxxGRkKkIIuqvbplCS3sn33txd9hRRCRkKoKIunR0ITdPq+C/frtLn2AmEnEqgghbdNNkjpxu4/G1el+BSJSpCCJs9qRSrqop4hvPbde5ApEIUxFEmJnxpfdezqHms9xy3/O89/7f8PDqXXR1edjRRGQYqQgi7uraEr714VncMKUcA/7hyVdZ/P21dKoMRCKj34+qlJHv5mmV3DytEnfngRd28C+/3My3XtjO4rlTwo4mIsNAewTyJjNj0U2TuO3K0Xx1eQPHz+i8gUgUqAjkPGbG4rlTaOvs4r83NIUdR0SGgYpA3uLy6kKmVRXoZaUiEaEikLcwMz507TjW7TnOU6/sDzuOiAwxFYHE9eHZ47mmtpgvPrGRfcdbwo4jIkNIRSBxZaSn8R9/MpOuLuezj6zXewtERjAVgfRqfFke/3D7dF7aeZQn1umDbERGKhWB9OmP68Zx1bhilv5qMy1tujidyEikIpA+paUZf/+eyzh06iw/eHlP2HFEZAioCKRfb5tQyuxJpXzjuW38ZO1e3HW+QGQkURHIgPzD7dMpzs3ks4++wrde2BF2HBG5iHStIRmQy6uLWP637+CvfrSOpb/czCuNxxmVlc4Nk8vpcueOq6rJyUwPO6aIXAAVgQxYWppx3x9fxejCHB79XSNpacZP1sZeTXTv01spyMngS++9nBumlIecVEQGw1LxeG9dXZ3X19eHHSPy2jq62HrgFE0nWvnei7vZc/QMu4+c5tLRhXz9T2cypbIg7Igi0o2ZrXH3up7TEzpHYGalZrbczBqC25I+xhaa2T4zu7/btCwze8DMtprZZjP7o0TyyPDKykjjirFFzJ9exbKPX8tPF9/AXTdP4Y2TrSz+/lqWv3aA9s6usGOKSD8S2iMws38Djrr7UjNbApS4++d7Gfs1oCIYf1cw7ctAurv/vZmlAaXufri/5WqPILk9v/UQf/FQPW0dXUyqyOPGKeX8atMbFORkUFuai5lRVZjNtRNLuaK6iNqyXLIzdH5BZKj1tkeQaBFsAea6e5OZjQFWuvu0OONmAZ8DfgXUdSuCRuBSdz89mOWqCJLfqdZ2frv9CP/n2QY27TvJOy6pICczjcajLZjB3mMtnGhpByDN4MqaYkYXZlNVmMOEsjwmludRkJNBeX42NSWjyEjXC9xEEjVURXDc3Yu7PT7m7iU9xqQBzwIfBm4hKAIzKwY2Ao8Bc4HtwF3ufqCXZS0CFgHU1tbO2r179wXnluHV1tFFVsb5T+RdXc5rTSfZdrCZbQebeXHHEU62ttN0vJVTZzvOG5uRZowrzaVwVCYTy3KZXJFPbXBbkpdFaW4WZpCVnkZamg3nqomklN6KoN9XDZnZM8DoOLO+OMBlLwZ+4e6NZuf9J80AaoBV7v5ZM/ss8BVihfEW7v4A8ADE9ggGuGxJAj1LAGKvQLpibBFXjC06b7q7c+R0G7sOn+ZMWycHTray8/Bpdh05zanWDl5oOMxP18e/NHZpXhZ140soL8jGgPzsDCoKsjn3e1eSm8mMmmIygrLIyUynsiBb5SGR128RuPu83uaZ2QEzG9Pt0NDBOMOuA+aY2WIgH8gys2bgC8AZ4Ilg3GPAJwa7AjKymBnl+dmU52fHne/utHfG9ibeONHK0dNtnGhpx3EaDjSzYe9x1u45BsDJlg7a+jlZnZOZRtGoTFrbu7hqXDFZ3Q5BZWUYowtHMSorjdFFo8gMCiMtzRhXkkttWW5sL8SgJDdLhSIpK9H3ETwFLASWBrdP9hzg7neeu29mHyV2aGhJ8PhnxA4LPUvssNFrCeaREc7MyMowZo4rhnF9j+3scpq7HWZqPHqGhoOn3nzcfLaTXYdPc6KlHQNeazpJ9yOlrR2drNxyiLMdXXT2cxnurIw0cjLSGFM0ipysdEYXZjOqjzfYdTkcONlKa/v5F/IrzctiYnk+xbmZwVcW5+qlungUl40pwPh94WRn6HCYJC7RIlgKPGpmnwD2AB8EMLM64FPu/sl+vv/zwMNm9lXgEPCxBPOIvCk9zSgalfnm46I4h6IGorPLOdx8lq6gJdo7nD1Hz9B47AwdXU5HZxdvnIg9qe8PbrcfOt3vS2cr8rMpyct687E7HDh5lhd3HKWlfWBXeq0oyOaWSyvJSI+VQWluFjNqiklPP78sRhfmkN5LYWSk9z0/ns4uZ/X2I4zKSmNU5vlPI2faOnhy/X663MnPif8UU5qbRUluVtx5QOyFAsEhvoHKzkinqiibdLt4xWhmFI/KvOCydXdeaDjM9oPNFy3TR64bf9FfPKE3lIkkGXensyt2ruRUa3swDbYcOMXeYy3dxsHLO4+wYe+JN6cdb2nvd+8lHjPITE+juiiHzAE8yZxoaefgqbO9zs/JTCMnMz3upcud2AsIUkV2RtoFXz6lq8vf8uKHRG2+510XnOeCTxaLyPAyMzLSjarCHKoKc96cPrXqre/U/vTcyec9PnGmnR2Hz//r80xbJwdPtdLb33yt7V28caKFlvZOmk60vrnn05f0tDTmT6+iIDuDsz2e1M2gbnwJZb2c54FYkTT38QR57HQbx8609Zuju9NnOzl0qpWL+adtR6fzxsnWhIrrirFF3HJpJWkXaU8lO86LLxKlIhAZQYpyM7m6ttc3+CeNolGZ5x2262ls8ahhTCN6l46ISMSpCEREIk5FICIScSoCEZGIUxGIiEScikBEJOJUBCIiEaciEBGJuJS8xISZHQIu9AMJyoF+PwUtRWhdkpPWJfmMlPWAxNZlvLtX9JyYkkWQCDOrj3etjVSkdUlOWpfkM1LWA4ZmXXRoSEQk4lQEIiIRF8UieCDsABeR1iU5aV2Sz0hZDxiCdYncOQIRETlfFPcIRESkGxWBiEjERaYIzOxdZrbFzLaZ2ZKw8wyWme0ys41mtt7M6oNppWa23Mwagtuk/EQSM/uOmR00s03dpsXNbjFfD7bTBjO7Jrzkb9XLunzJzPYF22a9md3Wbd4XgnXZYmbvDCd1fGY2zsyeM7PXzexVM/ubYHrKbZs+1iXlto2Z5ZjZy2b2SrAuXw6mTzSzl4Lt8oiZZQXTs4PH24L5Ewa9UHcf8V9AOrAdmARkAa8A08PONch12AWU95j2b8CS4P4S4F/DztlL9puAa4BN/WUHbgN+CRgwG3gp7PwDWJcvAXfHGTs9+F3LBiYGv4PpYa9Dt3xjgGuC+wXA1iBzym2bPtYl5bZN8O+bH9zPBF4K/r0fBT4UTP9P4NPB/cXAfwb3PwQ8MthlRmWP4Fpgm7vvcPc24EfAgpAzXQwLgGXB/WXA+0LM0it3fwE42mNyb9kXAA95zItAsZmNGZ6k/etlXXqzAPiRu591953ANmK/i0nB3ZvcfW1w/xTwOjCWFNw2faxLb5J22wT/vuc+eDoz+HLgD4AfB9N7bpdz2+vHwC1mg/uA5KgUwVigsdvjvfT9S5KMHHjazNaY2aJgWpW7N0HsPwJQGVq6weste6puq7uCwyXf6XaILmXWJTiccDWxvz5Tetv0WBdIwW1jZulmth44CCwntsdy3N07giHd8765LsH8E0DZYJYXlSKI146p9rrZG9z9GuDdwF+a2U1hBxoiqbitvglMBmYCTcC9wfSUWBczywceBz7j7if7GhpnWlKtT5x1Sclt4+6d7j4TqCG2p3JZvGHBbcLrEpUi2AuM6/a4BtgfUpYL4u77g9uDwBPEfjkOnNs1D24Phpdw0HrLnnLbyt0PBP9xu4D/x+8PMST9uphZJrEnzu+7+0+CySm5beKtSypvGwB3Pw6sJHaOoNjMMoJZ3fO+uS7B/CIGfvgSiE4R/A6YGpx1zyJ2QuWpkDMNmJnlmVnBufvArcAmYuuwMBi2EHgynIQXpLfsTwEfCYVfBZ4AAAEbSURBVF6hMhs4ce4wRbLqcZz8D4ltG4ity4eCV3VMBKYCLw93vt4Ex5G/Dbzu7vd1m5Vy26a3dUnFbWNmFWZWHNwfBcwjds7jOeADwbCe2+Xc9voA8KwHZ44HLOwz5MP1RewVD1uJHWv7Yth5Bpl9ErFXOLwCvHouP7HjgCuAhuC2NOysveT/IbHd8nZif718orfsxHZz/2+wnTYCdWHnH8C6PBxk3RD8pxzTbfwXg3XZArw77Pw91uVGYocQNgDrg6/bUnHb9LEuKbdtgBnAuiDzJuAfg+mTiJXVNuAxIDuYnhM83hbMnzTYZeoSEyIiEReVQ0MiItILFYGISMSpCEREIk5FICIScSoCEZGIUxGIiEScikBEJOL+P9bI4U+cV2tIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model, in batch mode\n",
    "num_epochs = 300\n",
    "# num_epochs = 10000\n",
    "\n",
    "# sigma = torch.nn.Parameter(torch.tensor(40.0), requires_grad=True)\n",
    "\n",
    "cumul_delta_thresh = torch.zeros(nb_classes,)\n",
    "delta_thresh = torch.zeros(nb_classes,)\n",
    "\n",
    "\n",
    "PREC_learned_AT_thresholds = THRESHmodel.thresh\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    debut = time.time()\n",
    "    \n",
    "    THRESHmodel.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    # inputs:  predictions_tensor\n",
    "#     outputs = THRESHmodel(pth_train_probs)\n",
    "    outputs = THRESHmodel(pth_dev_probs)\n",
    "    \n",
    "#     if epoch % 10 == 0:\n",
    "#         print(outputs[-1])\n",
    "#     loss = criterion(outputs, pth_train_gt)\n",
    "    loss = criterion(outputs, pth_dev_gt)\n",
    "\n",
    "    # Backward and optimize\n",
    "    THRESHoptimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "#     loss.mean().backward()\n",
    "#         loss.backward(at_batch_y)\n",
    "    # loss.backward(torch.ones_like(loss))\n",
    "    \n",
    "#     scheduler.step()\n",
    "    \n",
    "    THRESHoptimizer.step()\n",
    "    # THRESHmodel.clamp()\n",
    "    losses.append(loss)\n",
    "    \n",
    "    duree_epoch = time.time() - debut\n",
    "\n",
    "#     print ('Epoch [{}/{}], Loss: {:.4f}, Duration: {:.1f} s' \n",
    "#            .format(epoch+1, num_epochs, loss.mean(), duree_epoch))\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}, Duration: {:.1f} s' \n",
    "           .format(epoch+1, num_epochs, loss, duree_epoch))\n",
    "\n",
    "    learned_AT_thresholds = THRESHmodel.thresh\n",
    "    \n",
    "    delta_thresh = learned_AT_thresholds - PREC_learned_AT_thresholds\n",
    "    cumul_delta_thresh += delta_thresh\n",
    "    PREC_learned_AT_thresholds = learned_AT_thresholds\n",
    "    if epoch % 50 == 0: print('threshs[:20]', learned_AT_thresholds[:20])\n",
    "    # if torch.sum(delta_thresh) < 0.01: break\n",
    "    \n",
    "print('delta:', cumul_delta_thresh)\n",
    "plt.figure()\n",
    "# plt.figure(figsize=(8,6))\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T10:47:54.305868Z",
     "start_time": "2020-03-18T10:47:54.299882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.2334044 , 0.1633871 , 0.1468532 , 0.07942785, 0.06755099,\n",
       "        0.067748  , 0.10473426, 0.08418565, 0.08418454, 0.08033026,\n",
       "        0.13163842, 0.10812174, 0.25765574, 0.13798587, 0.09702846,\n",
       "        0.11798835, 0.061661  , 0.14166154, 0.07240261, 0.2039157 ],\n",
       "       dtype=float32),\n",
       " array([140., 140., 140., 140., 140., 140., 140., 140., 140., 140., 140.,\n",
       "        140., 140., 140., 140., 140., 140., 140., 140., 140.],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_AT_thresholds=THRESHmodel.thresh.clone().detach().cpu().numpy()\n",
    "sigma = THRESHmodel.sigma.clone().detach().cpu().numpy()\n",
    "learned_AT_thresholds[:20], sigma[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T10:48:04.846339Z",
     "start_time": "2020-03-18T10:48:04.712218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto_thresholds 0.2334 0.1634 0.1469 0.0794 0.0676 0.0677 0.1047 0.0842 0.0842 0.0803 0.1316 0.1081 0.2577 0.1380 0.0970 0.1180 0.0617 0.1417 0.0724 0.2039 0.2399 0.0619 0.1020 0.0853 0.1420 0.1425 0.0930 0.0856 0.1737 0.0765 0.1013 0.1006 0.1421 0.1453 0.1014 0.0753 0.0924 0.2410 0.2143 0.0840 0.1669 0.0850 0.0901 0.1142 0.0838 0.0622 0.1413 0.1438 0.0764 0.0901 0.1422 0.1251 0.0972 0.0674 0.0736 0.0708 0.0625 0.1117 0.0877 0.0773 0.1531 0.0770 0.0926 0.1083 0.0616 0.0672 0.1402 0.0764 0.1830 0.1486 0.1510 0.1052 0.1425 0.1421 0.1002 0.1024 0.0937 0.1045 0.0799 0.0848 0.1137 0.1321 0.1478 0.0698 0.0811 0.0653 0.1325 0.1660 0.1055 0.0617 0.1438 0.1417 0.1417 0.1318 0.1026 0.0635 0.2011 0.0842 0.1413 0.1415 0.0988 0.1060 0.0987 0.1739 0.0796 0.0787 0.1622 0.1433 0.0984 0.1411 0.0710 0.0858 0.0646 0.0650 0.1424 0.0863 0.1413 0.0647 0.1423 0.1469 0.0998 0.1487 0.0775 0.0733 0.1303 0.0604 0.1418 0.1412 0.1430 0.1731 0.0642 0.1540 0.0537 0.1413 0.2354 0.1425 0.1414 0.1604 0.0872 0.0795 0.0804 0.2061 0.1645 0.0880 0.0763 0.0726 0.0703 0.1485 0.0877 0.0827 0.1278 0.0732 0.1470 0.1145 0.2516 0.0601 0.0936 0.1433 0.1123 \n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.885     0.939        26\n",
      "           1      1.000     0.921     0.959        63\n",
      "           2      1.000     0.852     0.920        27\n",
      "           3      1.000     1.000     1.000        48\n",
      "           4      1.000     1.000     1.000        23\n",
      "           5      1.000     1.000     1.000        45\n",
      "           6      1.000     0.924     0.960        92\n",
      "           7      1.000     0.925     0.961        40\n",
      "           8      0.960     0.960     0.960        25\n",
      "           9      1.000     1.000     1.000        65\n",
      "          10      1.000     1.000     1.000       131\n",
      "          11      0.977     0.935     0.956        46\n",
      "          12      1.000     0.893     0.943        28\n",
      "          13      1.000     1.000     1.000        61\n",
      "          14      1.000     1.000     1.000       215\n",
      "          15      0.976     0.851     0.909        47\n",
      "          16      1.000     1.000     1.000        43\n",
      "          17      0.963     0.963     0.963        27\n",
      "          18      0.967     1.000     0.983        58\n",
      "          19      0.979     0.920     0.948        50\n",
      "          20      0.971     0.944     0.958        36\n",
      "          21      0.889     0.952     0.920        42\n",
      "          22      1.000     1.000     1.000        58\n",
      "          23      1.000     1.000     1.000        39\n",
      "          24      0.971     1.000     0.985        33\n",
      "          25      0.944     1.000     0.971        34\n",
      "          26      1.000     1.000     1.000        41\n",
      "          27      1.000     1.000     1.000        52\n",
      "          28      1.000     0.950     0.974        20\n",
      "          29      0.966     0.966     0.966        29\n",
      "          30      1.000     0.860     0.925        43\n",
      "          31      1.000     1.000     1.000        25\n",
      "          32      1.000     1.000     1.000        36\n",
      "          33      1.000     0.833     0.909        30\n",
      "          34      0.926     0.781     0.847        32\n",
      "          35      1.000     1.000     1.000        40\n",
      "          36      0.971     0.981     0.976       103\n",
      "          37      1.000     0.826     0.905        23\n",
      "          38      1.000     1.000     1.000        34\n",
      "          39      0.923     0.923     0.923        26\n",
      "          40      1.000     1.000     1.000        24\n",
      "          41      1.000     1.000     1.000        65\n",
      "          42      0.979     1.000     0.989        46\n",
      "          43      1.000     0.919     0.958        37\n",
      "          44      1.000     1.000     1.000        88\n",
      "          45      1.000     1.000     1.000        25\n",
      "          46      1.000     0.885     0.939        26\n",
      "          47      1.000     1.000     1.000        24\n",
      "          48      0.844     0.927     0.884        41\n",
      "          49      1.000     1.000     1.000        49\n",
      "          50      1.000     1.000     1.000        24\n",
      "          51      0.963     0.929     0.945        28\n",
      "          52      0.953     0.976     0.964       125\n",
      "          53      0.958     0.958     0.958        24\n",
      "          54      0.944     0.927     0.936        55\n",
      "          55      1.000     0.895     0.944        38\n",
      "          56      0.952     1.000     0.976        20\n",
      "          57      1.000     1.000     1.000        27\n",
      "          58      1.000     1.000     1.000        20\n",
      "          59      0.960     0.960     0.960        25\n",
      "          60      0.935     0.967     0.951        30\n",
      "          61      1.000     1.000     1.000        51\n",
      "          62      1.000     1.000     1.000        30\n",
      "          63      1.000     1.000     1.000       124\n",
      "          64      1.000     1.000     1.000        29\n",
      "          65      1.000     1.000     1.000        37\n",
      "          66      0.985     0.928     0.955        69\n",
      "          67      1.000     0.871     0.931        31\n",
      "          68      1.000     1.000     1.000        27\n",
      "          69      1.000     1.000     1.000        25\n",
      "          70      0.985     0.905     0.944        74\n",
      "          71      1.000     0.927     0.962        41\n",
      "          72      1.000     0.909     0.952        33\n",
      "          73      1.000     0.912     0.954        34\n",
      "          74      1.000     1.000     1.000        27\n",
      "          75      0.993     0.979     0.986       144\n",
      "          76      1.000     1.000     1.000        32\n",
      "          77      1.000     1.000     1.000        48\n",
      "          78      0.955     0.955     0.955        22\n",
      "          79      0.926     0.962     0.943        26\n",
      "          80      0.977     0.896     0.935        48\n",
      "          81      1.000     0.984     0.992        63\n",
      "          82      0.870     1.000     0.930        20\n",
      "          83      1.000     0.988     0.994        85\n",
      "          84      0.966     1.000     0.982        56\n",
      "          85      1.000     0.964     0.981        55\n",
      "          86      0.974     1.000     0.987        37\n",
      "          87      0.962     0.735     0.833        34\n",
      "          88      0.982     0.991     0.986       109\n",
      "          89      0.973     1.000     0.986        36\n",
      "          90      1.000     1.000     1.000        38\n",
      "          91      1.000     1.000     1.000        28\n",
      "          92      1.000     1.000     1.000        25\n",
      "          93      0.928     0.928     0.928        69\n",
      "          94      1.000     0.964     0.982        28\n",
      "          95      1.000     0.955     0.977        22\n",
      "          96      0.986     0.900     0.941        80\n",
      "          97      0.979     0.969     0.974        97\n",
      "          98      1.000     1.000     1.000        23\n",
      "          99      1.000     1.000     1.000        24\n",
      "         100      0.962     0.980     0.971        51\n",
      "         101      0.863     1.000     0.926        44\n",
      "         102      1.000     1.000     1.000        26\n",
      "         103      1.000     0.889     0.941        27\n",
      "         104      0.912     0.943     0.927        88\n",
      "         105      0.964     0.931     0.947        29\n",
      "         106      0.966     0.966     0.966        29\n",
      "         107      1.000     1.000     1.000        40\n",
      "         108      0.970     1.000     0.985        32\n",
      "         109      1.000     0.913     0.955        23\n",
      "         110      1.000     0.967     0.983        30\n",
      "         111      1.000     1.000     1.000        60\n",
      "         112      1.000     1.000     1.000        24\n",
      "         113      1.000     1.000     1.000        63\n",
      "         114      1.000     1.000     1.000        42\n",
      "         115      1.000     0.905     0.950        21\n",
      "         116      1.000     0.957     0.978        23\n",
      "         117      1.000     1.000     1.000        76\n",
      "         118      0.974     0.927     0.950        41\n",
      "         119      1.000     1.000     1.000        47\n",
      "         120      1.000     0.964     0.982        28\n",
      "         121      0.971     0.850     0.907        40\n",
      "         122      0.945     0.956     0.950        90\n",
      "         123      0.931     0.964     0.947        28\n",
      "         124      0.986     0.897     0.940        78\n",
      "         125      1.000     1.000     1.000        22\n",
      "         126      1.000     1.000     1.000        29\n",
      "         127      1.000     1.000     1.000        22\n",
      "         128      0.966     0.778     0.862        36\n",
      "         129      0.979     0.888     0.931       107\n",
      "         130      0.963     0.963     0.963        27\n",
      "         131      0.941     0.985     0.962       195\n",
      "         132      1.000     1.000     1.000        23\n",
      "         133      0.958     0.920     0.939        25\n",
      "         134      1.000     1.000     1.000       458\n",
      "         135      0.959     1.000     0.979        47\n",
      "         136      1.000     0.826     0.905        23\n",
      "         137      0.939     0.939     0.939        33\n",
      "         138      1.000     1.000     1.000        59\n",
      "         139      0.939     0.902     0.920        51\n",
      "         140      0.966     0.933     0.949        30\n",
      "         141      0.988     0.976     0.982        85\n",
      "         142      1.000     1.000     1.000        34\n",
      "         143      1.000     1.000     1.000        57\n",
      "         144      1.000     1.000     1.000        64\n",
      "         145      1.000     1.000     1.000        31\n",
      "         146      1.000     1.000     1.000        64\n",
      "         147      1.000     1.000     1.000        26\n",
      "         148      1.000     1.000     1.000        27\n",
      "         149      1.000     1.000     1.000        54\n",
      "         150      1.000     1.000     1.000        36\n",
      "         151      1.000     1.000     1.000        47\n",
      "         152      1.000     0.843     0.915        51\n",
      "         153      1.000     1.000     1.000        44\n",
      "         154      1.000     1.000     1.000        32\n",
      "         155      0.976     0.976     0.976        42\n",
      "         156      0.989     0.938     0.963        97\n",
      "         157      0.971     0.791     0.872        43\n",
      "         158      1.000     0.867     0.929        30\n",
      "\n",
      "   micro avg      0.983     0.965     0.974      7789\n",
      "   macro avg      0.983     0.959     0.970      7789\n",
      "weighted avg      0.983     0.965     0.973      7789\n",
      " samples avg      0.971     0.971     0.968      7789\n",
      "\n",
      "set acc: 0.945\n",
      "dev\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        13\n",
      "           1      1.000     0.091     0.167        22\n",
      "           2      0.000     0.000     0.000        24\n",
      "           3      0.500     0.696     0.582        23\n",
      "           4      0.333     0.200     0.250        10\n",
      "           5      0.459     0.773     0.576        22\n",
      "           6      0.571     0.114     0.190        35\n",
      "           7      0.250     0.050     0.083        20\n",
      "           8      1.000     0.182     0.308        11\n",
      "           9      0.588     0.800     0.678        25\n",
      "          10      0.912     0.981     0.945        53\n",
      "          11      0.583     0.412     0.483        17\n",
      "          12      0.000     0.000     0.000        10\n",
      "          13      1.000     1.000     1.000        42\n",
      "          14      0.936     0.911     0.923       112\n",
      "          15      0.389     0.350     0.368        20\n",
      "          16      0.696     0.533     0.604        30\n",
      "          17      0.000     0.000     0.000        13\n",
      "          18      0.680     0.548     0.607        31\n",
      "          19      0.000     0.000     0.000        21\n",
      "          20      0.000     0.000     0.000        16\n",
      "          21      0.500     0.250     0.333        24\n",
      "          22      0.400     0.273     0.324        22\n",
      "          23      0.625     0.278     0.385        18\n",
      "          24      0.000     0.000     0.000        16\n",
      "          25      0.000     0.000     0.000         8\n",
      "          26      0.583     0.538     0.560        13\n",
      "          27      0.333     0.069     0.114        29\n",
      "          28      0.000     0.000     0.000         9\n",
      "          29      0.400     0.167     0.235        12\n",
      "          30      0.333     0.125     0.182         8\n",
      "          31      0.750     1.000     0.857        12\n",
      "          32      0.000     0.000     0.000        13\n",
      "          33      0.000     0.000     0.000        13\n",
      "          34      1.000     0.100     0.182        10\n",
      "          35      0.880     0.957     0.917        23\n",
      "          36      0.357     0.106     0.164        47\n",
      "          37      0.000     0.000     0.000         9\n",
      "          38      1.000     0.286     0.444        21\n",
      "          39      0.333     0.077     0.125        13\n",
      "          40      1.000     0.375     0.545         8\n",
      "          41      0.500     0.018     0.034        57\n",
      "          42      0.667     0.160     0.258        25\n",
      "          43      0.400     0.105     0.167        19\n",
      "          44      0.734     0.940     0.825        50\n",
      "          45      0.667     0.400     0.500        10\n",
      "          46      0.000     0.000     0.000         9\n",
      "          47      0.000     0.000     0.000        27\n",
      "          48      0.333     0.238     0.278        21\n",
      "          49      0.654     0.810     0.723        21\n",
      "          50      0.000     0.000     0.000        12\n",
      "          51      1.000     0.100     0.182        20\n",
      "          52      0.673     0.471     0.555        70\n",
      "          53      0.750     0.273     0.400        11\n",
      "          54      0.455     0.370     0.408        27\n",
      "          55      0.400     0.111     0.174        18\n",
      "          56      0.500     0.125     0.200         8\n",
      "          57      1.000     0.182     0.308        11\n",
      "          58      0.167     0.083     0.111        12\n",
      "          59      1.000     0.300     0.462        10\n",
      "          60      0.000     0.000     0.000        13\n",
      "          61      0.514     0.720     0.600        25\n",
      "          62      0.889     0.533     0.667        15\n",
      "          63      0.667     0.929     0.776        56\n",
      "          64      0.500     0.588     0.541        17\n",
      "          65      0.429     0.643     0.514        14\n",
      "          66      0.800     0.089     0.160        45\n",
      "          67      0.333     0.111     0.167        18\n",
      "          68      0.500     0.071     0.125        14\n",
      "          69      0.545     0.500     0.522        12\n",
      "          70      0.500     0.029     0.056        34\n",
      "          71      0.429     0.200     0.273        15\n",
      "          72      0.000     0.000     0.000         9\n",
      "          73      0.000     0.000     0.000        16\n",
      "          74      0.385     0.833     0.526         6\n",
      "          75      0.565     0.217     0.313        60\n",
      "          76      0.643     0.750     0.692        12\n",
      "          77      0.625     0.238     0.345        21\n",
      "          78      0.500     0.308     0.381        13\n",
      "          79      0.625     0.312     0.417        16\n",
      "          80      0.667     0.214     0.324        28\n",
      "          81      0.682     0.577     0.625        26\n",
      "          82      1.000     0.917     0.957        12\n",
      "          83      0.667     0.667     0.667        45\n",
      "          84      1.000     0.097     0.176        31\n",
      "          85      0.474     0.321     0.383        28\n",
      "          86      0.929     0.765     0.839        17\n",
      "          87      0.500     0.067     0.118        15\n",
      "          88      0.333     0.019     0.036        53\n",
      "          89      0.400     0.105     0.167        19\n",
      "          90      0.000     0.000     0.000        21\n",
      "          91      0.000     0.000     0.000        24\n",
      "          92      0.000     0.000     0.000        11\n",
      "          93      0.118     0.048     0.068        42\n",
      "          94      0.000     0.000     0.000        13\n",
      "          95      0.500     0.077     0.133        13\n",
      "          96      1.000     0.030     0.059        33\n",
      "          97      0.562     0.462     0.507        39\n",
      "          98      0.000     0.000     0.000        15\n",
      "          99      0.000     0.000     0.000        15\n",
      "         100      0.304     0.241     0.269        29\n",
      "         101      0.727     0.941     0.821        17\n",
      "         102      0.667     0.200     0.308        10\n",
      "         103      0.000     0.000     0.000        19\n",
      "         104      0.545     0.556     0.550        54\n",
      "         105      0.500     0.071     0.125        14\n",
      "         106      0.000     0.000     0.000        19\n",
      "         107      0.000     0.000     0.000        25\n",
      "         108      0.833     0.294     0.435        17\n",
      "         109      0.000     0.000     0.000        15\n",
      "         110      1.000     0.048     0.091        21\n",
      "         111      0.438     0.700     0.538        30\n",
      "         112      1.000     0.583     0.737        12\n",
      "         113      0.607     0.607     0.607        28\n",
      "         114      0.000     0.000     0.000        10\n",
      "         115      0.600     0.273     0.375        11\n",
      "         116      0.000     0.000     0.000        14\n",
      "         117      0.727     0.653     0.688        49\n",
      "         118      0.000     0.000     0.000        16\n",
      "         119      1.000     0.190     0.320        21\n",
      "         120      0.833     0.278     0.417        18\n",
      "         121      0.364     0.250     0.296        16\n",
      "         122      0.543     0.500     0.521        50\n",
      "         123      0.333     0.067     0.111        15\n",
      "         124      0.412     0.171     0.241        41\n",
      "         125      0.714     0.385     0.500        13\n",
      "         126      0.000     0.000     0.000        22\n",
      "         127      0.000     0.000     0.000        13\n",
      "         128      0.000     0.000     0.000         7\n",
      "         129      0.000     0.000     0.000        44\n",
      "         130      0.667     0.200     0.308        10\n",
      "         131      0.633     0.532     0.578        94\n",
      "         132      0.750     0.353     0.480        17\n",
      "         133      0.000     0.000     0.000         7\n",
      "         134      0.901     0.979     0.938       233\n",
      "         135      0.000     0.000     0.000        18\n",
      "         136      0.000     0.000     0.000         7\n",
      "         137      0.000     0.000     0.000        11\n",
      "         138      1.000     0.032     0.062        31\n",
      "         139      0.750     0.556     0.638        27\n",
      "         140      0.200     0.077     0.111        13\n",
      "         141      0.000     0.000     0.000        41\n",
      "         142      0.200     0.050     0.080        20\n",
      "         143      0.667     0.143     0.235        28\n",
      "         144      0.364     0.235     0.286        34\n",
      "         145      1.000     0.053     0.100        19\n",
      "         146      0.246     0.378     0.298        37\n",
      "         147      0.667     0.250     0.364         8\n",
      "         148      0.438     0.438     0.438        16\n",
      "         149      0.447     0.548     0.493        31\n",
      "         150      0.400     0.308     0.348        13\n",
      "         151      0.600     0.789     0.682        19\n",
      "         152      0.000     0.000     0.000        24\n",
      "         153      1.000     0.059     0.111        17\n",
      "         154      0.636     0.538     0.583        13\n",
      "         155      0.875     0.241     0.378        29\n",
      "         156      0.410     0.327     0.364        49\n",
      "         157      0.000     0.000     0.000        18\n",
      "         158      0.333     0.167     0.222         6\n",
      "\n",
      "   micro avg      0.635     0.364     0.463      3827\n",
      "   macro avg      0.452     0.268     0.295      3827\n",
      "weighted avg      0.525     0.364     0.384      3827\n",
      " samples avg      0.491     0.412     0.416      3827\n",
      "\n",
      "set acc: 0.204\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        30\n",
      "           1      0.200     0.043     0.070        47\n",
      "           2      0.000     0.000     0.000        20\n",
      "           3      0.613     0.576     0.594        33\n",
      "           4      0.500     0.125     0.200        24\n",
      "           5      0.571     0.645     0.606        31\n",
      "           6      0.400     0.077     0.129        78\n",
      "           7      0.000     0.000     0.000        22\n",
      "           8      0.556     0.200     0.294        25\n",
      "           9      0.627     0.711     0.667        45\n",
      "          10      0.906     0.972     0.938       109\n",
      "          11      0.619     0.433     0.510        30\n",
      "          12      0.000     0.000     0.000        23\n",
      "          13      1.000     0.955     0.977        44\n",
      "          14      0.974     0.944     0.958       195\n",
      "          15      0.208     0.172     0.189        29\n",
      "          16      0.606     0.667     0.635        30\n",
      "          17      0.000     0.000     0.000        21\n",
      "          18      0.429     0.357     0.390        42\n",
      "          19      0.000     0.000     0.000        26\n",
      "          20      0.500     0.091     0.154        22\n",
      "          21      0.583     0.280     0.378        50\n",
      "          22      0.448     0.271     0.338        48\n",
      "          23      0.571     0.211     0.308        19\n",
      "          24      0.000     0.000     0.000        19\n",
      "          25      0.000     0.000     0.000        17\n",
      "          26      0.609     0.389     0.475        36\n",
      "          27      0.727     0.222     0.340        36\n",
      "          28      1.000     0.045     0.087        22\n",
      "          29      0.286     0.100     0.148        20\n",
      "          30      0.500     0.115     0.188        26\n",
      "          31      0.667     0.857     0.750        14\n",
      "          32      0.000     0.000     0.000        23\n",
      "          33      0.000     0.000     0.000        21\n",
      "          34      0.000     0.000     0.000        19\n",
      "          35      0.923     0.706     0.800        34\n",
      "          36      0.533     0.096     0.163        83\n",
      "          37      0.250     0.043     0.074        23\n",
      "          38      1.000     0.387     0.558        31\n",
      "          39      0.125     0.042     0.062        24\n",
      "          40      0.769     0.500     0.606        20\n",
      "          41      1.000     0.021     0.042        47\n",
      "          42      0.375     0.068     0.115        44\n",
      "          43      0.444     0.133     0.205        30\n",
      "          44      0.708     0.895     0.791        57\n",
      "          45      0.667     0.421     0.516        19\n",
      "          46      0.000     0.000     0.000        17\n",
      "          47      0.000     0.000     0.000        21\n",
      "          48      0.548     0.405     0.466        42\n",
      "          49      0.647     0.917     0.759        36\n",
      "          50      0.000     0.000     0.000        17\n",
      "          51      0.143     0.043     0.067        23\n",
      "          52      0.548     0.465     0.503        99\n",
      "          53      0.667     0.211     0.320        19\n",
      "          54      0.718     0.549     0.622        51\n",
      "          55      0.571     0.148     0.235        27\n",
      "          56      0.400     0.077     0.129        26\n",
      "          57      1.000     0.056     0.105        18\n",
      "          58      0.200     0.040     0.067        25\n",
      "          59      1.000     0.211     0.348        19\n",
      "          60      0.000     0.000     0.000        21\n",
      "          61      0.472     0.680     0.557        25\n",
      "          62      0.938     0.789     0.857        19\n",
      "          63      0.577     0.857     0.690        70\n",
      "          64      0.407     0.647     0.500        17\n",
      "          65      0.471     0.364     0.410        22\n",
      "          66      0.500     0.053     0.095        57\n",
      "          67      0.333     0.074     0.121        27\n",
      "          68      0.167     0.050     0.077        20\n",
      "          69      0.583     0.412     0.483        17\n",
      "          70      0.200     0.013     0.025        75\n",
      "          71      0.375     0.231     0.286        26\n",
      "          72      0.500     0.027     0.051        37\n",
      "          73      0.000     0.000     0.000        32\n",
      "          74      0.346     0.429     0.383        21\n",
      "          75      0.562     0.262     0.358       103\n",
      "          76      0.500     0.250     0.333        16\n",
      "          77      0.875     0.226     0.359        31\n",
      "          78      0.778     0.318     0.452        22\n",
      "          79      0.333     0.545     0.414        11\n",
      "          80      0.286     0.043     0.075        46\n",
      "          81      0.630     0.659     0.644        44\n",
      "          82      0.864     0.826     0.844        23\n",
      "          83      0.642     0.567     0.602        60\n",
      "          84      1.000     0.043     0.082        47\n",
      "          85      0.591     0.277     0.377        47\n",
      "          86      0.667     0.778     0.718        18\n",
      "          87      0.250     0.042     0.071        24\n",
      "          88      0.286     0.027     0.049        74\n",
      "          89      0.250     0.061     0.098        33\n",
      "          90      0.000     0.000     0.000        38\n",
      "          91      0.000     0.000     0.000        16\n",
      "          92      0.000     0.000     0.000        19\n",
      "          93      0.219     0.106     0.143        66\n",
      "          94      0.000     0.000     0.000        20\n",
      "          95      0.286     0.087     0.133        23\n",
      "          96      0.000     0.000     0.000        61\n",
      "          97      0.436     0.250     0.318        68\n",
      "          98      0.000     0.000     0.000        20\n",
      "          99      1.000     0.033     0.065        30\n",
      "         100      0.174     0.157     0.165        51\n",
      "         101      0.714     0.789     0.750        38\n",
      "         102      0.909     0.385     0.541        26\n",
      "         103      0.500     0.034     0.065        29\n",
      "         104      0.446     0.451     0.448        91\n",
      "         105      0.333     0.042     0.074        24\n",
      "         106      0.000     0.000     0.000        24\n",
      "         107      0.000     0.000     0.000        43\n",
      "         108      0.000     0.000     0.000        23\n",
      "         109      1.000     0.050     0.095        20\n",
      "         110      0.429     0.097     0.158        31\n",
      "         111      0.353     0.486     0.409        37\n",
      "         112      0.706     0.667     0.686        18\n",
      "         113      0.667     0.444     0.533        36\n",
      "         114      0.000     0.000     0.000        18\n",
      "         115      0.500     0.042     0.077        24\n",
      "         116      0.000     0.000     0.000        16\n",
      "         117      0.656     0.714     0.684        56\n",
      "         118      0.000     0.000     0.000        36\n",
      "         119      0.500     0.047     0.085        43\n",
      "         120      0.000     0.000     0.000        31\n",
      "         121      0.375     0.240     0.293        25\n",
      "         122      0.381     0.430     0.404        93\n",
      "         123      0.143     0.040     0.062        25\n",
      "         124      0.400     0.138     0.205        58\n",
      "         125      0.583     0.368     0.452        19\n",
      "         126      1.000     0.023     0.045        43\n",
      "         127      0.000     0.000     0.000        19\n",
      "         128      0.000     0.000     0.000        15\n",
      "         129      0.125     0.018     0.031        57\n",
      "         130      0.200     0.042     0.069        24\n",
      "         131      0.605     0.578     0.591       154\n",
      "         132      0.400     0.105     0.167        19\n",
      "         133      0.500     0.043     0.080        23\n",
      "         134      0.907     0.974     0.940       351\n",
      "         135      0.000     0.000     0.000        29\n",
      "         136      0.000     0.000     0.000        21\n",
      "         137      0.000     0.000     0.000        19\n",
      "         138      1.000     0.019     0.038        52\n",
      "         139      0.778     0.519     0.622        54\n",
      "         140      0.273     0.143     0.187        21\n",
      "         141      0.500     0.013     0.024        80\n",
      "         142      0.500     0.094     0.158        32\n",
      "         143      0.444     0.089     0.148        45\n",
      "         144      0.452     0.311     0.368        45\n",
      "         145      0.000     0.000     0.000        25\n",
      "         146      0.282     0.400     0.331        50\n",
      "         147      0.429     0.107     0.171        28\n",
      "         148      0.208     0.250     0.227        20\n",
      "         149      0.298     0.318     0.308        44\n",
      "         150      0.357     0.208     0.263        24\n",
      "         151      0.258     0.500     0.340        16\n",
      "         152      0.500     0.018     0.034        56\n",
      "         153      0.400     0.054     0.095        37\n",
      "         154      0.786     0.458     0.579        24\n",
      "         155      0.500     0.125     0.200        40\n",
      "         156      0.258     0.186     0.216        86\n",
      "         157      0.125     0.050     0.071        40\n",
      "         158      0.769     0.417     0.541        24\n",
      "\n",
      "   micro avg      0.597     0.323     0.419      6146\n",
      "   macro avg      0.417     0.236     0.261      6146\n",
      "weighted avg      0.489     0.323     0.344      6146\n",
      " samples avg      0.461     0.377     0.383      6146\n",
      "\n",
      "set acc: 0.184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pellegri/tools/miniconda3/envs/envPytorch1/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pellegri/tools/miniconda3/envs/envPytorch1/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print_thresholds(learned_AT_thresholds, nb_classes)\n",
    "train_pred = train_outputs_numpy>learned_AT_thresholds\n",
    "dev_pred = dev_outputs_numpy>learned_AT_thresholds\n",
    "test_pred = test_outputs_numpy>learned_AT_thresholds\n",
    "print('train')\n",
    "print_scores(y_train_numpy, train_pred)\n",
    "# compute_accuracy_from_numpy_tensors(y_train_numpy, train_pred)\n",
    "print('dev')\n",
    "print_scores(y_dev_numpy, dev_pred)\n",
    "print('test')\n",
    "print_scores(y_test_numpy, test_pred)\n",
    "# compute_accuracy_from_numpy_tensors(y_test_numpy, test_pred)\n",
    "# 0.3366 0.2834 0.2761 0.3185 0.2849 0.2611 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T14:34:26.862023Z",
     "start_time": "2020-03-10T14:34:26.852993Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numerical application, for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_train_probs = torch.tensor(train_outputs_numpy, dtype=torch.float).to(device)\n",
    "pth_dev_probs = torch.tensor(dev_outputs_numpy, dtype=torch.float).to(device)\n",
    "pth_test_probs = torch.tensor(test_outputs_numpy, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "pth_train_gt = y_train_pth.to(device, dtype=torch.float)\n",
    "pth_dev_gt = y_dev_pth.to(device, dtype=torch.float)\n",
    "pth_test_gt = y_test_pth.to(device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T10:51:58.024898Z",
     "start_time": "2020-03-18T10:51:45.610474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma: 1.0\n",
      "sigma: 10.0\n",
      "sigma: 20.0\n",
      "sigma: 30.0\n",
      "sigma: 40.0\n",
      "sigma: 50.0\n",
      "sigma: 60.0\n",
      "sigma: 70.0\n",
      "sigma: 80.0\n",
      "sigma: 90.0\n",
      "sigma: 100.0\n",
      "sigma: 110.0\n",
      "sigma: 120.0\n",
      "sigma: 130.0\n",
      "sigma: 140.0\n"
     ]
    }
   ],
   "source": [
    "criterion = F1_loss_objective\n",
    "# # fh = open(\"datasets/emotions/sglthresh_emotions_F1obj_micro_sigma_LabelPowerset.txt\",\"wt\")\n",
    "# fh = open(\"datasets/emotions/sglthresh_emotions_F1obj_micro_sigma_myNetwork.txt\",\"wt\")\n",
    "\n",
    "# criterion = torch.nn.BCELoss(reduction=\"mean\")\n",
    "# fh = open(\"datasets/emotions/sglthresh_emotions_BCEobj_micro_sigma.txt\",\"wt\")\n",
    "\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 300\n",
    "# fh.write(\"a,f1train,ptrain,rtrain,f1test,ptest,rtest\\n\")\n",
    "metrics_list = []\n",
    "scale_param_values_list = []\n",
    "\n",
    "scale_list = range(1, 150, 10) # F1\n",
    "# scale_list = range(80, 150, 5) # BCE\n",
    "\n",
    "for sigma_value in scale_list:\n",
    "    \n",
    "    sigma_value = float(sigma_value)\n",
    "    if sigma_value>1: sigma_value-=1.\n",
    "    scale_param_values_list.append(sigma_value)\n",
    "        \n",
    "    print(\"sigma:\", sigma_value)\n",
    "    \n",
    "    THRESHmodel = ThresholdModel(threshold_fn=threshold_fn, t=0.1, sigma=sigma_value, nb_classes=nb_classes)\n",
    "    THRESHmodel = THRESHmodel.to(device, dtype=torch.float)\n",
    "    # criterion = torch.nn.BCELoss(reduction=\"mean\")\n",
    "\n",
    "#     THRESHoptimizer = torch.optim.Adam([\n",
    "#                 {'params': THRESHmodel.thresh}\n",
    "#             ], lr=learning_rate)\n",
    "\n",
    "    #     learn the thresholds and sigma:\n",
    "    THRESHoptimizer = torch.optim.Adam([\n",
    "                    {'params': THRESHmodel.thresh},\n",
    "                    {'params': THRESHmodel.sigma, 'lr': 1.}\n",
    "                ], lr=learning_rate)\n",
    "\n",
    "    cumul_delta_thresh = torch.zeros(nb_classes,)\n",
    "    delta_thresh = torch.zeros(nb_classes,)\n",
    "\n",
    "    for el in THRESHmodel.parameters():\n",
    "        PREC_learned_AT_thresholds = el.clone().detach().cpu()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        debut = time.time()\n",
    "\n",
    "        THRESHmodel.train()\n",
    "\n",
    "        # Forward pass\n",
    "        # inputs:  predictions_tensor\n",
    "        outputs = THRESHmodel(pth_dev_probs)\n",
    "\n",
    "    #     if epoch % 10 == 0:\n",
    "    #         print(outputs[-1])\n",
    "        loss = criterion(outputs, pth_dev_gt)\n",
    "\n",
    "        # Backward and optimize\n",
    "        THRESHoptimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "    #     loss.mean().backward()\n",
    "    #         loss.backward(at_batch_y)\n",
    "        # loss.backward(torch.ones_like(loss))\n",
    "\n",
    "        THRESHoptimizer.step()\n",
    "        # THRESHmodel.clamp()\n",
    "        losses.append(loss)\n",
    "\n",
    "        duree_epoch = time.time() - debut\n",
    "\n",
    "#         print ('Epoch [{}/{}], Loss: {:.4f}, Duration: {:.1f} s' \n",
    "#                .format(epoch+1, num_epochs, loss, duree_epoch))\n",
    "\n",
    "        for el in THRESHmodel.parameters():\n",
    "            learned_AT_thresholds = el.clone().detach().cpu()\n",
    "\n",
    "        delta_thresh = learned_AT_thresholds - PREC_learned_AT_thresholds\n",
    "        cumul_delta_thresh += delta_thresh\n",
    "        PREC_learned_AT_thresholds = learned_AT_thresholds\n",
    "#         if epoch % 10 == 0: print('threshs:', learned_AT_thresholds)\n",
    "        # if torch.sum(delta_thresh) < 0.01: break\n",
    "\n",
    "#     print('delta:', cumul_delta_thresh)\n",
    "#     plt.figure()\n",
    "#     plt.plot(losses)\n",
    "    \n",
    "    learned_AT_thresholds=THRESHmodel.thresh.clone().detach().cpu().numpy()\n",
    "    dev_pred = dev_outputs_numpy>learned_AT_thresholds\n",
    "    test_pred = test_outputs_numpy>learned_AT_thresholds\n",
    "#     print(\"dev\")\n",
    "#     print_scores(y_dev_numpy, dev_pred)\n",
    "#     # compute_accuracy_from_numpy_tensors(y_train_numpy, train_pred)\n",
    "#     print(\"test\")\n",
    "#     print_scores(y_test_numpy, test_pred)\n",
    "    \n",
    "    p2, r2, fscore2, support = precision_recall_fscore_support(y_dev_numpy, dev_pred, pos_label=1, average='micro')\n",
    "#     p3, r3, fscore3, support = precision_recall_fscore_support(gt_y, preds, pos_label=1, average='macro')\n",
    "    p2te, r2te, fscore2te, _ = precision_recall_fscore_support(y_test_numpy, test_pred, pos_label=1, average='micro')\n",
    "\n",
    "#     fh.write(\"%.2f,%.2f,%.2f,%.2f,%.2f,%.2f,%.2f\\n\"%(sigma_value, 100.*fscore2, 100.*p2, 100.*r2, \n",
    "#                                                    100.*fscore2te, 100.*p2te, 100.*r2te))\n",
    "    metrics_list.append([p2, r2, fscore2, p2te, r2te, fscore2te])\n",
    "    \n",
    "# fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T10:52:48.050008Z",
     "start_time": "2020-03-18T10:52:47.503273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_influence_de_a(scale_param_values, ptest, rtest, f1test, peval, reval, f1eval):\n",
    "    \n",
    "    fontsize=18\n",
    "    plotEval = False\n",
    "    learnThresholdsAndSigma=True\n",
    "    multi='LP'\n",
    "#     multi='BR'\n",
    "    if plotEval:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(8,6))\n",
    "        if learnThresholdsAndSigma:\n",
    "            bname = \"exp/%s/%s_%s_LogReg_influence_sigma_objective_F1_learnThresholdsAndSigma_DEV_EVAL\"%(dataset,dataset,multi)\n",
    "        else:\n",
    "            bname = \"exp/%s/%s_%s_LogReg_influence_sigma_objective_F1_learnThresholdsOnly_DEV_EVAL\"%(dataset,dataset,multi)\n",
    "        output_fig_path_PNG= bname + \".png\"\n",
    "        output_fig_path_EPS=bname + \".eps\"\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots(1, 1, sharey=True, figsize=(8,6))\n",
    "        if learnThresholdsAndSigma:\n",
    "            bname = \"exp/%s/%s_%s_LogReg_influence_sigma_objective_F1_learnThresholdsAndSigma_DEV\"%(dataset,dataset,multi)\n",
    "        else:\n",
    "            bname = \"exp/%s/%s_%s_LogReg_influence_sigma_objective_F1_learnThresholdsOnly_DEV\"%(dataset,dataset,multi)\n",
    "        output_fig_path_PNG= bname + \".png\"\n",
    "        output_fig_path_EPS=bname + \".eps\"\n",
    "        \n",
    "    xlimsup=140\n",
    "#     xlimsup=30\n",
    "    \n",
    "    ax1.plot([1, xlimsup], [0.397, 0.397], '-.', label='F1 - default threshold')\n",
    "    ax1.plot([1, xlimsup], [0.460, 0.460], '--', label='F1 - numThresh')\n",
    "    ax1.plot(scale_param_values, ptest, '-+', label='precision - SGLThresh t+a')\n",
    "    ax1.plot(scale_param_values, rtest, label='recall - SGLThresh t+a')\n",
    "    ax1.plot(scale_param_values, f1test, '-o', label='F1 - SGLThresh t+a')\n",
    "#     ax1.plot([1, xlimsup], [0.5659, 0.5659], '-.', label='F1 - static')\n",
    "#     ax1.plot([1, xlimsup], [0.6118, 0.6118], '--', label='F1 - numThresh')\n",
    "#     ax1.plot([1, xlimsup], [0.6128, 0.6128], '--', label='F1 - numThresh')\n",
    "#     ax1.legend(fontsize=fontsize)\n",
    "    ax1.set_yticks([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    ax1.set_yticklabels(['0.30', '0.40', '0.50', '0.60', '0.70'])\n",
    "    ax1.tick_params(axis='x', labelsize=fontsize)\n",
    "    ax1.tick_params(axis='y', labelsize=fontsize)\n",
    "    ax1.set_xlabel('Sigmoid scale parameter',fontsize=fontsize)\n",
    "#     ax1.set_ylim([0.52, 0.66])\n",
    "#     ax1.set_title(\"Dev subset\",fontsize=14)\n",
    "    if plotEval:\n",
    "        ax2.plot([1, xlimsup], [0.386, 0.386], '-.', label='F1 - default threshold')\n",
    "        ax2.plot(scale_param_values, peval, '-+', label='precision - SGLThresh t+a')\n",
    "        ax2.plot(scale_param_values, reval, '-', label='recall - SGLThresh t+a')\n",
    "        ax2.plot(scale_param_values, f1eval, '-o', label='F1 - SGLThresh t+a')\n",
    "    #     ax2.plot([1, xlimsup], [0.5698, 0.5698], '-.', label='F1 - static')\n",
    "    #     ax2.plot([1, xlimsup], [0.6024, 0.6024], '--', label='F1 - numThresh')\n",
    "    #     ax2.plot([1, xlimsup], [0.5828, 0.5828], '--', label='F1 - numThresh')\n",
    "    #     ax2.legend(fontsize=fontsize)\n",
    "        ax2.tick_params(axis='x', labelsize=fontsize)\n",
    "        ax2.set_xlabel(\"Sigmoid scale parameter\",fontsize=fontsize)\n",
    "#         ax2.set_title(\"Test subset\",fontsize=14)\n",
    "    #     plt.suptitle(\"BCE objective\",fontsize=16)\n",
    "    #     plt.suptitle(\"F1 objective\",fontsize=16)\n",
    "    plt.savefig(output_fig_path_PNG)\n",
    "    plt.savefig(output_fig_path_EPS)\n",
    "\n",
    "metrics_array = np.array(metrics_list)\n",
    "scale_param_values = np.array(scale_param_values_list)\n",
    "# scale_param_values = np.array(range(1, 80, 5))\n",
    "plot_influence_de_a(scale_param_values, metrics_array[:,0], metrics_array[:,1], metrics_array[:,2], metrics_array[:,3], metrics_array[:,4], metrics_array[:,5])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_file_path = 'exp/%s/influence_de_a_Obj_F1.csv'%(dataset)\n",
    "scale_list = range(1, 150, 10)\n",
    "\n",
    "with open(influence_file_path, \"wt\") as fh:\n",
    "    fh.write(\"a,f1test,ptest,rtest,f1eval,peval,reval\\n\")\n",
    "    for i in range(len(metrics_list)):\n",
    "        sigma_value = scale_list[i]\n",
    "        if sigma_value>1: sigma_value-=1.\n",
    "        fh.write(\"%d\"%sigma_value)\n",
    "        for j in range(6):\n",
    "            fh.write(\",%.3f\"%(metrics_list[i][j]))\n",
    "        fh.write(\"\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumThresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T12:23:28.556121Z",
     "start_time": "2020-03-18T12:23:28.550194Z"
    }
   },
   "outputs": [],
   "source": [
    "from sgl_utils.opt import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## utility function for F1 with multiprocessing: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:33:36.287046Z",
     "start_time": "2020-03-12T15:33:36.229611Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # multiprocessing\n",
    "\n",
    "# def calculate_f1(y_true, output, thresholds, average):\n",
    "#     \"\"\"Calculate F1 score.\n",
    "#     Args:\n",
    "#       y_true: (N, (optional)frames_num], classes_num)\n",
    "#       output: (N, (optional)[frames_num], classes_num)\n",
    "#       thresholds: (classes_num,), initial thresholds\n",
    "#       average: 'micro' | 'macro'\n",
    "#     \"\"\"\n",
    "#     if y_true.ndim == 3:\n",
    "#         (N, T, F) = y_true.shape\n",
    "#         y_true = y_true.reshape((N * T, F))\n",
    "#         output = output.reshape((N * T, F))\n",
    "\n",
    "#     classes_num = y_true.shape[-1]\n",
    "#     binarized_output = np.zeros_like(output)\n",
    "# #     print('class num:', classes_num)\n",
    "    \n",
    "#     for k in range(classes_num):\n",
    "#         binarized_output[:, k] = (np.sign(output[:, k] - thresholds[k]) + 1) // 2\n",
    "\n",
    "#     if average == 'micro':\n",
    "#         return metrics.f1_score(y_true.flatten(), binarized_output.flatten())\n",
    "    \n",
    "#     f1_array = []\n",
    "#     for k in range(classes_num):\n",
    "#         f1_array.append(metrics.f1_score(y_true[:, k], binarized_output[:, k]))\n",
    "\n",
    "#     if average == 'macro':\n",
    "#         return np.average(f1_array)\n",
    "#     elif average is None:\n",
    "#         return f1_array\n",
    "#     else:\n",
    "#         raise Exception('Incorrect argument!')\n",
    "\n",
    "\n",
    "        \n",
    "# def single_class_calculate_at_gradient(y_true, output, average, thresholds):\n",
    "# # def single_class_calculate_at_gradient(params):\n",
    "#     \"\"\"Used with multiprocessing\"\"\"\n",
    "    \n",
    "# #     print(\"params\", len(params))\n",
    "# #     print(params)\n",
    "# #     thresholds, y_true, output, average = params\n",
    "    \n",
    "#     classes_num = len(thresholds)\n",
    "#     delta = 0.01\n",
    "#     grads = []\n",
    "    \n",
    "#     f1 = calculate_f1(y_true, output, thresholds, average)\n",
    "    \n",
    "#     for k, threshold in enumerate(thresholds):\n",
    "#         new_thresholds = thresholds.copy()\n",
    "#         cnt = 0\n",
    "#         while cnt < classes_num:\n",
    "#             cnt += 1\n",
    "#             new_thresholds[k] += delta\n",
    "#             f1_new = calculate_f1(y_true, output, new_thresholds, average)\n",
    "#             if f1_new != f1:\n",
    "#                 break\n",
    "\n",
    "#         grad = (f1_new - f1) / (delta * cnt)\n",
    "#         grads.append(grad)\n",
    "#         return grads\n",
    "        \n",
    "        \n",
    "# def calculate_at_gradient(y_true, output, thresholds, average):\n",
    "#     \"\"\"Calculate gradient of thresholds numerically.\n",
    "#     Args:\n",
    "#       y_true: (N, (optional)frames_num], classes_num)\n",
    "#       output: (N, (optional)[frames_num], classes_num)\n",
    "#       thresholds: (classes_num,), initial thresholds\n",
    "#       average: 'micro' | 'macro'\n",
    "#     Returns:\n",
    "#       grads: vector\n",
    "#     \"\"\"\n",
    "#     f1 = calculate_f1(y_true, output, thresholds, average)\n",
    "#     classes_num = len(thresholds)\n",
    "# #     delta = 0.01\n",
    "# #     grads = []\n",
    "    \n",
    "# #     print(\"calculate_at_gradient, f1:\", f1)\n",
    "#     nb_lists_comprised_of_ten_thresh = len(thresholds)//10\n",
    "    \n",
    "#     ten_threshold_chunks = [thresholds[i*10:(i+1)*10] for i in range(nb_lists_comprised_of_ten_thresh)]\n",
    "#     print(\"ten_threshold_chunks\", len(ten_threshold_chunks))\n",
    "#     print(ten_threshold_chunks[0])\n",
    "#     print(ten_threshold_chunks[-1])\n",
    "    \n",
    "#     pool = Pool(processes=10)\n",
    "#     func = partial(single_class_calculate_at_gradient, y_true, output, average)\n",
    "    \n",
    "#     gradients = pool.map(func, ten_threshold_chunks)\n",
    "    \n",
    "#     return gradients\n",
    "\n",
    "\n",
    "# def optimize_at_with_gd(y_true, output, thresholds, average):\n",
    "#     \"\"\"Optimize thresholds for AT.\n",
    "#     Args:\n",
    "#       y_true: (N, (optional)frames_num], classes_num)\n",
    "#       output: (N, (optional)[frames_num], classes_num)\n",
    "#       thresholds: (classes_num,), initial thresholds\n",
    "#       average: 'micro' | 'macro'\n",
    "#     Returns:\n",
    "#       metric: float\n",
    "#       thresholds: vector\n",
    "#     \"\"\"\n",
    "#     opt = Adam()\n",
    "#     opt.alpha = 1e-2\n",
    "#     metric_asfo_epoch = []\n",
    "    \n",
    "# #     n_epochs = 100\n",
    "#     n_epochs = 1\n",
    "    \n",
    "#     for i in range(n_epochs):\n",
    "#         grads = calculate_at_gradient(y_true, output, thresholds, average)\n",
    "# #         if i==0: print(\"grads:\", grads)\n",
    "#         grads = [-e for e in grads]\n",
    "#         thresholds = opt.GetNewParams(thresholds, grads)\n",
    "#         metric = calculate_f1(y_true, output, thresholds, average)\n",
    "#         if i%50==0:\n",
    "#             print('Iteration: {}, Score: {:.3f}, thresholds: {}'.format(\n",
    "#                 i, metric, np.array(thresholds)))\n",
    "#         metric_asfo_epoch.append(metric)\n",
    "        \n",
    "#     return metric, thresholds, metric_asfo_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:33:37.051551Z",
     "start_time": "2020-03-12T15:33:37.032449Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "l = [[1], [2]]\n",
    "t = tuple(l)\n",
    "l, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## utility function for F1 w/o multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T12:23:31.487496Z",
     "start_time": "2020-03-18T12:23:31.460457Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculate_f1(y_true, output, thresholds, average):\n",
    "    \"\"\"Calculate F1 score.\n",
    "    Args:\n",
    "      y_true: (N, (optional)frames_num], classes_num)\n",
    "      output: (N, (optional)[frames_num], classes_num)\n",
    "      thresholds: (classes_num,), initial thresholds\n",
    "      average: 'micro' | 'macro'\n",
    "    \"\"\"\n",
    "    if y_true.ndim == 3:\n",
    "        (N, T, F) = y_true.shape\n",
    "        y_true = y_true.reshape((N * T, F))\n",
    "        output = output.reshape((N * T, F))\n",
    "\n",
    "    classes_num = y_true.shape[-1]\n",
    "    binarized_output = np.zeros_like(output)\n",
    "\n",
    "    for k in range(classes_num):\n",
    "        binarized_output[:, k] = (np.sign(output[:, k] - thresholds[k]) + 1) // 2\n",
    "\n",
    "    if average == 'micro':\n",
    "        return metrics.f1_score(y_true.flatten(), binarized_output.flatten())\n",
    "    \n",
    "    f1_array = []\n",
    "    for k in range(classes_num):\n",
    "        f1_array.append(metrics.f1_score(y_true[:, k], binarized_output[:, k]))\n",
    "\n",
    "    if average == 'macro':\n",
    "        return np.average(f1_array)\n",
    "    elif average is None:\n",
    "        return f1_array\n",
    "    else:\n",
    "        raise Exception('Incorrect argument!')\n",
    "\n",
    "\n",
    "def calculate_at_gradient(y_true, output, thresholds, average):\n",
    "    \"\"\"Calculate gradient of thresholds numerically.\n",
    "    Args:\n",
    "      y_true: (N, (optional)frames_num], classes_num)\n",
    "      output: (N, (optional)[frames_num], classes_num)\n",
    "      thresholds: (classes_num,), initial thresholds\n",
    "      average: 'micro' | 'macro'\n",
    "    Returns:\n",
    "      grads: vector\n",
    "    \"\"\"\n",
    "    f1 = calculate_f1(y_true, output, thresholds, average)\n",
    "    delta = 0.01\n",
    "    grads = []\n",
    "\n",
    "    for k, threshold in enumerate(thresholds):\n",
    "        new_thresholds = thresholds.copy()\n",
    "        delta = 0.01\n",
    "        cnt = 0\n",
    "        while cnt < 10:\n",
    "            cnt += 1\n",
    "            new_thresholds[k] += delta\n",
    "            f1_new = calculate_f1(y_true, output, new_thresholds, average)\n",
    "\n",
    "            if f1_new != f1:\n",
    "                break\n",
    "\n",
    "        grad = (f1_new - f1) / (delta * cnt)\n",
    "        grads.append(grad)\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def optimize_at_with_gd(y_true, output, thresholds, average):\n",
    "    \"\"\"Optimize thresholds for AT.\n",
    "    Args:\n",
    "      y_true: (N, (optional)frames_num], classes_num)\n",
    "      output: (N, (optional)[frames_num], classes_num)\n",
    "      thresholds: (classes_num,), initial thresholds\n",
    "      average: 'micro' | 'macro'\n",
    "    Returns:\n",
    "      metric: float\n",
    "      thresholds: vector\n",
    "    \"\"\"\n",
    "    opt = Adam()\n",
    "    opt.alpha = 1e-2\n",
    "    metric = calculate_f1(y_true, output, thresholds, average)\n",
    "    metric_asfo_epoch = [metric]\n",
    "    for i in range(100):\n",
    "        debut = time.time()\n",
    "        grads = calculate_at_gradient(y_true, output, thresholds, average)\n",
    "        grads = [-e for e in grads]\n",
    "        thresholds = opt.GetNewParams(thresholds, grads)\n",
    "        metric = calculate_f1(y_true, output, thresholds, average)\n",
    "        if i%50==0:\n",
    "            print('Iteration: {}, Score: {:.3f}, thresholds: {}'.format(\n",
    "                i, metric, np.array(thresholds)))\n",
    "        metric_asfo_epoch.append(metric)\n",
    "        fin=time.time()-debut\n",
    "        print(\" epoch duration: %.0f sec\"%fin)\n",
    "    return metric, thresholds, metric_asfo_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numerical application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T19:27:10.218581Z",
     "start_time": "2020-03-18T12:25:12.418541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Score: 0.377, thresholds: [0.10642953 0.10958414 0.1        0.09068792 0.10318321 0.09068792\n",
      " 0.1085915  0.10859216 0.09008567 0.1085915  0.10958414 0.09052729\n",
      " 0.10269472 0.1        0.09008567 0.1085915  0.09008567 0.1\n",
      " 0.09001322 0.1085915  0.1085915  0.1085915  0.09014003 0.09008567\n",
      " 0.1        0.1        0.1085915  0.09008567 0.10488368 0.09014003\n",
      " 0.09052729 0.09033004 0.1        0.1        0.09033004 0.09023792\n",
      " 0.09001322 0.10642953 0.1085915  0.09033004 0.10642953 0.09008567\n",
      " 0.09033004 0.10488368 0.09002162 0.10642953 0.1        0.1\n",
      " 0.10980866 0.09002162 0.1        0.10958414 0.09000684 0.1085915\n",
      " 0.0900273  0.09014003 0.1        0.09164898 0.1        0.09033004\n",
      " 0.10642953 0.10980866 0.09115321 0.10996406 0.10980866 0.1085915\n",
      " 0.10642953 0.1085915  0.10318321 0.10488368 0.10980866 0.09018966\n",
      " 0.1        0.1        0.09070001 0.09002405 0.1085915  0.09033004\n",
      " 0.09000962 0.09008567 0.1085915  0.1085915  0.1085915  0.1085915\n",
      " 0.09008567 0.09008567 0.10958414 0.10387024 0.1085915  0.10488368\n",
      " 0.1        0.1        0.1        0.10958414 0.1        0.1\n",
      " 0.1085915  0.1085915  0.1        0.1        0.09014003 0.1085915\n",
      " 0.09164898 0.10488368 0.09000259 0.09395395 0.10642953 0.1\n",
      " 0.09236432 0.1        0.1        0.10958414 0.09164898 0.1085915\n",
      " 0.1        0.09014003 0.1        0.0900273  0.1        0.1085915\n",
      " 0.10387024 0.1085915  0.09014003 0.1        0.1085915  0.09033004\n",
      " 0.1        0.1        0.1        0.1085915  0.1        0.1099838\n",
      " 0.1085915  0.1        0.10999938 0.1        0.1        0.1085915\n",
      " 0.1        0.09000962 0.10929448 0.1085915  0.1085915  0.09002162\n",
      " 0.09014003 0.1        0.09004818 0.1085915  0.09014003 0.09006895\n",
      " 0.10958414 0.09000606 0.1085915  0.09164898 0.1085915  0.09264876\n",
      " 0.09026777 0.1        0.1085915 ]\n",
      " epoch duration: 48 sec\n",
      " epoch duration: 52 sec\n",
      " epoch duration: 56 sec\n",
      " epoch duration: 58 sec\n",
      " epoch duration: 61 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 61 sec\n",
      " epoch duration: 60 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 60 sec\n",
      " epoch duration: 61 sec\n",
      " epoch duration: 60 sec\n",
      " epoch duration: 61 sec\n",
      " epoch duration: 61 sec\n",
      " epoch duration: 61 sec\n",
      " epoch duration: 61 sec\n",
      " epoch duration: 60 sec\n",
      " epoch duration: 61 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 64 sec\n",
      " epoch duration: 66 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 61 sec\n",
      " epoch duration: 60 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 65 sec\n",
      " epoch duration: 66 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 64 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 64 sec\n",
      " epoch duration: 64 sec\n",
      " epoch duration: 64 sec\n",
      " epoch duration: 65 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 64 sec\n",
      "Iteration: 50, Score: 0.460, thresholds: [0.281449   0.19651805 0.1        0.0807794  0.22190696 0.06090856\n",
      " 0.09807457 0.194706   0.0404524  0.07091582 0.1388119  0.09776386\n",
      " 0.3754571  0.1        0.0932621  0.12977052 0.05601784 0.1\n",
      " 0.05418923 0.24503752 0.32272161 0.10430185 0.09824974 0.07955226\n",
      " 0.1        0.1        0.08210567 0.08039743 0.21169253 0.07201098\n",
      " 0.08826187 0.09285881 0.1        0.1        0.09751088 0.07704519\n",
      " 0.06217374 0.35433259 0.17506547 0.03776071 0.13985188 0.04794256\n",
      " 0.06231074 0.09254465 0.06905508 0.21755319 0.1        0.1\n",
      " 0.16770604 0.08285993 0.1        0.10545001 0.09521233 0.09572022\n",
      " 0.04317908 0.02827374 0.1        0.09045253 0.1        0.05092726\n",
      " 0.16971713 0.10202806 0.07046282 0.10423135 0.1114067  0.10034762\n",
      " 0.12978532 0.26993607 0.17245659 0.12494213 0.15853663 0.09589641\n",
      " 0.1        0.1        0.0893749  0.08536348 0.09781348 0.09610994\n",
      " 0.06400265 0.06219142 0.11895765 0.13337709 0.15122753 0.04951499\n",
      " 0.05410615 0.08914939 0.11874668 0.14049836 0.08061852 0.34404693\n",
      " 0.1        0.1        0.1        0.13317191 0.1        0.1\n",
      " 0.26224484 0.104248   0.1        0.1        0.09884718 0.10999904\n",
      " 0.0369388  0.19449423 0.06537294 0.06878051 0.19090496 0.1\n",
      " 0.04659994 0.1        0.1        0.14952898 0.046505   0.03544221\n",
      " 0.1        0.09185242 0.1        0.05411381 0.1        0.14428855\n",
      " 0.07695639 0.1476791  0.06694251 0.1        0.12743608 0.04095365\n",
      " 0.1        0.1        0.1        0.43276944 0.1        0.14459783\n",
      " 0.10685616 0.1        0.24333669 0.1        0.1        0.1771912\n",
      " 0.1        0.07467145 0.17946578 0.25853571 0.13865931 0.08876934\n",
      " 0.0917442  0.1        0.0446788  0.14673306 0.08328563 0.07996199\n",
      " 0.11956726 0.0692166  0.15122753 0.09200209 0.24624829 0.02830055\n",
      " 0.08632422 0.1        0.10215845]\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 64 sec\n",
      " epoch duration: 65 sec\n",
      " epoch duration: 65 sec\n",
      " epoch duration: 66 sec\n",
      " epoch duration: 64 sec\n",
      " epoch duration: 64 sec\n",
      " epoch duration: 60 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 65 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 64 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 64 sec\n",
      " epoch duration: 64 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 62 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 63 sec\n",
      " epoch duration: 62 sec\n",
      "auto_thresholds 0.2838 0.1972 0.1000 0.0812 0.2231 0.0617 0.0970 0.1954 0.0396 0.0693 0.1383 0.1004 0.3831 0.1000 0.0930 0.1263 0.0547 0.1000 0.0552 0.2465 0.5872 0.1013 0.0964 0.0788 0.1000 0.1000 0.0787 0.0800 0.2126 0.0696 0.0894 0.0956 0.1000 0.1000 0.0965 0.0731 0.0613 0.3599 0.1717 0.0360 0.1495 0.0481 0.0607 0.0962 0.0673 0.2185 0.1000 0.1000 0.1697 0.0836 0.1000 0.1064 0.0958 0.0985 0.0416 0.0286 0.1000 0.0897 0.1000 0.0550 0.1702 0.1025 0.0690 0.1026 0.1139 0.1027 0.1281 0.2718 0.1704 0.1254 0.1589 0.0953 0.1000 0.1000 0.0881 0.0858 0.0965 0.0972 0.0636 0.0630 0.1149 0.1337 0.1515 0.0504 0.0537 0.0930 0.1186 0.1365 0.0810 0.3509 0.1000 0.1000 0.1000 0.1271 0.1000 0.1000 0.2641 0.1048 0.1000 0.1000 0.0991 0.1080 0.0439 0.1952 0.0666 0.0661 0.1915 0.1000 0.0430 0.1000 0.1000 0.1512 0.0491 0.0358 0.1000 0.0905 0.1000 0.0541 0.1000 0.1423 0.0414 0.1456 0.0659 0.1000 0.1282 0.0447 0.1000 0.1000 0.1000 0.4599 0.1000 0.1466 0.1080 0.1000 0.2268 0.1000 0.1000 0.1777 0.1000 0.0742 0.1800 0.2600 0.1396 0.0860 0.0955 0.1000 0.0428 0.1441 0.0867 0.0794 0.1171 0.0674 0.1515 0.0912 0.2504 0.0292 0.0841 0.1000 0.1014 \n",
      "train manual_thres f1: 0.362\n",
      "train auto_thres f1: 0.460\n",
      "computation duration: 6235 sec\n"
     ]
    }
   ],
   "source": [
    "# nb_classes=159\n",
    "cell_debut = time.time()\n",
    "\n",
    "t=0.1\n",
    "thresh = [t]*nb_classes\n",
    "\n",
    "average = 'micro'\n",
    "\n",
    "manual_thres_f1 = calculate_f1(y_dev_numpy, dev_outputs_numpy, thresholds=thresh, average=average)\n",
    "# print_scores(y_test_numpy, test_pred)\n",
    "\n",
    "# Optimize thresholds\n",
    "# (auto_thres_f1, auto_thresholds, metric_asfo_epoch) = optimize_at_with_gd(y_train_numpy, train_outputs_numpy, \n",
    "#                                                                           thresh, average=average)\n",
    "(auto_thres_f1, auto_thresholds, metric_asfo_epoch) = optimize_at_with_gd(y_dev_numpy, dev_outputs_numpy, \n",
    "                                                                          thresh, average=average)\n",
    "\n",
    "print_thresholds(auto_thresholds, nb_classes)\n",
    "# print(\"%.3f %.3f\"%(manual_thres_f1*100, fscore2*100))\n",
    "\n",
    "print('train manual_thres f1: {:.3f}'.format(manual_thres_f1))\n",
    "print('train auto_thres f1: {:.3f}'.format(auto_thres_f1))\n",
    "fin=time.time()-cell_debut\n",
    "print(\"computation duration: %.0f sec\"%fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T20:01:50.445301Z",
     "start_time": "2020-03-18T20:01:50.128272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.36, 0.38, 0.4 , 0.42, 0.44, 0.46, 0.48]),\n",
       " <a list of 7 Text yticklabel objects>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fontsize=14\n",
    "plt.plot(metric_asfo_epoch)\n",
    "plt.xlabel(\"epochs\", fontsize=fontsize)\n",
    "plt.ylabel(\"F1\", fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "# plt.savefig(\"datasets/%s/%s_F1_asof_epochs_myNetwork.png\"%(dataset, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T20:01:58.319959Z",
     "start_time": "2020-03-18T20:01:58.198589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto_thresholds 0.2838 0.1972 0.1000 0.0812 0.2231 0.0617 0.0970 0.1954 0.0396 0.0693 0.1383 0.1004 0.3831 0.1000 0.0930 0.1263 0.0547 0.1000 0.0552 0.2465 0.5872 0.1013 0.0964 0.0788 0.1000 0.1000 0.0787 0.0800 0.2126 0.0696 0.0894 0.0956 0.1000 0.1000 0.0965 0.0731 0.0613 0.3599 0.1717 0.0360 0.1495 0.0481 0.0607 0.0962 0.0673 0.2185 0.1000 0.1000 0.1697 0.0836 0.1000 0.1064 0.0958 0.0985 0.0416 0.0286 0.1000 0.0897 0.1000 0.0550 0.1702 0.1025 0.0690 0.1026 0.1139 0.1027 0.1281 0.2718 0.1704 0.1254 0.1589 0.0953 0.1000 0.1000 0.0881 0.0858 0.0965 0.0972 0.0636 0.0630 0.1149 0.1337 0.1515 0.0504 0.0537 0.0930 0.1186 0.1365 0.0810 0.3509 0.1000 0.1000 0.1000 0.1271 0.1000 0.1000 0.2641 0.1048 0.1000 0.1000 0.0991 0.1080 0.0439 0.1952 0.0666 0.0661 0.1915 0.1000 0.0430 0.1000 0.1000 0.1512 0.0491 0.0358 0.1000 0.0905 0.1000 0.0541 0.1000 0.1423 0.0414 0.1456 0.0659 0.1000 0.1282 0.0447 0.1000 0.1000 0.1000 0.4599 0.1000 0.1466 0.1080 0.1000 0.2268 0.1000 0.1000 0.1777 0.1000 0.0742 0.1800 0.2600 0.1396 0.0860 0.0955 0.1000 0.0428 0.1441 0.0867 0.0794 0.1171 0.0674 0.1515 0.0912 0.2504 0.0292 0.0841 0.1000 0.1014 \n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.885     0.939        26\n",
      "           1      1.000     0.889     0.941        63\n",
      "           2      0.960     0.889     0.923        27\n",
      "           3      1.000     1.000     1.000        48\n",
      "           4      1.000     1.000     1.000        23\n",
      "           5      1.000     1.000     1.000        45\n",
      "           6      1.000     0.924     0.960        92\n",
      "           7      1.000     0.900     0.947        40\n",
      "           8      0.960     0.960     0.960        25\n",
      "           9      1.000     1.000     1.000        65\n",
      "          10      1.000     1.000     1.000       131\n",
      "          11      0.977     0.935     0.956        46\n",
      "          12      1.000     0.893     0.943        28\n",
      "          13      1.000     1.000     1.000        61\n",
      "          14      1.000     1.000     1.000       215\n",
      "          15      0.976     0.851     0.909        47\n",
      "          16      1.000     1.000     1.000        43\n",
      "          17      0.964     1.000     0.982        27\n",
      "          18      0.967     1.000     0.983        58\n",
      "          19      0.979     0.920     0.948        50\n",
      "          20      1.000     0.889     0.941        36\n",
      "          21      0.930     0.952     0.941        42\n",
      "          22      1.000     1.000     1.000        58\n",
      "          23      1.000     1.000     1.000        39\n",
      "          24      0.971     1.000     0.985        33\n",
      "          25      0.944     1.000     0.971        34\n",
      "          26      1.000     1.000     1.000        41\n",
      "          27      1.000     1.000     1.000        52\n",
      "          28      1.000     0.950     0.974        20\n",
      "          29      0.966     0.966     0.966        29\n",
      "          30      1.000     0.930     0.964        43\n",
      "          31      1.000     1.000     1.000        25\n",
      "          32      1.000     1.000     1.000        36\n",
      "          33      1.000     0.833     0.909        30\n",
      "          34      0.926     0.781     0.847        32\n",
      "          35      1.000     1.000     1.000        40\n",
      "          36      0.919     0.990     0.953       103\n",
      "          37      1.000     0.826     0.905        23\n",
      "          38      1.000     1.000     1.000        34\n",
      "          39      0.926     0.962     0.943        26\n",
      "          40      1.000     1.000     1.000        24\n",
      "          41      1.000     1.000     1.000        65\n",
      "          42      0.979     1.000     0.989        46\n",
      "          43      1.000     0.919     0.958        37\n",
      "          44      1.000     1.000     1.000        88\n",
      "          45      1.000     1.000     1.000        25\n",
      "          46      1.000     0.923     0.960        26\n",
      "          47      1.000     1.000     1.000        24\n",
      "          48      1.000     0.829     0.907        41\n",
      "          49      1.000     1.000     1.000        49\n",
      "          50      1.000     1.000     1.000        24\n",
      "          51      0.964     0.964     0.964        28\n",
      "          52      0.953     0.976     0.964       125\n",
      "          53      1.000     0.875     0.933        24\n",
      "          54      0.881     0.945     0.912        55\n",
      "          55      0.627     0.974     0.763        38\n",
      "          56      1.000     0.950     0.974        20\n",
      "          57      1.000     1.000     1.000        27\n",
      "          58      1.000     1.000     1.000        20\n",
      "          59      0.923     0.960     0.941        25\n",
      "          60      0.935     0.967     0.951        30\n",
      "          61      1.000     1.000     1.000        51\n",
      "          62      1.000     1.000     1.000        30\n",
      "          63      1.000     1.000     1.000       124\n",
      "          64      1.000     1.000     1.000        29\n",
      "          65      1.000     1.000     1.000        37\n",
      "          66      0.985     0.928     0.955        69\n",
      "          67      1.000     0.774     0.873        31\n",
      "          68      1.000     1.000     1.000        27\n",
      "          69      1.000     1.000     1.000        25\n",
      "          70      0.985     0.892     0.936        74\n",
      "          71      1.000     0.927     0.962        41\n",
      "          72      1.000     0.939     0.969        33\n",
      "          73      1.000     0.941     0.970        34\n",
      "          74      1.000     1.000     1.000        27\n",
      "          75      0.986     0.979     0.983       144\n",
      "          76      1.000     1.000     1.000        32\n",
      "          77      1.000     1.000     1.000        48\n",
      "          78      0.955     0.955     0.955        22\n",
      "          79      0.839     1.000     0.912        26\n",
      "          80      0.977     0.896     0.935        48\n",
      "          81      1.000     0.984     0.992        63\n",
      "          82      0.870     1.000     0.930        20\n",
      "          83      1.000     1.000     1.000        85\n",
      "          84      0.966     1.000     0.982        56\n",
      "          85      1.000     0.945     0.972        55\n",
      "          86      0.974     1.000     0.987        37\n",
      "          87      0.962     0.735     0.833        34\n",
      "          88      0.982     0.991     0.986       109\n",
      "          89      1.000     0.944     0.971        36\n",
      "          90      1.000     1.000     1.000        38\n",
      "          91      1.000     1.000     1.000        28\n",
      "          92      1.000     1.000     1.000        25\n",
      "          93      0.928     0.928     0.928        69\n",
      "          94      1.000     0.964     0.982        28\n",
      "          95      1.000     0.909     0.952        22\n",
      "          96      0.986     0.887     0.934        80\n",
      "          97      0.979     0.969     0.974        97\n",
      "          98      1.000     1.000     1.000        23\n",
      "          99      1.000     1.000     1.000        24\n",
      "         100      0.962     0.980     0.971        51\n",
      "         101      0.863     1.000     0.926        44\n",
      "         102      1.000     1.000     1.000        26\n",
      "         103      1.000     0.852     0.920        27\n",
      "         104      0.903     0.955     0.928        88\n",
      "         105      0.900     0.931     0.915        29\n",
      "         106      0.966     0.966     0.966        29\n",
      "         107      1.000     1.000     1.000        40\n",
      "         108      0.941     1.000     0.970        32\n",
      "         109      1.000     0.913     0.955        23\n",
      "         110      1.000     0.967     0.983        30\n",
      "         111      1.000     1.000     1.000        60\n",
      "         112      1.000     1.000     1.000        24\n",
      "         113      1.000     1.000     1.000        63\n",
      "         114      1.000     1.000     1.000        42\n",
      "         115      1.000     0.905     0.950        21\n",
      "         116      1.000     0.957     0.978        23\n",
      "         117      0.987     1.000     0.993        76\n",
      "         118      0.974     0.927     0.950        41\n",
      "         119      1.000     1.000     1.000        47\n",
      "         120      0.966     1.000     0.982        28\n",
      "         121      0.971     0.850     0.907        40\n",
      "         122      0.909     1.000     0.952        90\n",
      "         123      0.964     0.964     0.964        28\n",
      "         124      0.986     0.897     0.940        78\n",
      "         125      1.000     1.000     1.000        22\n",
      "         126      1.000     1.000     1.000        29\n",
      "         127      1.000     1.000     1.000        22\n",
      "         128      0.938     0.833     0.882        36\n",
      "         129      1.000     0.813     0.897       107\n",
      "         130      0.963     0.963     0.963        27\n",
      "         131      0.919     0.985     0.950       195\n",
      "         132      1.000     1.000     1.000        23\n",
      "         133      0.920     0.920     0.920        25\n",
      "         134      1.000     1.000     1.000       458\n",
      "         135      0.959     1.000     0.979        47\n",
      "         136      1.000     0.826     0.905        23\n",
      "         137      0.969     0.939     0.954        33\n",
      "         138      1.000     1.000     1.000        59\n",
      "         139      0.939     0.902     0.920        51\n",
      "         140      1.000     0.933     0.966        30\n",
      "         141      0.988     0.976     0.982        85\n",
      "         142      1.000     1.000     1.000        34\n",
      "         143      1.000     1.000     1.000        57\n",
      "         144      1.000     1.000     1.000        64\n",
      "         145      1.000     1.000     1.000        31\n",
      "         146      1.000     1.000     1.000        64\n",
      "         147      1.000     1.000     1.000        26\n",
      "         148      1.000     1.000     1.000        27\n",
      "         149      1.000     1.000     1.000        54\n",
      "         150      1.000     1.000     1.000        36\n",
      "         151      1.000     1.000     1.000        47\n",
      "         152      1.000     0.843     0.915        51\n",
      "         153      1.000     1.000     1.000        44\n",
      "         154      1.000     1.000     1.000        32\n",
      "         155      0.932     0.976     0.953        42\n",
      "         156      0.979     0.948     0.963        97\n",
      "         157      0.971     0.791     0.872        43\n",
      "         158      1.000     0.867     0.929        30\n",
      "\n",
      "   micro avg      0.978     0.964     0.971      7789\n",
      "   macro avg      0.980     0.959     0.968      7789\n",
      "weighted avg      0.980     0.964     0.971      7789\n",
      " samples avg      0.971     0.971     0.968      7789\n",
      "\n",
      "set acc: 0.941\n",
      "dev\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        13\n",
      "           1      1.000     0.091     0.167        22\n",
      "           2      0.000     0.000     0.000        24\n",
      "           3      0.500     0.696     0.582        23\n",
      "           4      0.000     0.000     0.000        10\n",
      "           5      0.429     0.818     0.562        22\n",
      "           6      0.500     0.114     0.186        35\n",
      "           7      0.000     0.000     0.000        20\n",
      "           8      0.333     0.273     0.300        11\n",
      "           9      0.538     0.840     0.656        25\n",
      "          10      0.912     0.981     0.945        53\n",
      "          11      0.583     0.412     0.483        17\n",
      "          12      0.000     0.000     0.000        10\n",
      "          13      1.000     1.000     1.000        42\n",
      "          14      0.927     0.911     0.919       112\n",
      "          15      0.389     0.350     0.368        20\n",
      "          16      0.640     0.533     0.582        30\n",
      "          17      0.000     0.000     0.000        13\n",
      "          18      0.625     0.645     0.635        31\n",
      "          19      0.000     0.000     0.000        21\n",
      "          20      0.000     0.000     0.000        16\n",
      "          21      0.400     0.083     0.138        24\n",
      "          22      0.375     0.273     0.316        22\n",
      "          23      0.462     0.333     0.387        18\n",
      "          24      0.000     0.000     0.000        16\n",
      "          25      0.000     0.000     0.000         8\n",
      "          26      0.583     0.538     0.560        13\n",
      "          27      0.222     0.069     0.105        29\n",
      "          28      0.000     0.000     0.000         9\n",
      "          29      0.333     0.167     0.222        12\n",
      "          30      0.333     0.125     0.182         8\n",
      "          31      0.750     1.000     0.857        12\n",
      "          32      0.000     0.000     0.000        13\n",
      "          33      0.000     0.000     0.000        13\n",
      "          34      0.500     0.100     0.167        10\n",
      "          35      0.815     0.957     0.880        23\n",
      "          36      0.298     0.532     0.382        47\n",
      "          37      0.000     0.000     0.000         9\n",
      "          38      1.000     0.286     0.444        21\n",
      "          39      0.333     0.231     0.273        13\n",
      "          40      1.000     0.375     0.545         8\n",
      "          41      0.276     0.140     0.186        57\n",
      "          42      0.500     0.200     0.286        25\n",
      "          43      0.400     0.105     0.167        19\n",
      "          44      0.690     0.980     0.810        50\n",
      "          45      1.000     0.200     0.333        10\n",
      "          46      0.000     0.000     0.000         9\n",
      "          47      0.000     0.000     0.000        27\n",
      "          48      0.750     0.143     0.240        21\n",
      "          49      0.630     0.810     0.708        21\n",
      "          50      0.000     0.000     0.000        12\n",
      "          51      1.000     0.100     0.182        20\n",
      "          52      0.635     0.471     0.541        70\n",
      "          53      0.667     0.182     0.286        11\n",
      "          54      0.381     0.593     0.464        27\n",
      "          55      0.294     0.833     0.435        18\n",
      "          56      0.000     0.000     0.000         8\n",
      "          57      0.667     0.182     0.286        11\n",
      "          58      0.000     0.000     0.000        12\n",
      "          59      1.000     0.300     0.462        10\n",
      "          60      0.000     0.000     0.000        13\n",
      "          61      0.565     0.520     0.542        25\n",
      "          62      0.800     0.533     0.640        15\n",
      "          63      0.642     0.929     0.759        56\n",
      "          64      0.778     0.412     0.538        17\n",
      "          65      0.500     0.214     0.300        14\n",
      "          66      0.800     0.089     0.160        45\n",
      "          67      1.000     0.056     0.105        18\n",
      "          68      0.333     0.071     0.118        14\n",
      "          69      0.500     0.500     0.500        12\n",
      "          70      0.500     0.029     0.056        34\n",
      "          71      0.375     0.200     0.261        15\n",
      "          72      0.000     0.000     0.000         9\n",
      "          73      0.000     0.000     0.000        16\n",
      "          74      0.357     0.833     0.500         6\n",
      "          75      0.440     0.367     0.400        60\n",
      "          76      0.643     0.750     0.692        12\n",
      "          77      0.556     0.238     0.333        21\n",
      "          78      0.500     0.308     0.381        13\n",
      "          79      0.429     0.375     0.400        16\n",
      "          80      0.667     0.214     0.324        28\n",
      "          81      0.714     0.577     0.638        26\n",
      "          82      1.000     0.917     0.957        12\n",
      "          83      0.583     0.778     0.667        45\n",
      "          84      0.667     0.129     0.216        31\n",
      "          85      0.500     0.250     0.333        28\n",
      "          86      0.929     0.765     0.839        17\n",
      "          87      0.500     0.067     0.118        15\n",
      "          88      0.333     0.038     0.068        53\n",
      "          89      0.000     0.000     0.000        19\n",
      "          90      0.000     0.000     0.000        21\n",
      "          91      0.000     0.000     0.000        24\n",
      "          92      0.000     0.000     0.000        11\n",
      "          93      0.111     0.048     0.067        42\n",
      "          94      0.000     0.000     0.000        13\n",
      "          95      0.000     0.000     0.000        13\n",
      "          96      1.000     0.030     0.059        33\n",
      "          97      0.625     0.256     0.364        39\n",
      "          98      0.000     0.000     0.000        15\n",
      "          99      0.000     0.000     0.000        15\n",
      "         100      0.304     0.241     0.269        29\n",
      "         101      0.727     0.941     0.821        17\n",
      "         102      0.667     0.200     0.308        10\n",
      "         103      0.000     0.000     0.000        19\n",
      "         104      0.444     0.741     0.556        54\n",
      "         105      0.500     0.143     0.222        14\n",
      "         106      0.000     0.000     0.000        19\n",
      "         107      0.000     0.000     0.000        25\n",
      "         108      0.833     0.294     0.435        17\n",
      "         109      0.000     0.000     0.000        15\n",
      "         110      0.000     0.000     0.000        21\n",
      "         111      0.630     0.567     0.596        30\n",
      "         112      1.000     0.583     0.737        12\n",
      "         113      0.417     0.893     0.568        28\n",
      "         114      0.000     0.000     0.000        10\n",
      "         115      0.600     0.273     0.375        11\n",
      "         116      0.000     0.000     0.000        14\n",
      "         117      0.630     0.694     0.660        49\n",
      "         118      0.000     0.000     0.000        16\n",
      "         119      0.800     0.190     0.308        21\n",
      "         120      0.833     0.278     0.417        18\n",
      "         121      0.364     0.250     0.296        16\n",
      "         122      0.460     0.580     0.513        50\n",
      "         123      0.000     0.000     0.000        15\n",
      "         124      0.389     0.171     0.237        41\n",
      "         125      0.714     0.385     0.500        13\n",
      "         126      0.000     0.000     0.000        22\n",
      "         127      0.000     0.000     0.000        13\n",
      "         128      0.000     0.000     0.000         7\n",
      "         129      0.000     0.000     0.000        44\n",
      "         130      1.000     0.100     0.182        10\n",
      "         131      0.614     0.543     0.576        94\n",
      "         132      1.000     0.118     0.211        17\n",
      "         133      0.000     0.000     0.000         7\n",
      "         134      0.859     0.991     0.920       233\n",
      "         135      0.000     0.000     0.000        18\n",
      "         136      0.000     0.000     0.000         7\n",
      "         137      0.000     0.000     0.000        11\n",
      "         138      0.000     0.000     0.000        31\n",
      "         139      0.714     0.556     0.625        27\n",
      "         140      0.000     0.000     0.000        13\n",
      "         141      0.000     0.000     0.000        41\n",
      "         142      0.200     0.050     0.080        20\n",
      "         143      0.571     0.143     0.229        28\n",
      "         144      0.400     0.118     0.182        34\n",
      "         145      0.000     0.000     0.000        19\n",
      "         146      0.248     0.865     0.386        37\n",
      "         147      0.500     0.250     0.333         8\n",
      "         148      0.438     0.438     0.438        16\n",
      "         149      0.436     0.548     0.486        31\n",
      "         150      0.364     0.308     0.333        13\n",
      "         151      0.536     0.789     0.638        19\n",
      "         152      0.000     0.000     0.000        24\n",
      "         153      1.000     0.059     0.111        17\n",
      "         154      0.636     0.538     0.583        13\n",
      "         155      0.500     0.483     0.491        29\n",
      "         156      0.386     0.347     0.366        49\n",
      "         157      0.000     0.000     0.000        18\n",
      "         158      0.250     0.167     0.200         6\n",
      "\n",
      "   micro avg      0.577     0.382     0.460      3827\n",
      "   macro avg      0.396     0.272     0.281      3827\n",
      "weighted avg      0.470     0.382     0.378      3827\n",
      " samples avg      0.493     0.430     0.428      3827\n",
      "\n",
      "set acc: 0.199\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        30\n",
      "           1      0.222     0.043     0.071        47\n",
      "           2      0.000     0.000     0.000        20\n",
      "           3      0.600     0.545     0.571        33\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.526     0.645     0.580        31\n",
      "           6      0.421     0.103     0.165        78\n",
      "           7      0.000     0.000     0.000        22\n",
      "           8      0.500     0.520     0.510        25\n",
      "           9      0.610     0.800     0.692        45\n",
      "          10      0.905     0.963     0.933       109\n",
      "          11      0.565     0.433     0.491        30\n",
      "          12      0.000     0.000     0.000        23\n",
      "          13      1.000     0.955     0.977        44\n",
      "          14      0.974     0.949     0.961       195\n",
      "          15      0.208     0.172     0.189        29\n",
      "          16      0.588     0.667     0.625        30\n",
      "          17      0.000     0.000     0.000        21\n",
      "          18      0.490     0.571     0.527        42\n",
      "          19      0.000     0.000     0.000        26\n",
      "          20      0.000     0.000     0.000        22\n",
      "          21      0.750     0.120     0.207        50\n",
      "          22      0.433     0.271     0.333        48\n",
      "          23      0.400     0.211     0.276        19\n",
      "          24      0.000     0.000     0.000        19\n",
      "          25      0.000     0.000     0.000        17\n",
      "          26      0.538     0.389     0.452        36\n",
      "          27      0.727     0.222     0.340        36\n",
      "          28      1.000     0.045     0.087        22\n",
      "          29      0.500     0.250     0.333        20\n",
      "          30      0.429     0.115     0.182        26\n",
      "          31      0.667     0.857     0.750        14\n",
      "          32      0.000     0.000     0.000        23\n",
      "          33      0.000     0.000     0.000        21\n",
      "          34      0.000     0.000     0.000        19\n",
      "          35      0.926     0.735     0.820        34\n",
      "          36      0.310     0.590     0.407        83\n",
      "          37      0.333     0.043     0.077        23\n",
      "          38      1.000     0.387     0.558        31\n",
      "          39      0.077     0.042     0.054        24\n",
      "          40      0.769     0.500     0.606        20\n",
      "          41      0.145     0.170     0.157        47\n",
      "          42      0.385     0.114     0.175        44\n",
      "          43      0.444     0.133     0.205        30\n",
      "          44      0.624     0.930     0.746        57\n",
      "          45      1.000     0.158     0.273        19\n",
      "          46      0.000     0.000     0.000        17\n",
      "          47      0.000     0.000     0.000        21\n",
      "          48      1.000     0.190     0.320        42\n",
      "          49      0.623     0.917     0.742        36\n",
      "          50      0.000     0.000     0.000        17\n",
      "          51      0.143     0.043     0.067        23\n",
      "          52      0.548     0.465     0.503        99\n",
      "          53      0.000     0.000     0.000        19\n",
      "          54      0.434     0.647     0.520        51\n",
      "          55      0.182     0.593     0.278        27\n",
      "          56      0.500     0.077     0.133        26\n",
      "          57      1.000     0.056     0.105        18\n",
      "          58      0.250     0.040     0.069        25\n",
      "          59      1.000     0.263     0.417        19\n",
      "          60      0.000     0.000     0.000        21\n",
      "          61      0.632     0.480     0.545        25\n",
      "          62      0.882     0.789     0.833        19\n",
      "          63      0.569     0.886     0.693        70\n",
      "          64      0.333     0.176     0.231        17\n",
      "          65      0.333     0.091     0.143        22\n",
      "          66      0.500     0.053     0.095        57\n",
      "          67      0.000     0.000     0.000        27\n",
      "          68      0.167     0.050     0.077        20\n",
      "          69      0.571     0.471     0.516        17\n",
      "          70      0.200     0.013     0.025        75\n",
      "          71      0.412     0.269     0.326        26\n",
      "          72      0.500     0.027     0.051        37\n",
      "          73      0.000     0.000     0.000        32\n",
      "          74      0.387     0.571     0.462        21\n",
      "          75      0.442     0.447     0.444       103\n",
      "          76      0.500     0.250     0.333        16\n",
      "          77      0.800     0.258     0.390        31\n",
      "          78      0.667     0.364     0.471        22\n",
      "          79      0.276     0.727     0.400        11\n",
      "          80      0.333     0.043     0.077        46\n",
      "          81      0.644     0.659     0.652        44\n",
      "          82      0.864     0.826     0.844        23\n",
      "          83      0.536     0.750     0.625        60\n",
      "          84      0.857     0.128     0.222        47\n",
      "          85      0.647     0.234     0.344        47\n",
      "          86      0.625     0.833     0.714        18\n",
      "          87      0.250     0.042     0.071        24\n",
      "          88      0.286     0.054     0.091        74\n",
      "          89      0.000     0.000     0.000        33\n",
      "          90      0.000     0.000     0.000        38\n",
      "          91      0.000     0.000     0.000        16\n",
      "          92      0.000     0.000     0.000        19\n",
      "          93      0.219     0.106     0.143        66\n",
      "          94      0.000     0.000     0.000        20\n",
      "          95      0.500     0.087     0.148        23\n",
      "          96      0.000     0.000     0.000        61\n",
      "          97      0.524     0.162     0.247        68\n",
      "          98      0.000     0.000     0.000        20\n",
      "          99      1.000     0.067     0.125        30\n",
      "         100      0.174     0.157     0.165        51\n",
      "         101      0.714     0.789     0.750        38\n",
      "         102      0.833     0.385     0.526        26\n",
      "         103      0.500     0.034     0.065        29\n",
      "         104      0.420     0.604     0.495        91\n",
      "         105      0.200     0.042     0.069        24\n",
      "         106      0.000     0.000     0.000        24\n",
      "         107      0.000     0.000     0.000        43\n",
      "         108      0.000     0.000     0.000        23\n",
      "         109      1.000     0.050     0.095        20\n",
      "         110      0.500     0.065     0.114        31\n",
      "         111      0.467     0.378     0.418        37\n",
      "         112      0.684     0.722     0.703        18\n",
      "         113      0.346     0.778     0.479        36\n",
      "         114      0.000     0.000     0.000        18\n",
      "         115      0.500     0.042     0.077        24\n",
      "         116      0.000     0.000     0.000        16\n",
      "         117      0.592     0.804     0.682        56\n",
      "         118      0.000     0.000     0.000        36\n",
      "         119      0.500     0.047     0.085        43\n",
      "         120      0.000     0.000     0.000        31\n",
      "         121      0.375     0.240     0.293        25\n",
      "         122      0.376     0.570     0.453        93\n",
      "         123      0.000     0.000     0.000        25\n",
      "         124      0.400     0.138     0.205        58\n",
      "         125      0.600     0.632     0.615        19\n",
      "         126      1.000     0.023     0.045        43\n",
      "         127      0.000     0.000     0.000        19\n",
      "         128      0.000     0.000     0.000        15\n",
      "         129      0.000     0.000     0.000        57\n",
      "         130      0.000     0.000     0.000        24\n",
      "         131      0.589     0.578     0.584       154\n",
      "         132      1.000     0.105     0.190        19\n",
      "         133      0.500     0.043     0.080        23\n",
      "         134      0.880     0.983     0.929       351\n",
      "         135      0.000     0.000     0.000        29\n",
      "         136      0.000     0.000     0.000        21\n",
      "         137      0.000     0.000     0.000        19\n",
      "         138      1.000     0.019     0.038        52\n",
      "         139      0.778     0.519     0.622        54\n",
      "         140      0.000     0.000     0.000        21\n",
      "         141      0.500     0.013     0.024        80\n",
      "         142      0.429     0.094     0.154        32\n",
      "         143      0.364     0.089     0.143        45\n",
      "         144      0.412     0.156     0.226        45\n",
      "         145      0.000     0.000     0.000        25\n",
      "         146      0.202     0.820     0.324        50\n",
      "         147      0.375     0.107     0.167        28\n",
      "         148      0.208     0.250     0.227        20\n",
      "         149      0.300     0.341     0.319        44\n",
      "         150      0.375     0.250     0.300        24\n",
      "         151      0.263     0.625     0.370        16\n",
      "         152      0.500     0.018     0.034        56\n",
      "         153      0.333     0.054     0.093        37\n",
      "         154      0.786     0.458     0.579        24\n",
      "         155      0.525     0.525     0.525        40\n",
      "         156      0.253     0.233     0.242        86\n",
      "         157      0.118     0.050     0.070        40\n",
      "         158      0.786     0.458     0.579        24\n",
      "\n",
      "   micro avg      0.534     0.350     0.423      6146\n",
      "   macro avg      0.389     0.252     0.258      6146\n",
      "weighted avg      0.460     0.350     0.346      6146\n",
      " samples avg      0.462     0.398     0.395      6146\n",
      "\n",
      "set acc: 0.176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pellegri/tools/miniconda3/envs/envPytorch1/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pellegri/tools/miniconda3/envs/envPytorch1/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print_thresholds(auto_thresholds, nb_classes)\n",
    "train_pred = train_outputs_numpy>auto_thresholds\n",
    "dev_pred = dev_outputs_numpy>auto_thresholds\n",
    "test_pred = test_outputs_numpy>auto_thresholds\n",
    "print('train')\n",
    "print_scores(y_train_numpy, train_pred)\n",
    "# compute_accuracy_from_numpy_tensors(y_train_numpy, train_pred)\n",
    "print('dev')\n",
    "print_scores(y_dev_numpy, dev_pred)\n",
    "print('test')\n",
    "print_scores(y_test_numpy, test_pred)\n",
    "# compute_accuracy_from_numpy_tensors(y_test_numpy, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"exp/bibtex/LP_logReg_numThresh_F1_asof_epochs_01.npz\", metric_asfo_epoch=np.array(metric_asfo_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"exp/bibtex/LP_logReg_numThresh_F1_thresholds_01.npz\", numThresh_thresholds=auto_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgl_loss_dev_threshANDsigma_list = [-1*el.clone().detach().cpu().numpy() for el in losses]\n",
    "np.savez(\"exp/bibtex/LP_logReg_SGLThresh_F1_asof_epochs_threshANDsigma.npz\", sgl_loss_dev_threshANDsigma=np.array(sgl_loss_dev_threshANDsigma_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgl_loss_dev_thresh_list = [-1*el.clone().detach().cpu().numpy() for el in losses]\n",
    "np.savez(\"exp/bibtex/LP_logReg_SGLThresh_F1_asof_epochs_thresh.npz\", sgl_loss_dev_thresh=np.array(sgl_loss_dev_thresh_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fontsize=14\n",
    "linewidth=2\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(metric_asfo_epoch, ':k', linewidth=linewidth, label=\"numThresh\")\n",
    "plt.plot(sgl_loss_dev_thresh_list, '--b', linewidth=linewidth, label=\"SGLThresh t\")\n",
    "plt.plot(sgl_loss_dev_threshANDsigma_list, 'g', linewidth=linewidth, label=\"SGLThresh t+a\")\n",
    "plt.xlabel(\"epochs\", fontsize=fontsize)\n",
    "plt.ylabel(\"F1\", fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize, loc='center right')\n",
    "plt.savefig(\"exp/bibtex/bibtex_LP_logReg_F1_numThresh_SGL_DEV_asof_epochs_01.png\")\n",
    "plt.savefig(\"exp/bibtex/bibtex_LP_logReg_F1_numThresh_SGL_DEV_asof_epochs_01.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3622508943080902, 0.3622508943080902, 0.3622508943080902)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_asfo_epoch[0], sgl_loss_dev_thresh_list[0], sgl_loss_dev_threshANDsigma_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_asfo_epoch.insert(0, sgl_loss_dev_thresh_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T22:28:05.759716Z",
     "start_time": "2020-03-13T22:28:05.715902Z"
    }
   },
   "outputs": [],
   "source": [
    "def BCE_numpy(y_gt, y_pred):\n",
    "    epsilon=1e-12\n",
    "    return np.mean(-y_gt*np.log(y_pred+epsilon)-(1-y_gt)*np.log(1-y_pred+epsilon))\n",
    "    \n",
    "\n",
    "def calculate_BCE(y_true, output, thresholds, average):\n",
    "    \"\"\"Calculate BCE score.\n",
    "    Args:\n",
    "      y_true: (N, (optional)frames_num], classes_num)\n",
    "      output: (N, (optional)[frames_num], classes_num)\n",
    "      thresholds: (classes_num,), initial thresholds\n",
    "      average: 'micro' | 'macro'\n",
    "    \"\"\"\n",
    "    if y_true.ndim == 3:\n",
    "        (N, T, F) = y_true.shape\n",
    "        y_true = y_true.reshape((N * T, F))\n",
    "        output = output.reshape((N * T, F))\n",
    "\n",
    "    classes_num = y_true.shape[-1]\n",
    "    binarized_output = np.zeros_like(output)\n",
    "#     print('class num:', classes_num)\n",
    "    \n",
    "    for k in range(classes_num):\n",
    "        binarized_output[:, k] = (np.sign(output[:, k] - thresholds[k]) + 1) // 2\n",
    "\n",
    "    if average == 'micro':\n",
    "        return BCE_numpy(y_true.flatten(), binarized_output.flatten())\n",
    "    \n",
    "    bce_array = []\n",
    "    for k in range(classes_num):\n",
    "        bce_array.append(BCE_numpy(y_true[:, k], binarized_output[:, k]))\n",
    "\n",
    "    if average == 'macro':\n",
    "        return np.average(bce_array)\n",
    "    elif average is None:\n",
    "        return bce_array\n",
    "    else:\n",
    "        raise Exception('Incorrect argument!')\n",
    "\n",
    "\n",
    "def BCE_calculate_at_gradient(y_true, output, thresholds, average):\n",
    "    \"\"\"Calculate gradient of thresholds numerically.\n",
    "    Args:\n",
    "      y_true: (N, (optional)frames_num], classes_num)\n",
    "      output: (N, (optional)[frames_num], classes_num)\n",
    "      thresholds: (classes_num,), initial thresholds\n",
    "      average: 'micro' | 'macro'\n",
    "    Returns:\n",
    "      grads: vector\n",
    "    \"\"\"\n",
    "    bce = calculate_BCE(y_true, output, thresholds, average)\n",
    "    classes_num = len(thresholds)\n",
    "    \n",
    "    delta = 0.01\n",
    "    grads = []\n",
    "#     print(\"calculate_at_gradient, bce:\", bce)\n",
    "    \n",
    "    for k, threshold in enumerate(thresholds):\n",
    "        new_thresholds = thresholds.copy()\n",
    "        cnt = 0\n",
    "        while cnt < 10:\n",
    "            cnt += 1\n",
    "            new_thresholds[k] += delta\n",
    "            bce_new = calculate_BCE(y_true, output, new_thresholds, average)\n",
    "            if bce_new != bce:\n",
    "                break\n",
    "\n",
    "        grad = (bce_new - bce) / (delta * cnt)\n",
    "        grads.append(grad)\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def BCE_optimize_at_with_gd(y_true, output, thresholds, average):\n",
    "    \"\"\"Optimize thresholds for AT.\n",
    "    Args:\n",
    "      y_true: (N, (optional)frames_num], classes_num)\n",
    "      output: (N, (optional)[frames_num], classes_num)\n",
    "      thresholds: (classes_num,), initial thresholds\n",
    "      average: 'micro' | 'macro'\n",
    "    Returns:\n",
    "      metric: float\n",
    "      thresholds: vector\n",
    "    \"\"\"\n",
    "    opt = Adam()\n",
    "    opt.alpha = 1e-2\n",
    "    metric_asfo_epoch = []\n",
    "    print(\"start time:\", debut)\n",
    "    for i in range(100):\n",
    "        grads = BCE_calculate_at_gradient(y_true, output, thresholds, average)\n",
    "#         if i==0: print(\"grads:\", grads)\n",
    "        grads = [-e for e in grads]\n",
    "        thresholds = opt.GetNewParams(thresholds, grads)\n",
    "        metric = calculate_f1(y_true, output, thresholds, average)\n",
    "        if i%50==0:\n",
    "            print('Iteration: {}, F1 Score: {:.3f}, thresholds: {}'.format(\n",
    "                i, metric, np.array(thresholds)))\n",
    "        metric_asfo_epoch.append(metric)\n",
    "    \n",
    "    fin=time.time()-debut\n",
    "    print(\"duration:\", fin)\n",
    "        \n",
    "    return metric, thresholds, metric_asfo_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numerical application with BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T23:43:06.178671Z",
     "start_time": "2020-03-13T22:28:53.870749Z"
    }
   },
   "outputs": [],
   "source": [
    "t=0.3\n",
    "thresh = [t]*nb_classes\n",
    "\n",
    "average = 'micro'\n",
    "\n",
    "manual_thres_f1 = calculate_f1(y_test_numpy, test_pred_numpy, thresholds=thresh, average=average)\n",
    "p2, r2, fscore2, support = precision_recall_fscore_support(y_test, (test_pred_numpy>thresh)*1, \n",
    "                                                           pos_label=1, average=average)\n",
    "\n",
    "# Optimize thresholds\n",
    "(auto_thres_bce, auto_thresholds, metric_asfo_epoch) = BCE_optimize_at_with_gd(y_train_numpy, train_outputs_numpy, \n",
    "                                                                          thresh, average=average)\n",
    "\n",
    "print_thresholds(auto_thresholds, nb_classes)\n",
    "# print(\"%.3f %.3f\"%(manual_thres_f1*100, fscore2*100))\n",
    "\n",
    "auto_thres_f1 = calculate_f1(y_test_numpy, test_pred_numpy, thresholds=auto_thresholds, average=average)\n",
    "\n",
    "print('test manual_thres f1: {:.3f}'.format(manual_thres_f1))\n",
    "print('test auto_thres f1: {:.3f}'.format(auto_thres_f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BR with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BinaryRelevance(\n",
    "    classifier=SVC(kernel='linear', C=1., probability=True, random_state=123),\n",
    "    require_dense=[True, True]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_numpy, y_train_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train')\n",
    "train_pred = clf.predict(X_train_numpy)\n",
    "print_scores(y_train_numpy, train_pred)\n",
    "# compute_accuracy_from_numpy_tensors(y_train_numpy, train_pred)\n",
    "print('dev')\n",
    "dev_pred = clf.predict(X_dev_numpy)\n",
    "print_scores(y_dev_numpy, dev_pred)\n",
    "print('test')\n",
    "test_pred = clf.predict(X_test_numpy)\n",
    "print_scores(y_test_numpy, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'exp/%s/SVM.pkl'%dataset\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(clf, 'exp/%s/SVM.joblib'%dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_train_prob = clf.predict_proba(X_train_numpy)\n",
    "svm_dev_prob = clf.predict_proba(X_dev_numpy)\n",
    "svm_test_prob = clf.predict_proba(X_test_numpy)\n",
    "\n",
    "svm_train_prob = svm_train_prob.toarray()\n",
    "svm_dev_prob = svm_dev_prob.toarray()\n",
    "svm_test_prob = svm_test_prob.toarray()\n",
    "svm_train_prob.shape, svm_dev_prob.shape, svm_test_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"exp/%s/SVM_BR_probs.npz\"%dataset,\n",
    "         svm_train_prob=svm_train_prob,\n",
    "         svm_dev_prob=svm_dev_prob,\n",
    "        svm_test_prob=svm_test_prob\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps, rs, f1s = [], [], []\n",
    "\n",
    "for static_thresh in range(0, 100):\n",
    "    static_thresh /= 100.\n",
    "    train_pred = svm_train_prob>static_thresh\n",
    "    dev_pred = svm_dev_prob>static_thresh\n",
    "    test_pred = svm_test_prob>static_thresh\n",
    "\n",
    "    # p, r, f1 = micro_prec_rec_fscore(y_dev_numpy, dev_pred)\n",
    "    p, r, f1 = micro_prec_rec_fscore_class(y_dev_numpy, dev_pred)\n",
    "    ps.append(p)\n",
    "    rs.append(r)\n",
    "    f1s.append(f1)\n",
    "\n",
    "f1s = np.array(f1s)\n",
    "\n",
    "fontsize=14\n",
    "linewidth=2\n",
    "plt.figure(figsize=(8,6))\n",
    "for ind in range(f1s[0].shape[0]):\n",
    "    plt.plot(f1s[:,ind], label='%d'%ind)\n",
    "\n",
    "# plt.plot([23, 23], [0, 0.6], '-k')\n",
    "plt.xlabel(\"threshold\", fontsize=fontsize)\n",
    "plt.ylabel(\"F1\", fontsize=fontsize)\n",
    "plt.xlim([0,100])\n",
    "plt.xticks([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "# plt.legend(fontsize=fontsize, loc='upper right')\n",
    "# plt.savefig(\"exp/emotions/emotions_F1_DEV_asof_threshold.png\")\n",
    "# plt.savefig(\"exp/emotions/emotions_F1_DEV_asof_threshold.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LP with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LabelPowerset(\n",
    "    classifier=SVC(kernel='linear', C=0.01, probability=True, random_state=123),\n",
    "    require_dense=[True, True]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelPowerset(classifier=SVC(C=0.01, break_ties=False, cache_size=200,\n",
       "                             class_weight=None, coef0=0.0,\n",
       "                             decision_function_shape='ovr', degree=3,\n",
       "                             gamma='scale', kernel='linear', max_iter=-1,\n",
       "                             probability=True, random_state=123, shrinking=True,\n",
       "                             tol=0.001, verbose=False),\n",
       "              require_dense=[True, True])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_numpy, y_train_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pellegri/tools/miniconda3/envs/envPytorch1/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        26\n",
      "           1      1.000     0.032     0.062        63\n",
      "           2      0.000     0.000     0.000        27\n",
      "           3      1.000     0.062     0.118        48\n",
      "           4      0.000     0.000     0.000        23\n",
      "           5      1.000     0.044     0.085        45\n",
      "           6      1.000     0.109     0.196        92\n",
      "           7      0.000     0.000     0.000        40\n",
      "           8      0.000     0.000     0.000        25\n",
      "           9      1.000     0.200     0.333        65\n",
      "          10      0.904     0.939     0.921       131\n",
      "          11      1.000     0.087     0.160        46\n",
      "          12      0.000     0.000     0.000        28\n",
      "          13      0.968     0.984     0.976        61\n",
      "          14      0.477     0.986     0.643       215\n",
      "          15      0.390     0.489     0.434        47\n",
      "          16      0.962     0.581     0.725        43\n",
      "          17      0.000     0.000     0.000        27\n",
      "          18      0.000     0.000     0.000        58\n",
      "          19      1.000     0.060     0.113        50\n",
      "          20      0.000     0.000     0.000        36\n",
      "          21      0.000     0.000     0.000        42\n",
      "          22      1.000     0.172     0.294        58\n",
      "          23      1.000     0.026     0.050        39\n",
      "          24      1.000     0.061     0.114        33\n",
      "          25      0.000     0.000     0.000        34\n",
      "          26      1.000     0.293     0.453        41\n",
      "          27      1.000     0.058     0.109        52\n",
      "          28      0.000     0.000     0.000        20\n",
      "          29      1.000     0.034     0.067        29\n",
      "          30      0.000     0.000     0.000        43\n",
      "          31      0.760     0.760     0.760        25\n",
      "          32      1.000     0.083     0.154        36\n",
      "          33      1.000     0.033     0.065        30\n",
      "          34      0.000     0.000     0.000        32\n",
      "          35      0.857     0.600     0.706        40\n",
      "          36      1.000     0.039     0.075       103\n",
      "          37      0.000     0.000     0.000        23\n",
      "          38      0.933     0.412     0.571        34\n",
      "          39      0.000     0.000     0.000        26\n",
      "          40      1.000     0.083     0.154        24\n",
      "          41      1.000     0.077     0.143        65\n",
      "          42      1.000     0.087     0.160        46\n",
      "          43      1.000     0.027     0.053        37\n",
      "          44      0.871     0.307     0.454        88\n",
      "          45      1.000     0.200     0.333        25\n",
      "          46      1.000     0.077     0.143        26\n",
      "          47      1.000     0.125     0.222        24\n",
      "          48      1.000     0.024     0.048        41\n",
      "          49      0.745     0.837     0.788        49\n",
      "          50      1.000     0.250     0.400        24\n",
      "          51      0.000     0.000     0.000        28\n",
      "          52      0.594     0.328     0.423       125\n",
      "          53      0.000     0.000     0.000        24\n",
      "          54      0.000     0.000     0.000        55\n",
      "          55      0.000     0.000     0.000        38\n",
      "          56      0.000     0.000     0.000        20\n",
      "          57      1.000     0.148     0.258        27\n",
      "          58      1.000     0.200     0.333        20\n",
      "          59      0.000     0.000     0.000        25\n",
      "          60      0.000     0.000     0.000        30\n",
      "          61      1.000     0.118     0.211        51\n",
      "          62      1.000     0.433     0.605        30\n",
      "          63      1.000     0.073     0.135       124\n",
      "          64      1.000     0.103     0.188        29\n",
      "          65      1.000     0.054     0.103        37\n",
      "          66      1.000     0.029     0.056        69\n",
      "          67      1.000     0.032     0.062        31\n",
      "          68      1.000     0.148     0.258        27\n",
      "          69      0.867     0.520     0.650        25\n",
      "          70      0.000     0.000     0.000        74\n",
      "          71      1.000     0.122     0.217        41\n",
      "          72      1.000     0.030     0.059        33\n",
      "          73      1.000     0.029     0.057        34\n",
      "          74      1.000     0.481     0.650        27\n",
      "          75      1.000     0.056     0.105       144\n",
      "          76      1.000     0.188     0.316        32\n",
      "          77      1.000     0.062     0.118        48\n",
      "          78      0.000     0.000     0.000        22\n",
      "          79      0.000     0.000     0.000        26\n",
      "          80      1.000     0.021     0.041        48\n",
      "          81      0.679     0.603     0.639        63\n",
      "          82      0.061     1.000     0.115        20\n",
      "          83      0.941     0.376     0.538        85\n",
      "          84      1.000     0.071     0.133        56\n",
      "          85      0.917     0.200     0.328        55\n",
      "          86      0.000     0.000     0.000        37\n",
      "          87      0.000     0.000     0.000        34\n",
      "          88      1.000     0.083     0.153       109\n",
      "          89      1.000     0.083     0.154        36\n",
      "          90      1.000     0.026     0.051        38\n",
      "          91      1.000     0.071     0.133        28\n",
      "          92      1.000     0.080     0.148        25\n",
      "          93      0.000     0.000     0.000        69\n",
      "          94      0.000     0.000     0.000        28\n",
      "          95      0.000     0.000     0.000        22\n",
      "          96      1.000     0.062     0.118        80\n",
      "          97      1.000     0.113     0.204        97\n",
      "          98      0.000     0.000     0.000        23\n",
      "          99      1.000     0.125     0.222        24\n",
      "         100      0.051     0.863     0.097        51\n",
      "         101      0.000     0.000     0.000        44\n",
      "         102      1.000     0.077     0.143        26\n",
      "         103      1.000     0.111     0.200        27\n",
      "         104      0.000     0.000     0.000        88\n",
      "         105      1.000     0.034     0.067        29\n",
      "         106      1.000     0.069     0.129        29\n",
      "         107      1.000     0.150     0.261        40\n",
      "         108      1.000     0.281     0.439        32\n",
      "         109      0.000     0.000     0.000        23\n",
      "         110      1.000     0.033     0.065        30\n",
      "         111      0.099     0.900     0.178        60\n",
      "         112      1.000     0.500     0.667        24\n",
      "         113      0.960     0.381     0.545        63\n",
      "         114      1.000     0.024     0.047        42\n",
      "         115      0.000     0.000     0.000        21\n",
      "         116      1.000     0.043     0.083        23\n",
      "         117      1.000     0.145     0.253        76\n",
      "         118      0.000     0.000     0.000        41\n",
      "         119      1.000     0.149     0.259        47\n",
      "         120      1.000     0.179     0.303        28\n",
      "         121      1.000     0.050     0.095        40\n",
      "         122      1.000     0.078     0.144        90\n",
      "         123      1.000     0.036     0.069        28\n",
      "         124      1.000     0.090     0.165        78\n",
      "         125      1.000     0.182     0.308        22\n",
      "         126      1.000     0.103     0.188        29\n",
      "         127      1.000     0.045     0.087        22\n",
      "         128      1.000     0.028     0.054        36\n",
      "         129      1.000     0.056     0.106       107\n",
      "         130      1.000     0.037     0.071        27\n",
      "         131      0.726     0.231     0.350       195\n",
      "         132      1.000     0.087     0.160        23\n",
      "         133      0.000     0.000     0.000        25\n",
      "         134      1.000     0.980     0.990       458\n",
      "         135      1.000     0.021     0.042        47\n",
      "         136      1.000     0.087     0.160        23\n",
      "         137      0.000     0.000     0.000        33\n",
      "         138      1.000     0.034     0.066        59\n",
      "         139      0.000     0.000     0.000        51\n",
      "         140      1.000     0.100     0.182        30\n",
      "         141      1.000     0.071     0.132        85\n",
      "         142      1.000     0.206     0.341        34\n",
      "         143      1.000     0.193     0.324        57\n",
      "         144      1.000     0.219     0.359        64\n",
      "         145      1.000     0.129     0.229        31\n",
      "         146      0.356     0.578     0.440        64\n",
      "         147      1.000     0.346     0.514        26\n",
      "         148      1.000     0.259     0.412        27\n",
      "         149      0.202     0.907     0.331        54\n",
      "         150      0.558     0.667     0.608        36\n",
      "         151      1.000     0.128     0.226        47\n",
      "         152      1.000     0.020     0.038        51\n",
      "         153      1.000     0.159     0.275        44\n",
      "         154      0.800     0.625     0.702        32\n",
      "         155      1.000     0.024     0.047        42\n",
      "         156      1.000     0.031     0.060        97\n",
      "         157      1.000     0.023     0.045        43\n",
      "         158      1.000     0.100     0.182        30\n",
      "\n",
      "   micro avg      0.452     0.242     0.315      7789\n",
      "   macro avg      0.696     0.165     0.196      7789\n",
      "weighted avg      0.749     0.242     0.266      7789\n",
      " samples avg      0.362     0.321     0.333      7789\n",
      "\n",
      "set acc: 0.263\n",
      "dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pellegri/tools/miniconda3/envs/envPytorch1/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        13\n",
      "           1      0.000     0.000     0.000        22\n",
      "           2      0.000     0.000     0.000        24\n",
      "           3      0.000     0.000     0.000        23\n",
      "           4      0.000     0.000     0.000        10\n",
      "           5      0.000     0.000     0.000        22\n",
      "           6      0.000     0.000     0.000        35\n",
      "           7      0.000     0.000     0.000        20\n",
      "           8      0.000     0.000     0.000        11\n",
      "           9      0.857     0.240     0.375        25\n",
      "          10      0.804     0.849     0.826        53\n",
      "          11      0.000     0.000     0.000        17\n",
      "          12      0.000     0.000     0.000        10\n",
      "          13      1.000     1.000     1.000        42\n",
      "          14      0.412     0.946     0.575       112\n",
      "          15      0.333     0.350     0.341        20\n",
      "          16      1.000     0.200     0.333        30\n",
      "          17      0.000     0.000     0.000        13\n",
      "          18      0.000     0.000     0.000        31\n",
      "          19      0.000     0.000     0.000        21\n",
      "          20      0.000     0.000     0.000        16\n",
      "          21      0.000     0.000     0.000        24\n",
      "          22      0.000     0.000     0.000        22\n",
      "          23      0.000     0.000     0.000        18\n",
      "          24      0.000     0.000     0.000        16\n",
      "          25      0.000     0.000     0.000         8\n",
      "          26      1.000     0.077     0.143        13\n",
      "          27      0.000     0.000     0.000        29\n",
      "          28      0.000     0.000     0.000         9\n",
      "          29      0.000     0.000     0.000        12\n",
      "          30      0.000     0.000     0.000         8\n",
      "          31      1.000     0.667     0.800        12\n",
      "          32      0.000     0.000     0.000        13\n",
      "          33      0.000     0.000     0.000        13\n",
      "          34      0.000     0.000     0.000        10\n",
      "          35      0.923     0.522     0.667        23\n",
      "          36      0.000     0.000     0.000        47\n",
      "          37      0.000     0.000     0.000         9\n",
      "          38      1.000     0.190     0.320        21\n",
      "          39      0.000     0.000     0.000        13\n",
      "          40      0.000     0.000     0.000         8\n",
      "          41      0.000     0.000     0.000        57\n",
      "          42      0.000     0.000     0.000        25\n",
      "          43      0.000     0.000     0.000        19\n",
      "          44      0.917     0.220     0.355        50\n",
      "          45      0.000     0.000     0.000        10\n",
      "          46      0.000     0.000     0.000         9\n",
      "          47      0.000     0.000     0.000        27\n",
      "          48      0.000     0.000     0.000        21\n",
      "          49      0.640     0.762     0.696        21\n",
      "          50      0.000     0.000     0.000        12\n",
      "          51      0.000     0.000     0.000        20\n",
      "          52      0.632     0.171     0.270        70\n",
      "          53      0.000     0.000     0.000        11\n",
      "          54      0.000     0.000     0.000        27\n",
      "          55      0.000     0.000     0.000        18\n",
      "          56      0.000     0.000     0.000         8\n",
      "          57      0.000     0.000     0.000        11\n",
      "          58      0.000     0.000     0.000        12\n",
      "          59      0.000     0.000     0.000        10\n",
      "          60      0.000     0.000     0.000        13\n",
      "          61      0.000     0.000     0.000        25\n",
      "          62      1.000     0.267     0.421        15\n",
      "          63      0.000     0.000     0.000        56\n",
      "          64      0.000     0.000     0.000        17\n",
      "          65      0.000     0.000     0.000        14\n",
      "          66      0.000     0.000     0.000        45\n",
      "          67      0.000     0.000     0.000        18\n",
      "          68      0.000     0.000     0.000        14\n",
      "          69      0.429     0.250     0.316        12\n",
      "          70      0.000     0.000     0.000        34\n",
      "          71      0.000     0.000     0.000        15\n",
      "          72      0.000     0.000     0.000         9\n",
      "          73      0.000     0.000     0.000        16\n",
      "          74      0.000     0.000     0.000         6\n",
      "          75      0.000     0.000     0.000        60\n",
      "          76      0.000     0.000     0.000        12\n",
      "          77      0.000     0.000     0.000        21\n",
      "          78      0.000     0.000     0.000        13\n",
      "          79      0.000     0.000     0.000        16\n",
      "          80      0.000     0.000     0.000        28\n",
      "          81      0.600     0.577     0.588        26\n",
      "          82      0.080     1.000     0.148        12\n",
      "          83      1.000     0.089     0.163        45\n",
      "          84      0.000     0.000     0.000        31\n",
      "          85      0.429     0.107     0.171        28\n",
      "          86      0.000     0.000     0.000        17\n",
      "          87      0.000     0.000     0.000        15\n",
      "          88      0.000     0.000     0.000        53\n",
      "          89      0.000     0.000     0.000        19\n",
      "          90      0.000     0.000     0.000        21\n",
      "          91      0.000     0.000     0.000        24\n",
      "          92      0.000     0.000     0.000        11\n",
      "          93      0.000     0.000     0.000        42\n",
      "          94      0.000     0.000     0.000        13\n",
      "          95      0.000     0.000     0.000        13\n",
      "          96      0.000     0.000     0.000        33\n",
      "          97      0.000     0.000     0.000        39\n",
      "          98      0.000     0.000     0.000        15\n",
      "          99      0.000     0.000     0.000        15\n",
      "         100      0.043     0.690     0.080        29\n",
      "         101      0.000     0.000     0.000        17\n",
      "         102      0.000     0.000     0.000        10\n",
      "         103      0.000     0.000     0.000        19\n",
      "         104      0.000     0.000     0.000        54\n",
      "         105      0.000     0.000     0.000        14\n",
      "         106      0.000     0.000     0.000        19\n",
      "         107      0.000     0.000     0.000        25\n",
      "         108      0.000     0.000     0.000        17\n",
      "         109      0.000     0.000     0.000        15\n",
      "         110      0.000     0.000     0.000        21\n",
      "         111      0.085     0.900     0.156        30\n",
      "         112      1.000     0.083     0.154        12\n",
      "         113      0.500     0.036     0.067        28\n",
      "         114      0.000     0.000     0.000        10\n",
      "         115      0.000     0.000     0.000        11\n",
      "         116      0.000     0.000     0.000        14\n",
      "         117      0.000     0.000     0.000        49\n",
      "         118      0.000     0.000     0.000        16\n",
      "         119      0.000     0.000     0.000        21\n",
      "         120      0.000     0.000     0.000        18\n",
      "         121      0.000     0.000     0.000        16\n",
      "         122      0.000     0.000     0.000        50\n",
      "         123      0.000     0.000     0.000        15\n",
      "         124      0.000     0.000     0.000        41\n",
      "         125      0.000     0.000     0.000        13\n",
      "         126      0.000     0.000     0.000        22\n",
      "         127      0.000     0.000     0.000        13\n",
      "         128      0.000     0.000     0.000         7\n",
      "         129      0.000     0.000     0.000        44\n",
      "         130      0.000     0.000     0.000        10\n",
      "         131      0.632     0.128     0.212        94\n",
      "         132      0.000     0.000     0.000        17\n",
      "         133      0.000     0.000     0.000         7\n",
      "         134      1.000     0.970     0.985       233\n",
      "         135      0.000     0.000     0.000        18\n",
      "         136      0.000     0.000     0.000         7\n",
      "         137      0.000     0.000     0.000        11\n",
      "         138      0.000     0.000     0.000        31\n",
      "         139      0.000     0.000     0.000        27\n",
      "         140      0.000     0.000     0.000        13\n",
      "         141      0.000     0.000     0.000        41\n",
      "         142      0.000     0.000     0.000        20\n",
      "         143      0.000     0.000     0.000        28\n",
      "         144      0.000     0.000     0.000        34\n",
      "         145      0.000     0.000     0.000        19\n",
      "         146      0.228     0.486     0.310        37\n",
      "         147      0.000     0.000     0.000         8\n",
      "         148      0.000     0.000     0.000        16\n",
      "         149      0.176     0.774     0.287        31\n",
      "         150      0.273     0.231     0.250        13\n",
      "         151      0.000     0.000     0.000        19\n",
      "         152      0.000     0.000     0.000        24\n",
      "         153      0.000     0.000     0.000        17\n",
      "         154      0.750     0.462     0.571        13\n",
      "         155      0.000     0.000     0.000        29\n",
      "         156      0.000     0.000     0.000        49\n",
      "         157      0.000     0.000     0.000        18\n",
      "         158      0.000     0.000     0.000         6\n",
      "\n",
      "   micro avg      0.339     0.171     0.227      3827\n",
      "   macro avg      0.118     0.083     0.073      3827\n",
      "weighted avg      0.210     0.171     0.155      3827\n",
      " samples avg      0.279     0.230     0.243      3827\n",
      "\n",
      "set acc: 0.153\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        30\n",
      "           1      0.000     0.000     0.000        47\n",
      "           2      0.000     0.000     0.000        20\n",
      "           3      0.000     0.000     0.000        33\n",
      "           4      0.000     0.000     0.000        24\n",
      "           5      0.000     0.000     0.000        31\n",
      "           6      0.000     0.000     0.000        78\n",
      "           7      0.000     0.000     0.000        22\n",
      "           8      0.000     0.000     0.000        25\n",
      "           9      1.000     0.156     0.269        45\n",
      "          10      0.852     0.899     0.875       109\n",
      "          11      0.000     0.000     0.000        30\n",
      "          12      0.000     0.000     0.000        23\n",
      "          13      0.935     0.977     0.956        44\n",
      "          14      0.504     0.974     0.664       195\n",
      "          15      0.149     0.241     0.184        29\n",
      "          16      0.889     0.267     0.410        30\n",
      "          17      0.000     0.000     0.000        21\n",
      "          18      0.000     0.000     0.000        42\n",
      "          19      0.000     0.000     0.000        26\n",
      "          20      0.000     0.000     0.000        22\n",
      "          21      0.000     0.000     0.000        50\n",
      "          22      0.000     0.000     0.000        48\n",
      "          23      0.000     0.000     0.000        19\n",
      "          24      0.000     0.000     0.000        19\n",
      "          25      0.000     0.000     0.000        17\n",
      "          26      1.000     0.028     0.054        36\n",
      "          27      0.000     0.000     0.000        36\n",
      "          28      0.000     0.000     0.000        22\n",
      "          29      0.000     0.000     0.000        20\n",
      "          30      0.000     0.000     0.000        26\n",
      "          31      0.636     0.500     0.560        14\n",
      "          32      0.000     0.000     0.000        23\n",
      "          33      0.000     0.000     0.000        21\n",
      "          34      0.000     0.000     0.000        19\n",
      "          35      1.000     0.382     0.553        34\n",
      "          36      0.000     0.000     0.000        83\n",
      "          37      0.000     0.000     0.000        23\n",
      "          38      1.000     0.032     0.062        31\n",
      "          39      0.000     0.000     0.000        24\n",
      "          40      0.000     0.000     0.000        20\n",
      "          41      0.000     0.000     0.000        47\n",
      "          42      0.000     0.000     0.000        44\n",
      "          43      0.000     0.000     0.000        30\n",
      "          44      0.812     0.228     0.356        57\n",
      "          45      0.000     0.000     0.000        19\n",
      "          46      0.000     0.000     0.000        17\n",
      "          47      0.000     0.000     0.000        21\n",
      "          48      0.000     0.000     0.000        42\n",
      "          49      0.696     0.889     0.780        36\n",
      "          50      0.000     0.000     0.000        17\n",
      "          51      0.000     0.000     0.000        23\n",
      "          52      0.528     0.192     0.281        99\n",
      "          53      0.000     0.000     0.000        19\n",
      "          54      0.000     0.000     0.000        51\n",
      "          55      0.000     0.000     0.000        27\n",
      "          56      0.000     0.000     0.000        26\n",
      "          57      0.000     0.000     0.000        18\n",
      "          58      0.000     0.000     0.000        25\n",
      "          59      0.000     0.000     0.000        19\n",
      "          60      0.000     0.000     0.000        21\n",
      "          61      0.000     0.000     0.000        25\n",
      "          62      1.000     0.053     0.100        19\n",
      "          63      0.000     0.000     0.000        70\n",
      "          64      0.000     0.000     0.000        17\n",
      "          65      0.000     0.000     0.000        22\n",
      "          66      0.000     0.000     0.000        57\n",
      "          67      0.000     0.000     0.000        27\n",
      "          68      0.000     0.000     0.000        20\n",
      "          69      0.714     0.294     0.417        17\n",
      "          70      0.000     0.000     0.000        75\n",
      "          71      0.000     0.000     0.000        26\n",
      "          72      0.000     0.000     0.000        37\n",
      "          73      0.000     0.000     0.000        32\n",
      "          74      1.000     0.048     0.091        21\n",
      "          75      0.000     0.000     0.000       103\n",
      "          76      0.000     0.000     0.000        16\n",
      "          77      0.000     0.000     0.000        31\n",
      "          78      0.000     0.000     0.000        22\n",
      "          79      0.000     0.000     0.000        11\n",
      "          80      0.000     0.000     0.000        46\n",
      "          81      0.630     0.659     0.644        44\n",
      "          82      0.094     1.000     0.172        23\n",
      "          83      0.667     0.100     0.174        60\n",
      "          84      0.000     0.000     0.000        47\n",
      "          85      0.571     0.085     0.148        47\n",
      "          86      0.000     0.000     0.000        18\n",
      "          87      0.000     0.000     0.000        24\n",
      "          88      0.000     0.000     0.000        74\n",
      "          89      0.000     0.000     0.000        33\n",
      "          90      0.000     0.000     0.000        38\n",
      "          91      0.000     0.000     0.000        16\n",
      "          92      0.000     0.000     0.000        19\n",
      "          93      0.000     0.000     0.000        66\n",
      "          94      0.000     0.000     0.000        20\n",
      "          95      0.000     0.000     0.000        23\n",
      "          96      0.000     0.000     0.000        61\n",
      "          97      0.000     0.000     0.000        68\n",
      "          98      0.000     0.000     0.000        20\n",
      "          99      0.000     0.000     0.000        30\n",
      "         100      0.052     0.745     0.097        51\n",
      "         101      0.000     0.000     0.000        38\n",
      "         102      0.000     0.000     0.000        26\n",
      "         103      0.000     0.000     0.000        29\n",
      "         104      0.000     0.000     0.000        91\n",
      "         105      0.000     0.000     0.000        24\n",
      "         106      0.000     0.000     0.000        24\n",
      "         107      0.000     0.000     0.000        43\n",
      "         108      0.000     0.000     0.000        23\n",
      "         109      0.000     0.000     0.000        20\n",
      "         110      0.000     0.000     0.000        31\n",
      "         111      0.060     0.811     0.111        37\n",
      "         112      1.000     0.056     0.105        18\n",
      "         113      0.714     0.139     0.233        36\n",
      "         114      0.000     0.000     0.000        18\n",
      "         115      0.000     0.000     0.000        24\n",
      "         116      0.000     0.000     0.000        16\n",
      "         117      1.000     0.018     0.035        56\n",
      "         118      0.000     0.000     0.000        36\n",
      "         119      0.000     0.000     0.000        43\n",
      "         120      0.000     0.000     0.000        31\n",
      "         121      0.000     0.000     0.000        25\n",
      "         122      1.000     0.011     0.021        93\n",
      "         123      0.000     0.000     0.000        25\n",
      "         124      1.000     0.017     0.034        58\n",
      "         125      0.000     0.000     0.000        19\n",
      "         126      0.000     0.000     0.000        43\n",
      "         127      0.000     0.000     0.000        19\n",
      "         128      0.000     0.000     0.000        15\n",
      "         129      0.000     0.000     0.000        57\n",
      "         130      0.000     0.000     0.000        24\n",
      "         131      0.722     0.169     0.274       154\n",
      "         132      0.000     0.000     0.000        19\n",
      "         133      0.000     0.000     0.000        23\n",
      "         134      1.000     0.974     0.987       351\n",
      "         135      0.000     0.000     0.000        29\n",
      "         136      0.000     0.000     0.000        21\n",
      "         137      0.000     0.000     0.000        19\n",
      "         138      0.000     0.000     0.000        52\n",
      "         139      0.000     0.000     0.000        54\n",
      "         140      0.000     0.000     0.000        21\n",
      "         141      0.000     0.000     0.000        80\n",
      "         142      0.000     0.000     0.000        32\n",
      "         143      0.000     0.000     0.000        45\n",
      "         144      0.000     0.000     0.000        45\n",
      "         145      0.000     0.000     0.000        25\n",
      "         146      0.200     0.540     0.292        50\n",
      "         147      0.000     0.000     0.000        28\n",
      "         148      0.000     0.000     0.000        20\n",
      "         149      0.164     0.727     0.268        44\n",
      "         150      0.250     0.125     0.167        24\n",
      "         151      0.000     0.000     0.000        16\n",
      "         152      0.000     0.000     0.000        56\n",
      "         153      0.000     0.000     0.000        37\n",
      "         154      0.727     0.333     0.457        24\n",
      "         155      0.000     0.000     0.000        40\n",
      "         156      0.000     0.000     0.000        86\n",
      "         157      0.000     0.000     0.000        40\n",
      "         158      1.000     0.042     0.080        24\n",
      "\n",
      "   micro avg      0.339     0.167     0.224      6146\n",
      "   macro avg      0.148     0.080     0.069      6146\n",
      "weighted avg      0.240     0.167     0.150      6146\n",
      " samples avg      0.280     0.229     0.243      6146\n",
      "\n",
      "set acc: 0.153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pellegri/tools/miniconda3/envs/envPytorch1/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "train_pred = clf.predict(X_train_numpy)\n",
    "print_scores(y_train_numpy, train_pred)\n",
    "# compute_accuracy_from_numpy_tensors(y_train_numpy, train_pred)\n",
    "print('dev')\n",
    "dev_pred = clf.predict(X_dev_numpy)\n",
    "print_scores(y_dev_numpy, dev_pred)\n",
    "print('test')\n",
    "test_pred = clf.predict(X_test_numpy)\n",
    "print_scores(y_test_numpy, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LP with logReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LabelPowerset(\n",
    "#     classifier=LogisticRegression(penalty='l2', solver=\"lbfgs\", C=1., random_state=123, n_jobs=-1),\n",
    "    classifier=LogisticRegression(penalty='elasticnet', l1_ratio=0.5, solver=\"saga\", C=1, random_state=123, n_jobs=-1),\n",
    "    require_dense=[True, True]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_numpy_normed, y_train_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(clf, open(\"exp/bibtex/LP_logReg_model.pkl\", 'wb'))\n",
    "\n",
    "clf = pickle.load(open(\"exp/bibtex/LP_logReg_model.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.923     0.960        26\n",
      "           1      1.000     0.921     0.959        63\n",
      "           2      1.000     0.963     0.981        27\n",
      "           3      1.000     1.000     1.000        48\n",
      "           4      1.000     1.000     1.000        23\n",
      "           5      1.000     1.000     1.000        45\n",
      "           6      1.000     0.946     0.972        92\n",
      "           7      1.000     0.975     0.987        40\n",
      "           8      1.000     0.920     0.958        25\n",
      "           9      1.000     1.000     1.000        65\n",
      "          10      1.000     1.000     1.000       131\n",
      "          11      1.000     0.935     0.966        46\n",
      "          12      1.000     0.929     0.963        28\n",
      "          13      1.000     1.000     1.000        61\n",
      "          14      1.000     1.000     1.000       215\n",
      "          15      1.000     0.851     0.920        47\n",
      "          16      1.000     1.000     1.000        43\n",
      "          17      1.000     0.963     0.981        27\n",
      "          18      0.983     0.983     0.983        58\n",
      "          19      1.000     0.960     0.980        50\n",
      "          20      1.000     0.917     0.957        36\n",
      "          21      0.975     0.929     0.951        42\n",
      "          22      1.000     1.000     1.000        58\n",
      "          23      1.000     1.000     1.000        39\n",
      "          24      0.971     1.000     0.985        33\n",
      "          25      1.000     0.941     0.970        34\n",
      "          26      1.000     1.000     1.000        41\n",
      "          27      1.000     1.000     1.000        52\n",
      "          28      1.000     1.000     1.000        20\n",
      "          29      1.000     0.966     0.982        29\n",
      "          30      1.000     0.953     0.976        43\n",
      "          31      1.000     1.000     1.000        25\n",
      "          32      1.000     1.000     1.000        36\n",
      "          33      1.000     1.000     1.000        30\n",
      "          34      0.968     0.938     0.952        32\n",
      "          35      1.000     1.000     1.000        40\n",
      "          36      0.980     0.951     0.966       103\n",
      "          37      1.000     0.870     0.930        23\n",
      "          38      1.000     1.000     1.000        34\n",
      "          39      0.960     0.923     0.941        26\n",
      "          40      1.000     1.000     1.000        24\n",
      "          41      1.000     1.000     1.000        65\n",
      "          42      1.000     1.000     1.000        46\n",
      "          43      1.000     0.946     0.972        37\n",
      "          44      1.000     1.000     1.000        88\n",
      "          45      1.000     1.000     1.000        25\n",
      "          46      1.000     0.962     0.980        26\n",
      "          47      1.000     1.000     1.000        24\n",
      "          48      0.650     0.951     0.772        41\n",
      "          49      1.000     1.000     1.000        49\n",
      "          50      1.000     1.000     1.000        24\n",
      "          51      1.000     1.000     1.000        28\n",
      "          52      0.983     0.952     0.967       125\n",
      "          53      1.000     0.958     0.979        24\n",
      "          54      0.981     0.927     0.953        55\n",
      "          55      1.000     0.868     0.930        38\n",
      "          56      1.000     0.900     0.947        20\n",
      "          57      1.000     1.000     1.000        27\n",
      "          58      1.000     1.000     1.000        20\n",
      "          59      0.962     1.000     0.980        25\n",
      "          60      1.000     0.933     0.966        30\n",
      "          61      1.000     1.000     1.000        51\n",
      "          62      1.000     1.000     1.000        30\n",
      "          63      1.000     1.000     1.000       124\n",
      "          64      1.000     1.000     1.000        29\n",
      "          65      1.000     1.000     1.000        37\n",
      "          66      0.985     0.971     0.978        69\n",
      "          67      0.938     0.968     0.952        31\n",
      "          68      1.000     1.000     1.000        27\n",
      "          69      1.000     1.000     1.000        25\n",
      "          70      0.986     0.959     0.973        74\n",
      "          71      1.000     0.902     0.949        41\n",
      "          72      1.000     0.939     0.969        33\n",
      "          73      1.000     0.971     0.985        34\n",
      "          74      1.000     1.000     1.000        27\n",
      "          75      1.000     0.972     0.986       144\n",
      "          76      1.000     1.000     1.000        32\n",
      "          77      1.000     1.000     1.000        48\n",
      "          78      0.955     0.955     0.955        22\n",
      "          79      1.000     0.923     0.960        26\n",
      "          80      0.978     0.938     0.957        48\n",
      "          81      0.984     0.984     0.984        63\n",
      "          82      0.870     1.000     0.930        20\n",
      "          83      1.000     1.000     1.000        85\n",
      "          84      0.982     1.000     0.991        56\n",
      "          85      1.000     0.945     0.972        55\n",
      "          86      1.000     1.000     1.000        37\n",
      "          87      1.000     0.941     0.970        34\n",
      "          88      1.000     0.982     0.991       109\n",
      "          89      1.000     1.000     1.000        36\n",
      "          90      1.000     1.000     1.000        38\n",
      "          91      1.000     1.000     1.000        28\n",
      "          92      1.000     1.000     1.000        25\n",
      "          93      1.000     0.884     0.938        69\n",
      "          94      1.000     0.964     0.982        28\n",
      "          95      1.000     0.909     0.952        22\n",
      "          96      1.000     0.938     0.968        80\n",
      "          97      0.990     0.979     0.984        97\n",
      "          98      1.000     1.000     1.000        23\n",
      "          99      1.000     1.000     1.000        24\n",
      "         100      0.909     0.980     0.943        51\n",
      "         101      0.956     0.977     0.966        44\n",
      "         102      1.000     1.000     1.000        26\n",
      "         103      1.000     0.926     0.962        27\n",
      "         104      0.976     0.920     0.947        88\n",
      "         105      1.000     0.931     0.964        29\n",
      "         106      1.000     0.931     0.964        29\n",
      "         107      1.000     1.000     1.000        40\n",
      "         108      1.000     0.969     0.984        32\n",
      "         109      1.000     0.957     0.978        23\n",
      "         110      1.000     1.000     1.000        30\n",
      "         111      1.000     1.000     1.000        60\n",
      "         112      1.000     1.000     1.000        24\n",
      "         113      1.000     1.000     1.000        63\n",
      "         114      1.000     1.000     1.000        42\n",
      "         115      1.000     0.905     0.950        21\n",
      "         116      1.000     1.000     1.000        23\n",
      "         117      1.000     0.987     0.993        76\n",
      "         118      1.000     0.927     0.962        41\n",
      "         119      1.000     1.000     1.000        47\n",
      "         120      1.000     1.000     1.000        28\n",
      "         121      1.000     0.925     0.961        40\n",
      "         122      0.967     0.978     0.972        90\n",
      "         123      1.000     0.964     0.982        28\n",
      "         124      1.000     0.936     0.967        78\n",
      "         125      1.000     1.000     1.000        22\n",
      "         126      1.000     1.000     1.000        29\n",
      "         127      1.000     1.000     1.000        22\n",
      "         128      0.969     0.861     0.912        36\n",
      "         129      0.990     0.925     0.957       107\n",
      "         130      1.000     0.963     0.981        27\n",
      "         131      0.882     0.959     0.919       195\n",
      "         132      1.000     1.000     1.000        23\n",
      "         133      1.000     0.920     0.958        25\n",
      "         134      1.000     1.000     1.000       458\n",
      "         135      0.979     0.979     0.979        47\n",
      "         136      1.000     0.913     0.955        23\n",
      "         137      0.968     0.909     0.937        33\n",
      "         138      1.000     1.000     1.000        59\n",
      "         139      1.000     0.922     0.959        51\n",
      "         140      0.967     0.967     0.967        30\n",
      "         141      0.988     0.976     0.982        85\n",
      "         142      1.000     1.000     1.000        34\n",
      "         143      1.000     1.000     1.000        57\n",
      "         144      1.000     1.000     1.000        64\n",
      "         145      1.000     1.000     1.000        31\n",
      "         146      1.000     1.000     1.000        64\n",
      "         147      1.000     1.000     1.000        26\n",
      "         148      1.000     1.000     1.000        27\n",
      "         149      1.000     1.000     1.000        54\n",
      "         150      1.000     1.000     1.000        36\n",
      "         151      1.000     1.000     1.000        47\n",
      "         152      1.000     0.922     0.959        51\n",
      "         153      1.000     1.000     1.000        44\n",
      "         154      1.000     1.000     1.000        32\n",
      "         155      1.000     0.976     0.988        42\n",
      "         156      0.989     0.938     0.963        97\n",
      "         157      1.000     0.837     0.911        43\n",
      "         158      1.000     0.867     0.929        30\n",
      "\n",
      "   micro avg      0.989     0.971     0.980      7789\n",
      "   macro avg      0.991     0.969     0.980      7789\n",
      "weighted avg      0.990     0.971     0.980      7789\n",
      " samples avg      0.985     0.979     0.980      7789\n",
      "\n",
      "set acc: 0.973\n",
      "dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pellegri/tools/miniconda3/envs/envPytorch1/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.333     0.077     0.125        13\n",
      "           1      0.111     0.091     0.100        22\n",
      "           2      0.000     0.000     0.000        24\n",
      "           3      0.462     0.261     0.333        23\n",
      "           4      0.167     0.100     0.125        10\n",
      "           5      0.333     0.091     0.143        22\n",
      "           6      0.400     0.171     0.240        35\n",
      "           7      0.111     0.050     0.069        20\n",
      "           8      0.500     0.182     0.267        11\n",
      "           9      0.625     0.600     0.612        25\n",
      "          10      0.855     1.000     0.922        53\n",
      "          11      0.400     0.471     0.432        17\n",
      "          12      0.000     0.000     0.000        10\n",
      "          13      1.000     1.000     1.000        42\n",
      "          14      0.839     0.929     0.881       112\n",
      "          15      0.219     0.350     0.269        20\n",
      "          16      0.607     0.567     0.586        30\n",
      "          17      0.000     0.000     0.000        13\n",
      "          18      0.696     0.516     0.593        31\n",
      "          19      0.167     0.048     0.074        21\n",
      "          20      0.000     0.000     0.000        16\n",
      "          21      0.500     0.292     0.368        24\n",
      "          22      0.267     0.364     0.308        22\n",
      "          23      0.571     0.222     0.320        18\n",
      "          24      0.000     0.000     0.000        16\n",
      "          25      1.000     0.125     0.222         8\n",
      "          26      0.450     0.692     0.545        13\n",
      "          27      0.250     0.103     0.146        29\n",
      "          28      0.071     0.111     0.087         9\n",
      "          29      0.571     0.333     0.421        12\n",
      "          30      0.143     0.125     0.133         8\n",
      "          31      0.632     1.000     0.774        12\n",
      "          32      0.000     0.000     0.000        13\n",
      "          33      0.000     0.000     0.000        13\n",
      "          34      0.250     0.100     0.143        10\n",
      "          35      0.808     0.913     0.857        23\n",
      "          36      0.556     0.106     0.179        47\n",
      "          37      0.000     0.000     0.000         9\n",
      "          38      0.600     0.286     0.387        21\n",
      "          39      0.000     0.000     0.000        13\n",
      "          40      0.385     0.625     0.476         8\n",
      "          41      0.500     0.018     0.034        57\n",
      "          42      0.231     0.120     0.158        25\n",
      "          43      0.667     0.105     0.182        19\n",
      "          44      0.800     0.880     0.838        50\n",
      "          45      1.000     0.200     0.333        10\n",
      "          46      0.000     0.000     0.000         9\n",
      "          47      1.000     0.037     0.071        27\n",
      "          48      0.143     0.333     0.200        21\n",
      "          49      0.727     0.762     0.744        21\n",
      "          50      0.000     0.000     0.000        12\n",
      "          51      0.214     0.150     0.176        20\n",
      "          52      0.508     0.429     0.465        70\n",
      "          53      0.714     0.455     0.556        11\n",
      "          54      0.357     0.185     0.244        27\n",
      "          55      0.417     0.278     0.333        18\n",
      "          56      0.000     0.000     0.000         8\n",
      "          57      0.500     0.091     0.154        11\n",
      "          58      0.286     0.167     0.211        12\n",
      "          59      0.429     0.300     0.353        10\n",
      "          60      0.111     0.077     0.091        13\n",
      "          61      0.588     0.400     0.476        25\n",
      "          62      0.800     0.533     0.640        15\n",
      "          63      0.723     0.607     0.660        56\n",
      "          64      1.000     0.235     0.381        17\n",
      "          65      0.500     0.143     0.222        14\n",
      "          66      0.350     0.156     0.215        45\n",
      "          67      0.111     0.056     0.074        18\n",
      "          68      0.091     0.071     0.080        14\n",
      "          69      0.444     0.667     0.533        12\n",
      "          70      0.167     0.029     0.050        34\n",
      "          71      0.211     0.267     0.235        15\n",
      "          72      0.000     0.000     0.000         9\n",
      "          73      0.000     0.000     0.000        16\n",
      "          74      0.375     0.500     0.429         6\n",
      "          75      0.214     0.050     0.081        60\n",
      "          76      0.714     0.833     0.769        12\n",
      "          77      0.450     0.429     0.439        21\n",
      "          78      0.250     0.308     0.276        13\n",
      "          79      0.400     0.375     0.387        16\n",
      "          80      0.636     0.250     0.359        28\n",
      "          81      0.500     0.577     0.536        26\n",
      "          82      0.688     0.917     0.786        12\n",
      "          83      0.689     0.689     0.689        45\n",
      "          84      0.500     0.032     0.061        31\n",
      "          85      0.421     0.286     0.340        28\n",
      "          86      0.923     0.706     0.800        17\n",
      "          87      0.222     0.133     0.167        15\n",
      "          88      0.000     0.000     0.000        53\n",
      "          89      0.667     0.211     0.320        19\n",
      "          90      0.000     0.000     0.000        21\n",
      "          91      1.000     0.042     0.080        24\n",
      "          92      0.000     0.000     0.000        11\n",
      "          93      0.222     0.048     0.078        42\n",
      "          94      0.000     0.000     0.000        13\n",
      "          95      0.000     0.000     0.000        13\n",
      "          96      0.500     0.030     0.057        33\n",
      "          97      0.394     0.333     0.361        39\n",
      "          98      0.200     0.067     0.100        15\n",
      "          99      0.000     0.000     0.000        15\n",
      "         100      0.173     0.448     0.250        29\n",
      "         101      0.591     0.765     0.667        17\n",
      "         102      1.000     0.100     0.182        10\n",
      "         103      0.000     0.000     0.000        19\n",
      "         104      0.426     0.370     0.396        54\n",
      "         105      0.600     0.214     0.316        14\n",
      "         106      0.000     0.000     0.000        19\n",
      "         107      0.000     0.000     0.000        25\n",
      "         108      0.667     0.353     0.462        17\n",
      "         109      0.000     0.000     0.000        15\n",
      "         110      0.333     0.095     0.148        21\n",
      "         111      0.323     0.700     0.442        30\n",
      "         112      0.583     0.583     0.583        12\n",
      "         113      0.469     0.536     0.500        28\n",
      "         114      0.000     0.000     0.000        10\n",
      "         115      0.667     0.182     0.286        11\n",
      "         116      0.000     0.000     0.000        14\n",
      "         117      0.718     0.571     0.636        49\n",
      "         118      0.000     0.000     0.000        16\n",
      "         119      0.333     0.190     0.242        21\n",
      "         120      0.833     0.278     0.417        18\n",
      "         121      0.227     0.312     0.263        16\n",
      "         122      0.367     0.220     0.275        50\n",
      "         123      0.500     0.133     0.211        15\n",
      "         124      0.241     0.171     0.200        41\n",
      "         125      0.714     0.385     0.500        13\n",
      "         126      0.000     0.000     0.000        22\n",
      "         127      0.000     0.000     0.000        13\n",
      "         128      0.000     0.000     0.000         7\n",
      "         129      0.118     0.045     0.066        44\n",
      "         130      0.333     0.100     0.154        10\n",
      "         131      0.392     0.617     0.479        94\n",
      "         132      0.667     0.235     0.348        17\n",
      "         133      0.000     0.000     0.000         7\n",
      "         134      0.958     0.876     0.915       233\n",
      "         135      0.500     0.111     0.182        18\n",
      "         136      0.000     0.000     0.000         7\n",
      "         137      0.000     0.000     0.000        11\n",
      "         138      0.500     0.032     0.061        31\n",
      "         139      0.583     0.259     0.359        27\n",
      "         140      0.067     0.077     0.071        13\n",
      "         141      0.000     0.000     0.000        41\n",
      "         142      0.091     0.050     0.065        20\n",
      "         143      0.500     0.179     0.263        28\n",
      "         144      0.438     0.206     0.280        34\n",
      "         145      0.000     0.000     0.000        19\n",
      "         146      0.227     0.270     0.247        37\n",
      "         147      0.143     0.250     0.182         8\n",
      "         148      0.316     0.375     0.343        16\n",
      "         149      0.410     0.516     0.457        31\n",
      "         150      0.206     0.538     0.298        13\n",
      "         151      0.700     0.368     0.483        19\n",
      "         152      0.000     0.000     0.000        24\n",
      "         153      1.000     0.118     0.211        17\n",
      "         154      0.350     0.538     0.424        13\n",
      "         155      0.818     0.310     0.450        29\n",
      "         156      0.294     0.204     0.241        49\n",
      "         157      0.100     0.056     0.071        18\n",
      "         158      0.000     0.000     0.000         6\n",
      "\n",
      "   micro avg      0.495     0.331     0.397      3827\n",
      "   macro avg      0.364     0.248     0.262      3827\n",
      "weighted avg      0.441     0.331     0.345      3827\n",
      " samples avg      0.498     0.396     0.418      3827\n",
      "\n",
      "set acc: 0.235\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        30\n",
      "           1      0.148     0.085     0.108        47\n",
      "           2      0.000     0.000     0.000        20\n",
      "           3      0.818     0.273     0.409        33\n",
      "           4      0.250     0.083     0.125        24\n",
      "           5      0.778     0.226     0.350        31\n",
      "           6      0.333     0.103     0.157        78\n",
      "           7      0.083     0.045     0.059        22\n",
      "           8      0.625     0.200     0.303        25\n",
      "           9      0.706     0.533     0.608        45\n",
      "          10      0.835     0.972     0.898       109\n",
      "          11      0.500     0.467     0.483        30\n",
      "          12      0.000     0.000     0.000        23\n",
      "          13      1.000     1.000     1.000        44\n",
      "          14      0.936     0.974     0.955       195\n",
      "          15      0.178     0.276     0.216        29\n",
      "          16      0.556     0.667     0.606        30\n",
      "          17      0.000     0.000     0.000        21\n",
      "          18      0.433     0.310     0.361        42\n",
      "          19      0.000     0.000     0.000        26\n",
      "          20      0.300     0.136     0.187        22\n",
      "          21      0.600     0.240     0.343        50\n",
      "          22      0.370     0.354     0.362        48\n",
      "          23      0.667     0.105     0.182        19\n",
      "          24      0.000     0.000     0.000        19\n",
      "          25      0.000     0.000     0.000        17\n",
      "          26      0.463     0.528     0.494        36\n",
      "          27      1.000     0.083     0.154        36\n",
      "          28      0.300     0.136     0.187        22\n",
      "          29      0.455     0.250     0.323        20\n",
      "          30      0.333     0.231     0.273        26\n",
      "          31      0.545     0.857     0.667        14\n",
      "          32      0.000     0.000     0.000        23\n",
      "          33      0.000     0.000     0.000        21\n",
      "          34      0.050     0.053     0.051        19\n",
      "          35      0.897     0.765     0.825        34\n",
      "          36      0.571     0.048     0.089        83\n",
      "          37      0.100     0.043     0.061        23\n",
      "          38      0.810     0.548     0.654        31\n",
      "          39      0.000     0.000     0.000        24\n",
      "          40      0.867     0.650     0.743        20\n",
      "          41      0.500     0.043     0.078        47\n",
      "          42      0.273     0.068     0.109        44\n",
      "          43      1.000     0.100     0.182        30\n",
      "          44      0.766     0.860     0.810        57\n",
      "          45      0.500     0.211     0.296        19\n",
      "          46      0.000     0.000     0.000        17\n",
      "          47      0.000     0.000     0.000        21\n",
      "          48      0.197     0.333     0.248        42\n",
      "          49      0.767     0.917     0.835        36\n",
      "          50      0.000     0.000     0.000        17\n",
      "          51      0.095     0.087     0.091        23\n",
      "          52      0.571     0.444     0.500        99\n",
      "          53      0.462     0.316     0.375        19\n",
      "          54      0.765     0.255     0.382        51\n",
      "          55      0.545     0.222     0.316        27\n",
      "          56      0.500     0.115     0.188        26\n",
      "          57      1.000     0.056     0.105        18\n",
      "          58      0.667     0.240     0.353        25\n",
      "          59      0.714     0.263     0.385        19\n",
      "          60      0.095     0.095     0.095        21\n",
      "          61      0.600     0.360     0.450        25\n",
      "          62      0.842     0.842     0.842        19\n",
      "          63      0.763     0.643     0.698        70\n",
      "          64      0.286     0.118     0.167        17\n",
      "          65      0.500     0.045     0.083        22\n",
      "          66      0.276     0.140     0.186        57\n",
      "          67      0.125     0.111     0.118        27\n",
      "          68      0.250     0.250     0.250        20\n",
      "          69      0.500     0.647     0.564        17\n",
      "          70      0.417     0.067     0.115        75\n",
      "          71      0.216     0.308     0.254        26\n",
      "          72      0.500     0.054     0.098        37\n",
      "          73      0.000     0.000     0.000        32\n",
      "          74      0.538     0.333     0.412        21\n",
      "          75      0.489     0.214     0.297       103\n",
      "          76      0.455     0.312     0.370        16\n",
      "          77      0.571     0.129     0.211        31\n",
      "          78      0.500     0.364     0.421        22\n",
      "          79      0.375     0.545     0.444        11\n",
      "          80      0.273     0.065     0.105        46\n",
      "          81      0.492     0.682     0.571        44\n",
      "          82      0.543     0.826     0.655        23\n",
      "          83      0.667     0.567     0.613        60\n",
      "          84      0.500     0.021     0.041        47\n",
      "          85      0.538     0.298     0.384        47\n",
      "          86      0.542     0.722     0.619        18\n",
      "          87      0.231     0.125     0.162        24\n",
      "          88      0.133     0.027     0.045        74\n",
      "          89      0.500     0.091     0.154        33\n",
      "          90      0.000     0.000     0.000        38\n",
      "          91      0.000     0.000     0.000        16\n",
      "          92      0.000     0.000     0.000        19\n",
      "          93      0.250     0.106     0.149        66\n",
      "          94      1.000     0.050     0.095        20\n",
      "          95      0.500     0.130     0.207        23\n",
      "          96      0.267     0.066     0.105        61\n",
      "          97      0.346     0.265     0.300        68\n",
      "          98      0.200     0.050     0.080        20\n",
      "          99      0.667     0.067     0.121        30\n",
      "         100      0.156     0.451     0.232        51\n",
      "         101      0.609     0.737     0.667        38\n",
      "         102      0.588     0.385     0.465        26\n",
      "         103      0.091     0.034     0.050        29\n",
      "         104      0.384     0.308     0.341        91\n",
      "         105      0.250     0.042     0.071        24\n",
      "         106      0.000     0.000     0.000        24\n",
      "         107      0.000     0.000     0.000        43\n",
      "         108      0.000     0.000     0.000        23\n",
      "         109      0.500     0.100     0.167        20\n",
      "         110      0.429     0.097     0.158        31\n",
      "         111      0.267     0.622     0.374        37\n",
      "         112      0.588     0.556     0.571        18\n",
      "         113      0.425     0.472     0.447        36\n",
      "         114      0.000     0.000     0.000        18\n",
      "         115      1.000     0.083     0.154        24\n",
      "         116      0.000     0.000     0.000        16\n",
      "         117      0.627     0.571     0.598        56\n",
      "         118      0.667     0.056     0.103        36\n",
      "         119      0.368     0.163     0.226        43\n",
      "         120      0.000     0.000     0.000        31\n",
      "         121      0.286     0.400     0.333        25\n",
      "         122      0.386     0.344     0.364        93\n",
      "         123      0.200     0.040     0.067        25\n",
      "         124      0.208     0.172     0.189        58\n",
      "         125      0.667     0.316     0.429        19\n",
      "         126      1.000     0.023     0.045        43\n",
      "         127      0.000     0.000     0.000        19\n",
      "         128      0.000     0.000     0.000        15\n",
      "         129      0.184     0.123     0.147        57\n",
      "         130      0.333     0.042     0.074        24\n",
      "         131      0.424     0.617     0.503       154\n",
      "         132      0.800     0.211     0.333        19\n",
      "         133      1.000     0.043     0.083        23\n",
      "         134      0.970     0.906     0.937       351\n",
      "         135      0.000     0.000     0.000        29\n",
      "         136      0.000     0.000     0.000        21\n",
      "         137      0.000     0.000     0.000        19\n",
      "         138      0.400     0.038     0.070        52\n",
      "         139      0.812     0.241     0.371        54\n",
      "         140      0.333     0.190     0.242        21\n",
      "         141      0.200     0.050     0.080        80\n",
      "         142      0.407     0.344     0.373        32\n",
      "         143      0.333     0.111     0.167        45\n",
      "         144      0.250     0.156     0.192        45\n",
      "         145      0.000     0.000     0.000        25\n",
      "         146      0.263     0.300     0.280        50\n",
      "         147      0.174     0.143     0.157        28\n",
      "         148      0.120     0.150     0.133        20\n",
      "         149      0.278     0.341     0.306        44\n",
      "         150      0.206     0.542     0.299        24\n",
      "         151      0.350     0.438     0.389        16\n",
      "         152      0.125     0.018     0.031        56\n",
      "         153      0.125     0.027     0.044        37\n",
      "         154      0.478     0.458     0.468        24\n",
      "         155      0.500     0.175     0.259        40\n",
      "         156      0.196     0.116     0.146        86\n",
      "         157      0.250     0.100     0.143        40\n",
      "         158      0.571     0.500     0.533        24\n",
      "\n",
      "   micro avg      0.494     0.316     0.386      6146\n",
      "   macro avg      0.387     0.238     0.256      6146\n",
      "weighted avg      0.453     0.316     0.334      6146\n",
      " samples avg      0.491     0.390     0.411      6146\n",
      "\n",
      "set acc: 0.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pellegri/tools/miniconda3/envs/envPytorch1/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "train_pred = clf.predict(X_train_numpy_normed)\n",
    "print_scores(y_train_numpy, train_pred)\n",
    "# compute_accuracy_from_numpy_tensors(y_train_numpy, train_pred)\n",
    "print('dev')\n",
    "dev_pred = clf.predict(X_dev_numpy_normed)\n",
    "print_scores(y_dev_numpy, dev_pred)\n",
    "print('test')\n",
    "test_pred = clf.predict(X_test_numpy_normed)\n",
    "print_scores(y_test_numpy, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3269, 159), (1611, 159), (2515, 159))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg_train_prob = clf.predict_proba(X_train_numpy_normed)\n",
    "logReg_dev_prob = clf.predict_proba(X_dev_numpy_normed)\n",
    "logReg_test_prob = clf.predict_proba(X_test_numpy_normed)\n",
    "\n",
    "logReg_train_prob = logReg_train_prob.toarray()\n",
    "logReg_dev_prob = logReg_dev_prob.toarray()\n",
    "logReg_test_prob = logReg_test_prob.toarray()\n",
    "logReg_train_prob.shape, logReg_dev_prob.shape, logReg_test_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999990322447"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(logReg_train_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"exp/%s/logReg_LP_standardization_probs.npz\"%dataset,\n",
    "         logReg_train_prob=logReg_train_prob,\n",
    "         logReg_dev_prob=logReg_dev_prob,\n",
    "        logReg_test_prob=logReg_test_prob\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
